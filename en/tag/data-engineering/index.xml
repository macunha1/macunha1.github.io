<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>data-engineering | It's me, Macunha!</title><link>https://macunha.me/en/tag/data-engineering/</link><atom:link href="https://macunha.me/en/tag/data-engineering/index.xml" rel="self" type="application/rss+xml"/><description>data-engineering</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 21 Jul 2019 00:00:00 +0000</lastBuildDate><image><url>https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_512x512_fill_lanczos_center_2.png</url><title>data-engineering</title><link>https://macunha.me/en/tag/data-engineering/</link></image><item><title>ReclameAQUI Data Lake</title><link>https://macunha.me/en/project/2019/reclameaqui-data-lake/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2019/reclameaqui-data-lake/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>ReclameAQUI (Portuguese for &amp;ldquo;complain here&amp;rdquo;) is an interesting and unique
business. They&amp;rsquo;re a content aggregator for customers&amp;rsquo; experience sharing
(especially bad experiences) about shopping (online and offline). However, it
goes further than a mere &amp;ldquo;complaints website&amp;rdquo; offering an interface for
companies to answers complaints, helping customers with their issues.&lt;/p>
&lt;p>The service is simply the biggest in this regard (worldwide) receiving 600K
unique visitors each day, searching for a company&amp;rsquo;s reputation before closing a
deal/purchase.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Even though they are already advanced in the digital approach to business,
having most services hosted on Cloud computing and analytical culture, their
data lake needed some upgrades. The most relevant motivator of this project was
the sky-high bills from GCP especially related to BigQuery data consumption.&lt;/p>
&lt;p>Apart from the cost-reduction tasks and data ingestion process optimization, we
took the opportunity to implement data cryptograph at-rest, governance, and
obfuscation during query executions against the data lake. Making data
accessible by everyone in the company, controlling identity access and
management through LDAP (auditing each access, to be fully compliant with
GDPR), we could offer a self-service data lake so different business actors
could satisfy their needs &amp;ldquo;drinking&amp;rdquo; from the lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="tech-implementation">Tech implementation&lt;/h2>
&lt;p>Key objectives were cost-optimization of the existing Data Lake, improvement
(and extension) of existing data ingestion pipelines, and security enhancements.&lt;/p>
&lt;p>Starting from Data Lake&amp;rsquo;s cost optimization, we redesigned the data ingestion,
using a &amp;ldquo;landing&amp;rdquo; area for raw data, making data transformations later to suit
the desired data models. Saving the results in other Data Lake layers to achieve
greater performance in queries.&lt;/p>
&lt;p>We shifted away from the Streaming inserts in BigQuery by adding a step to load
data at the end of the ingestion pipeline. Apache NiFi was the main software
responsible for orchestrating and executing the pipeline, covering also the
improvements in data ingestion through processes re-engineering.&lt;/p>
&lt;p>Auditing in the Data Lake was managed through Apache Ranger. In order to have
it fully supported we implemented a JDBC driver using a component from Apache
Calcite called Avatica. Authentication for Apache Ranger went through a custom
plugin (also developed during the project) for LDAP consuming user info from
Google Cloud Identity, reflecting the existing organization&amp;rsquo;s users and groups
from Google Suite.&lt;/p>
&lt;p>To make the game more interesting, we containerized the workflow and heavily
used Kubernetes (GKE) to manage these components. Most of the Apache projects
didn&amp;rsquo;t have Helm Charts at the time and we developed and made some
of them open-source.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>During project time we could measure an estimative of roughly 56% in Data Lake
cost-optimization through reengineering of processes and resources, especially
the removal of streaming inserts to BigQuery.&lt;/p>
&lt;p>We made relevant progress in security and governance during the project with the
introduction of Apache Ranger and Data Lake auditing for access and usage,
providing advanced security capabilities to ReclameAQUI, which anticipated itself
towards GDPR and data privacy concerns.&lt;/p></description></item><item><title>Dotz Data Labs</title><link>https://macunha.me/en/project/2017/dotz-data-labs/</link><pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2017/dotz-data-labs/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>In order to be able to innovate and maintain itself in a highly changing
and evolving market, Dotz went through a process of digital
transformation and had the help of some consultants along the way.&lt;/p>
&lt;p>Among the steps to get closer to a digital model, the implementation of
a Data Lake emerged, with the requirements of being
&lt;a href="https://en.wikipedia.org/wiki/Serverless_computing" target="_blank" rel="noopener">serverless&lt;/a> and
cloud-native to support the decision-making process and shorten
time-to-market during the launch of products.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Dotz is one of the largest companies in the field of loyalty program in
Brazil and they&amp;rsquo;d face a high number of issues with data disconnection
making it difficult to analyze their users' behaviors. Since they
received data from numerous supermarkets and stores, it&amp;rsquo;s difficult to
clusterizate products, since the name is different depending on the
source. To help with this analysis, they decided to build a Data Lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>We built and deployed a Big Data's managed architecture using Google's
Cloud Platform (GCP) to support this strategy and allow a 360-degree
view of customers (users with points a.k.a. Dotz) and partners (the
supermarkets offering the loyalty program).&lt;/p>
&lt;p>The design was focused on cloud-managed services and serverless
computing offered by Google, serving the core competencies of a Data
Lake such as scalable storage using Google Cloud Storage, and Google
BigQuery. With part of the process running inside Kubernetes,
responsible for data cleansing ETL flow management.&lt;/p>
&lt;p>We streamed data with Apache Beam running under Google DataFlow,
parallel mass processing with Apache Spark jobs running on Google
DataProc, exploratory analysis with Google DataLab, Machine Learning
Analysis with Google ML and Data visualization in Google Data Studio.&lt;/p>
&lt;p>Data is transported using an event-driven model, where every data is
collected using a streaming model, even the ETL (which runs on a
micro-batch, to enable near real-time exploration). These data goes
through the data pipeline using Google Pub/Sub message-oriented
middleware, and every message is serialized using Avro format, which
reduces the payload and allows transportation to be cost-effective, fast
and reliable.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>It all allowed Dotz to have a better structure on their analytical
platform, previously managed inside a large MS SQL Server instance,
which moved to a Data Lake with layers allowing data categorization,
governance, quality and security.&lt;/p>
&lt;p>Supporting analytical processes of users data, faster exploration and
monetization of their knowledge on customers' behavior.&lt;/p></description></item><item><title>Easynvest Data Platform</title><link>https://macunha.me/en/project/2017/easynvest-data-platform/</link><pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2017/easynvest-data-platform/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Within Easynvest&amp;rsquo;s annual planning, an investment in the expansion of
data &amp;amp; analytics team aimed at shortening decision making process and
in delivering a higher quality to customers through a low cost operational
process.&lt;/p>
&lt;p>Among the main objectives of this project, we had the automation of
credit analysis (executed during the approval of customer registration,
using Machine Learning), a process that until then was long and manual,
being handled by the back office.&lt;/p>
&lt;p>Followed by a better offer of products to the client, carrying out the
categorization according to the profile of each customer, allowing
suggestions of more attractive products, in line with personal preferences,
as well as according to the profile of each investor (conservative,
moderate or aggressive).&lt;/p>
&lt;p>Last but not least, the intelligent detection of money laundry and
reporting to the responsible authorities.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>However, there were limitations in the data tools, mainly due to the
fact that they were proprietary software (with limited licenses) and
designed for usage inside data centers. In addition, the analytical
database was modeled for traditional Business Intelligence models
(OLAP, etc), making the decision making process heavy, due to the
demanding amount of interactions during ETL.&lt;/p>
&lt;p>Previously for a client to be approved, the process took 10 to 15 days.
Gathering all necessary information, providing a complete perspective of
the profile, including credit analysis. After collecting the information,
the back office generated an internal credit analysis score.&lt;/p>
&lt;p>In most cases, the client was not notified of updates regarding the
process and did not receive feedback at the end (if refused) unless
explicitly requested (by contacting support via chat or email, for
example) which made the process time consuming and costly. Not to
mention the countless amounts of customers lost to the competition during
this long wait.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>To make it possible, we built an hybrid-cloud implementation using AWS
cloud-based components (mainly AWS S3, EMR and ECS), to extend the data
centers' capability, implementing a cloud-first Hadoop ecosystem
(replacing the proprietary software components with open-source
equivalents). Giving Easynvest the possibility to grow the Data Lake
exponentially.&lt;/p>
&lt;p>The Data Lake design was robust, aimed to handle the execution of heavy
analytical processes through Machine Learning models, with support for
data quality, metadata governance, information security and data serving
(data owners could share data with consumers from other areas within the
company, allowing to self-service their analytical data).&lt;/p>
&lt;p>A Chatbot was also used to reduce the operational load in the
environment, this bot is responsible for maintaining and updating
infrastructure components. From triggering deployments to generating
encryption keys on-demand for data security and governance. Implemented
with the Errbot framework for Python interacting with Slack.&lt;/p>
&lt;p>Going further, we implemented the best practices in DevOps, using Jenkins
as a tool for CI/CD of the developed components alongside with Ansible for
Configuration Management.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Thanks to the utilization of layers in the Data Lake and the
implementation of data pipelines, we were able to reduce the data
ingestion time by 78% and to include metadata and data catalog, in
addition to automating much of the work that used to be done manually.&lt;/p>
&lt;p>Thus bringing positive results, especially reducing the registration
approval time for consumers from roughly 10 days to 1 day. It also made
the data platform more democratic, providing relevant information that
facilitates the analysis of areas such as risk (credit analysis) and
support, without having to give up security.&lt;/p></description></item></channel></rss>