<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>data-engineering | It's me, Macunha!</title><link>https://macunha.me/en/tag/data-engineering/</link><atom:link href="https://macunha.me/en/tag/data-engineering/index.xml" rel="self" type="application/rss+xml"/><description>data-engineering</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Matheus Cunha</copyright><lastBuildDate>Thu, 21 May 2020 23:00:57 +0200</lastBuildDate><image><url>https://macunha.me/media/icon_hu176de0364afaeda8922c372b574c3cbf_6946_512x512_fill_lanczos_center_3.png</url><title>data-engineering</title><link>https://macunha.me/en/tag/data-engineering/</link></image><item><title>Quickstart: Apache Spark on Kubernetes</title><link>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</link><pubDate>Thu, 21 May 2020 23:00:57 +0200</pubDate><guid>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="the-apache-spark-operator-for-kubernetes">The Apache Spark Operator for Kubernetes&lt;/h2>
&lt;p>Since its launch in 2014 by Google, Kubernetes has gained a lot of
popularity along with Docker itself and since 2016 has become the &lt;em>de
facto Container Orchestrator&lt;/em>, established as a market standard.
Having cloud-managed versions available in &lt;strong>all&lt;/strong> the &lt;em>major Clouds&lt;/em>.
&lt;a href="https://cloud.google.com/kubernetes-engine/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://docs.microsoft.com/en-us/azure/aks/" target="_blank" rel="noopener">[3]&lt;/a> (including
&lt;a href="https://www.digitalocean.com/products/kubernetes/" target="_blank" rel="noopener">Digital Ocean&lt;/a> and
&lt;a href="https://www.alibabacloud.com/product/kubernetes" target="_blank" rel="noopener">Alibaba&lt;/a>).&lt;/p>
&lt;p>With this popularity came various implementations and &lt;em>use-cases&lt;/em> of
the orchestrator, among them the execution of &lt;a href="https://kubernetes.io/docs/tutorials/stateful-application/" target="_blank" rel="noopener">Stateful
applications&lt;/a>
including &lt;a href="https://vitess.io/zh/docs/get-started/kubernetes/" target="_blank" rel="noopener">databases using containers&lt;/a>.&lt;/p>
&lt;p>What would be the motivation to host an orchestrated database? That&amp;rsquo;s
a great question. But let&amp;rsquo;s focus on the &lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md" target="_blank" rel="noopener">Spark Operator&lt;/a>
running workloads on Kubernetes.&lt;/p>
&lt;p>A native Spark Operator &lt;a href="https://github.com/kubernetes/kubernetes/issues/34377" target="_blank" rel="noopener">idea came out&lt;/a>
in 2016, before that you couldn&amp;rsquo;t run Spark jobs natively except
some &lt;em>hacky alternatives&lt;/em>, like &lt;a href="https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/" target="_blank" rel="noopener">running Apache Zeppelin&lt;/a>
inside Kubernetes or creating your &lt;a href="https://github.com/kubernetes/examples/tree/master/staging/spark" target="_blank" rel="noopener">Apache Spark cluster inside
Kubernetes (from the official Kubernetes organization on GitHub)&lt;/a>
referencing the &lt;a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Spark workers in Stand-alone mode&lt;/a>.&lt;/p>
&lt;p>However, the native execution would be far more interesting for taking
advantage of &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank" rel="noopener">Kubernetes Scheduler&lt;/a>
responsible for taking action of allocating resources, giving
elasticity and an simpler interface to manage Apache Spark workloads.&lt;/p>
&lt;p>Considering that, &lt;a href="https://issues.apache.org/jira/browse/SPARK-18278" target="_blank" rel="noopener">Apache Spark Operator development got attention&lt;/a>,
merged and released into &lt;a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">Spark version 2.3.0&lt;/a>
launched in &lt;a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">February, 2018&lt;/a>.&lt;/p>
&lt;p>If you&amp;rsquo;re eager for reading more regarding the Apache Spark proposal,
you can head to the &lt;a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target="_blank" rel="noopener">design document published in Google Docs.&lt;/a>&lt;/p>
&lt;h2 id="why-kubernetes">Why Kubernetes?&lt;/h2>
&lt;p>As companies are currently seeking to &lt;a href="https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html" target="_blank" rel="noopener">reinvent themselves through the
widely spoken digital transformation&lt;/a>
in order for them to be competitive and, above all, to survive in an
increasingly dynamic market, it is common to see approaches that
include Big Data, Artificial Intelligence and Cloud Computing
&lt;a href="https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/" target="_blank" rel="noopener">[3]&lt;/a>.&lt;/p>
&lt;p>An interesting comparison between the benefits of using Cloud Computing in the
context of Big Data instead of On-premises' servers can be read at &lt;a href="https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html" target="_blank" rel="noopener">Databricks
blog&lt;/a>,
which is the company &lt;a href="https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/" target="_blank" rel="noopener">founded by the creators of Apache Spark&lt;/a>.&lt;/p>
&lt;p>As we see a widespread adoption of Cloud Computing (even by companies
that would be able to afford the hardware and run on-premises), we
notice that most of these Cloud implementations don&amp;rsquo;t have an &lt;a href="https://hadoop.apache.org/" target="_blank" rel="noopener">Apache
Hadoop&lt;/a> since the Data Teams (BI/Data
Science/Analytics) increasingly choose to use tools like &lt;a href="https://cloud.google.com/bigquery/" target="_blank" rel="noopener">Google
BigQuery&lt;/a> or &lt;a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">AWS Redshift&lt;/a>.
Therefore, it doesn&amp;rsquo;t make sense to spin-up a Hadoop with the only intention to
use &lt;a href="https://hortonworks.com/apache/yarn/" target="_blank" rel="noopener">YARN&lt;/a> as the resources manager.&lt;/p>
&lt;p>An alternative is the use of Hadoop cluster providers such as &lt;a href="https://cloud.google.com/dataproc" target="_blank" rel="noopener">Google
DataProc&lt;/a> or &lt;a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener">AWS EMR&lt;/a>
for the creation of ephemeral clusters. Just to name a few options.&lt;/p>
&lt;p>To better understand the design of Spark Operator, the doc from &lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operatoR/blob/master/docs/design.md#the-crd-controller" target="_blank" rel="noopener">GCP on GitHub&lt;/a>
is a no-brainer.&lt;/p>
&lt;h1 id="lets-get-hands-on">Let&amp;rsquo;s get hands-on!&lt;/h1>
&lt;h2 id="warming-up-the-engine">Warming up the engine&lt;/h2>
&lt;p>Now that the word has been spread, let&amp;rsquo;s get our hands on it to show
the engine running. For that, let&amp;rsquo;s use:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.docker.com/" target="_blank" rel="noopener">Docker&lt;/a> as the container engine for
Kubernetes &lt;a href="https://docs.docker.com/install/" target="_blank" rel="noopener">(installation guide)&lt;/a>;&lt;/li>
&lt;li>Minikube &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">(installation guide)&lt;/a>
to facilitate the provisioning of the Kubernetes (yes, it will be
a local execution);&lt;/li>
&lt;li>For interaction with the Kubernetes API it is necessary to have
&lt;code>kubectl&lt;/code> installed, &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">if you don&amp;rsquo;t have it, follow instructions
here&lt;/a>.&lt;/li>
&lt;li>a compiled version of Apache Spark larger than 2.3.0.
&lt;ol>
&lt;li>you can either compile &lt;a href="https://github.com/apache/spark" target="_blank" rel="noopener">source code&lt;/a>,
which will took &lt;em>some hours&lt;/em> to finish, or&lt;/li>
&lt;li>download a compiled version &lt;a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">here&lt;/a>
(recommended).&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>Once the necessary tools are installed, it&amp;rsquo;s necessary to
include Apache Spark path in &lt;code>PATH&lt;/code> environment variable, to ease the
invocation of Apache Spark executables. Simply run:&lt;/p>
&lt;pre>&lt;code class="language-bash">export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin
&lt;/code>&lt;/pre>
&lt;h2 id="creating-the-minikube-cluster">Creating the Minikube &amp;ldquo;cluster&amp;rdquo;&lt;/h2>
&lt;p>At last, to have a Kubernetes &amp;ldquo;cluster&amp;rdquo; we will start a &lt;code>minikube&lt;/code>
with the intention of running an example from &lt;a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala" target="_blank" rel="noopener">Spark
repository&lt;/a>
called &lt;code>SparkPi&lt;/code> just as a demonstration.&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube start --cpus=2 \
--memory=4g
&lt;/code>&lt;/pre>
&lt;h2 id="building-the-docker-image">Building the Docker image&lt;/h2>
&lt;p>Let&amp;rsquo;s use the Minikube Docker daemon to not depend on an external registry (and
only generate Docker image layers on the VM, facilitating garbage disposal
later). Minikube has a wrapper that makes our life easier:&lt;/p>
&lt;pre>&lt;code class="language-bash">eval $(minikube docker-env)
&lt;/code>&lt;/pre>
&lt;p>After having the daemon environment variables configured, we need a
Docker image to run the jobs. There is a &lt;a href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh" target="_blank" rel="noopener">shell script in the Spark
repository&lt;/a>
to help with this. Considering that our &lt;code>PATH&lt;/code> was properly
configured, just run:&lt;/p>
&lt;pre>&lt;code class="language-bash">docker-image-tool.sh -m -t latest build
&lt;/code>&lt;/pre>
&lt;p>&lt;em>FYI:&lt;/em> The &lt;code>-m&lt;/code> parameter here indicates a minikube build.&lt;/p>
&lt;p>Let&amp;rsquo;s take the highway to execute SparkPi, using the same command
that would be used for a Hadoop Spark cluster &lt;a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">spark-submit&lt;/a>.&lt;/p>
&lt;p>However, Spark Operator supports defining jobs in the &amp;ldquo;Kubernetes
dialect&amp;rdquo; using &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">CRD&lt;/a>,
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples" target="_blank" rel="noopener">here are some examples&lt;/a> - for later.&lt;/p>
&lt;h1 id="fire-in-the-hole">Fire in the hole!&lt;/h1>
&lt;p>Mid the gap between the Scala version and .jar when you&amp;rsquo;re
parameterizing with your Apache Spark version:&lt;/p>
&lt;pre>&lt;code class="language-bash">spark-submit --master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=2 \
--executor-memory 1024m \
--conf spark.kubernetes.container.image=spark:latest \
local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # here
&lt;/code>&lt;/pre>
&lt;p>What&amp;rsquo;s new is:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--master&lt;/code>: Accepts a prefix &lt;code>k8s://&lt;/code> in the URL, for the
Kubernetes master API endpoint, exposed by the command
&lt;code>https://$(minikube ip):8443&lt;/code>. BTW, in case you want to
know, it&amp;rsquo;s a &lt;a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html" target="_blank" rel="noopener">shell command substitution&lt;/a>;&lt;/li>
&lt;li>&lt;code>--conf spark.kubernetes.container.image=&lt;/code>: Configures the Docker
image to run in Kubernetes.&lt;/li>
&lt;/ul>
&lt;p>Sample output:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&amp;gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&amp;gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
&lt;/code>&lt;/pre>
&lt;p>To see the job result (and the whole execution) we can run a
&lt;code>kubectl logs&lt;/code> passing the name of the driver pod as a parameter:&lt;/p>
&lt;pre>&lt;code class="language-bash">kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')
&lt;/code>&lt;/pre>
&lt;p>Which brings the output (omitted some entries), similar to:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
...
&lt;/code>&lt;/pre>
&lt;p>The result appears in:&lt;/p>
&lt;pre>&lt;code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
&lt;/code>&lt;/pre>
&lt;p>Finally, let&amp;rsquo;s delete the VM that Minikube generates, to clean up the
environment (unless you want to keep playing with it):&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube delete
&lt;/code>&lt;/pre>
&lt;h2 id="last-words">Last words&lt;/h2>
&lt;p>I hope your curiosity got &lt;em>sparked&lt;/em> and some ideas for further
development have raised for your Big Data workloads. If you have any
doubt or suggestion, don&amp;rsquo;t hesitate to share on the comment section.&lt;/p></description></item><item><title>ReclameAQUI Data Lake</title><link>https://macunha.me/en/project/reclameaqui-data-lake/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/reclameaqui-data-lake/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>ReclameAQUI (Portuguese for &amp;ldquo;complain here&amp;rdquo;) is an interesting and unique
business. They&amp;rsquo;re a content aggregator for customers' experience sharing
(especially bad experiences) about shopping (online and offline). However, it
goes further than a mere &amp;ldquo;complaints website&amp;rdquo; offering an interface for
companies to answers complaints, helping customers with their issues.&lt;/p>
&lt;p>The service is simply the biggest in this regard (worldwide) receiving 600K
unique visitors each day, searching for a company&amp;rsquo;s reputation before closing a
deal/purchase.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Even though they are already advanced in the digital approach to business,
having most services hosted on Cloud computing and analytical culture, their
data lake needed some upgrades. The most relevant motivator of this project was
the sky-high bills from GCP especially related to BigQuery data consumption.&lt;/p>
&lt;p>Apart from the cost-reduction tasks and data ingestion process optimization, we
took the opportunity to implement data cryptograph at-rest, governance, and
obfuscation during query executions against the data lake. Making data
accessible by everyone in the company, controlling identity access and
management through LDAP (auditing each access, to be fully compliant with
GDPR), we could offer a self-service data lake so different business actors
could satisfy their needs &amp;ldquo;drinking&amp;rdquo; from the lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="tech-implementation">Tech implementation&lt;/h2>
&lt;p>Key objectives were cost-optimization of the existing Data Lake, improvement
(and extension) of existing data ingestion pipelines, and security enhancements.&lt;/p>
&lt;p>Starting from Data Lake&amp;rsquo;s cost optimization, we redesigned the data ingestion,
using a &amp;ldquo;landing&amp;rdquo; area for raw data, making data transformations later to suit
the desired data models. Saving the results in other Data Lake layers to achieve
greater performance in queries.&lt;/p>
&lt;p>We shifted away from the Streaming inserts in BigQuery by adding a step to load
data at the end of the ingestion pipeline. Apache NiFi was the main software
responsible for orchestrating and executing the pipeline, covering also the
improvements in data ingestion through processes re-engineering.&lt;/p>
&lt;p>Auditing in the Data Lake was managed through Apache Ranger. In order to have
it fully supported we implemented a JDBC driver using a component from Apache
Calcite called Avatica. Authentication for Apache Ranger went through a custom
plugin (also developed during the project) for LDAP consuming user info from
Google Cloud Identity, reflecting the existing organization&amp;rsquo;s users and groups
from Google Suite.&lt;/p>
&lt;p>To make the game more interesting, we containerized the workflow and heavily
used Kubernetes (GKE) to manage these components. Most of the Apache projects
didn&amp;rsquo;t have Helm Charts at the time and we developed and made some
of them open-source.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>During project time we could measure an estimative of roughly 56% in Data Lake
cost-optimization through reengineering of processes and resources, especially
the removal of streaming inserts to BigQuery.&lt;/p>
&lt;p>We made relevant progress in security and governance during the project with the
introduction of Apache Ranger and Data Lake auditing for access and usage,
providing advanced security capabilities to ReclameAQUI, which anticipated itself
towards GDPR and data privacy concerns.&lt;/p></description></item><item><title>Dotz Data Labs</title><link>https://macunha.me/en/project/dotz-data-labs/</link><pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/dotz-data-labs/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>In order to be able to innovate and maintain itself in a highly changing
and evolving market, Dotz went through a process of digital
transformation and had the help of some consultants along the way.&lt;/p>
&lt;p>Among the steps to get closer to a digital model, the implementation of
a Data Lake emerged, with the requirements of being
&lt;a href="https://en.wikipedia.org/wiki/Serverless_computing" target="_blank" rel="noopener">serverless&lt;/a> and
cloud-native to support the decision-making process and shorten
time-to-market during the launch of products.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Dotz is one of the largest companies in the field of loyalty program in
Brazil and they&amp;rsquo;d face a high number of issues with data disconnection
making it difficult to analyze their users' behaviors. Since they
received data from numerous supermarkets and stores, it&amp;rsquo;s difficult to
clusterizate products, since the name is different depending on the
source. To help with this analysis, they decided to build a Data Lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>We built and deployed a Big Data's managed architecture using Google's
Cloud Platform (GCP) to support this strategy and allow a 360-degree
view of customers (users with points a.k.a. Dotz) and partners (the
supermarkets offering the loyalty program).&lt;/p>
&lt;p>The design was focused on cloud-managed services and serverless
computing offered by Google, serving the core competencies of a Data
Lake such as scalable storage using Google Cloud Storage, and Google
BigQuery. With part of the process running inside Kubernetes,
responsible for data cleansing ETL flow management.&lt;/p>
&lt;p>We streamed data with Apache Beam running under Google DataFlow,
parallel mass processing with Apache Spark jobs running on Google
DataProc, exploratory analysis with Google DataLab, Machine Learning
Analysis with Google ML and Data visualization in Google Data Studio.&lt;/p>
&lt;p>Data is transported using an event-driven model, where every data is
collected using a streaming model, even the ETL (which runs on a
micro-batch, to enable near real-time exploration). These data goes
through the data pipeline using Google Pub/Sub message-oriented
middleware, and every message is serialized using Avro format, which
reduces the payload and allows transportation to be cost-effective, fast
and reliable.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>It all allowed Dotz to have a better structure on their analytical
platform, previously managed inside a large MS SQL Server instance,
which moved to a Data Lake with layers allowing data categorization,
governance, quality and security.&lt;/p>
&lt;p>Supporting analytical processes of users data, faster exploration and
monetization of their knowledge on customers' behavior.&lt;/p></description></item><item><title>Easynvest Data Platform</title><link>https://macunha.me/en/project/easynvest-data-platform/</link><pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/easynvest-data-platform/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Within Easynvest&amp;rsquo;s annual planning, an investment in the expansion of
data &amp;amp; analytics team aimed at shortening decision making process and
in delivering a higher quality to customers through a low cost operational
process.&lt;/p>
&lt;p>Among the main objectives of this project, we had the automation of
credit analysis (executed during the approval of customer registration,
using Machine Learning), a process that until then was long and manual,
being handled by the back office.&lt;/p>
&lt;p>Followed by a better offer of products to the client, carrying out the
categorization according to the profile of each customer, allowing
suggestions of more attractive products, in line with personal preferences,
as well as according to the profile of each investor (conservative,
moderate or aggressive).&lt;/p>
&lt;p>Last but not least, the intelligent detection of money laundry and
reporting to the responsible authorities.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>However, there were limitations in the data tools, mainly due to the
fact that they were proprietary software (with limited licenses) and
designed for usage inside data centers. In addition, the analytical
database was modeled for traditional Business Intelligence models
(OLAP, etc), making the decision making process heavy, due to the
demanding amount of interactions during ETL.&lt;/p>
&lt;p>Previously for a client to be approved, the process took 10 to 15 days.
Gathering all necessary information, providing a complete perspective of
the profile, including credit analysis. After collecting the information,
the back office generated an internal credit analysis score.&lt;/p>
&lt;p>In most cases, the client was not notified of updates regarding the
process and did not receive feedback at the end (if refused) unless
explicitly requested (by contacting support via chat or email, for
example) which made the process time consuming and costly. Not to
mention the countless amounts of customers lost to the competition during
this long wait.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>To make it possible, we built an hybrid-cloud implementation using AWS
cloud-based components (mainly AWS S3, EMR and ECS), to extend the data
centers' capability, implementing a cloud-first Hadoop ecosystem
(replacing the proprietary software components with open-source
equivalents). Giving Easynvest the possibility to grow the Data Lake
exponentially.&lt;/p>
&lt;p>The Data Lake design was robust, aimed to handle the execution of heavy
analytical processes through Machine Learning models, with support for
data quality, metadata governance, information security and data serving
(data owners could share data with consumers from other areas within the
company, allowing to self-service their analytical data).&lt;/p>
&lt;p>A Chatbot was also used to reduce the operational load in the
environment, this bot is responsible for maintaining and updating
infrastructure components. From triggering deployments to generating
encryption keys on-demand for data security and governance. Implemented
with the Errbot framework for Python interacting with Slack.&lt;/p>
&lt;p>Going further, we implemented the best practices in DevOps, using Jenkins
as a tool for CI/CD of the developed components alongside with Ansible for
Configuration Management.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Thanks to the utilization of layers in the Data Lake and the
implementation of data pipelines, we were able to reduce the data
ingestion time by 78% and to include metadata and data catalog, in
addition to automating much of the work that used to be done manually.&lt;/p>
&lt;p>Thus bringing positive results, especially reducing the registration
approval time for consumers from roughly 10 days to 1 day. It also made
the data platform more democratic, providing relevant information that
facilitates the analysis of areas such as risk (credit analysis) and
support, without having to give up security.&lt;/p></description></item></channel></rss>