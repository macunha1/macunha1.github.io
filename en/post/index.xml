<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | It's me, Macunha!</title><link>https://macunha.me/en/post/</link><atom:link href="https://macunha.me/en/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 31 Mar 2021 00:00:00 +0200</lastBuildDate><image><url>https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_512x512_fill_lanczos_center_2.png</url><title>Posts</title><link>https://macunha.me/en/post/</link></image><item><title>Terraform Design Best Practices</title><link>https://macunha.me/en/post/2021/03/terraform-design-best-practices/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0200</pubDate><guid>https://macunha.me/en/post/2021/03/terraform-design-best-practices/</guid><description>&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>As someone who believes in empowering people and distributing power in order to
achieve higher outcomes I always felt that the best existing best-practices proposals
don&amp;rsquo;t touch some key aspects (IMHO) on code evolution and business structures.&lt;/p>
&lt;p>Therefore, this design document shall compose on the previous including some
self-service Ops and micro-services spice to the mix.&lt;/p>
&lt;p>On
&lt;a href="https://www.terraform-best-practices.com" target="_blank" rel="noopener">Terraform best practices&lt;/a> great insights on how to write code inside a
module is provided, e.g.
&lt;a href="https://www.terraform-best-practices.com/naming" target="_blank" rel="noopener">naming conventions&lt;/a>,
&lt;a href="https://www.terraform-best-practices.com/code-structure#getting-started-with-structuring-of-terraform-configurations" target="_blank" rel="noopener">Terraform file naming&lt;/a>.&lt;/p>
&lt;p>We can&amp;rsquo;t leave Terragrunt epic blog post unmentioned:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://blog.gruntwork.io/5-lessons-learned-from-writing-over-300-000-lines-of-infrastructure-code-36ba7fadeac1" target="_blank" rel="noopener">5 Lessons Learned From Writing Over 300,000 Lines of Infrastructure Code&lt;/a>;&lt;/li>
&lt;/ul>
&lt;p>As well as the
&lt;a href="https://terragrunt.gruntwork.io/docs/getting-started/quick-start/#promote-immutable-versioned-terraform-modules-across-environments" target="_blank" rel="noopener">Terragrunt documentation pointing&lt;/a> &amp;ldquo;one of the most important
lessons&amp;rdquo; is that:&lt;/p>
&lt;blockquote>
&lt;p>large modules should be considered harmful. That is, it is a Bad Idea to define
all of your environments (dev, stage, prod, etc), or even a large amount of
infrastructure (servers, databases, load balancers, DNS, etc), in a single
Terraform module. Large modules are slow, insecure, hard to update, hard to code
review, hard to test, and brittle (i.e., you have all your eggs in one basket).&lt;/p>
&lt;/blockquote>
&lt;h3 id="bad-idea-capitalized">&amp;ldquo;Bad Idea&amp;rdquo; capitalized!&lt;/h3>
&lt;p>Which is totally true, as this &amp;ldquo;Bad Idea&amp;rdquo; usually coming from a lack of care
towards Terraform code design tend to be harmful in the long run, with a
tendency towards making the implementation a
&lt;a href="https://en.wikipedia.org/wiki/Big%5Fball%5Fof%5Fmud" target="_blank" rel="noopener">big ball of mud&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>A Big Ball of Mud is a haphazardly structured, sprawling, sloppy,
duct-tape-and-baling-wire, spaghetti-code jungle. These systems show
unmistakable signs of unregulated growth, and repeated, expedient repair.
Information is shared promiscuously among distant elements of the system,
often to the point where nearly all the important information becomes global
or duplicated.&lt;/p>
&lt;/blockquote>
&lt;!--quoteend-->
&lt;blockquote>
&lt;p>The overall structure of the system may never have been well defined.&lt;/p>
&lt;/blockquote>
&lt;p>Oftentimes, Terraform code implementation fluctuate towards mono-repositories
(a.k.a. monorepos) containing all the specification in a single place. In order
to tame the chaos, the Terraform state needs to be at least sub-divided into
logical sections.&lt;/p>
&lt;h2 id="design">Design&lt;/h2>
&lt;h3 id="shallow-tree-of-shared-resources">Shallow &amp;ldquo;tree&amp;rdquo; of shared resources&lt;/h3>
&lt;p>Following the
&lt;a href="https://www.terraform-best-practices.com/code-structure#common-recommendations-for-structuring-code" target="_blank" rel="noopener">recommendations for structuring code&lt;/a> one of the proposals is to
keep a shallow &amp;ldquo;tree&amp;rdquo; of resources and modules. This tree produces a small and
clear distribution of Terraform code.&lt;/p>
&lt;p>Why a shallow &amp;ldquo;tree&amp;rdquo; of resources? It helps achieving a short amount of
resources and modules that result in a small
&lt;a href="https://www.terraform.io/docs/language/state/remote.html" target="_blank" rel="noopener">remote state&lt;/a> file. With a small
remote state we speed-up the development process and reduce waste (&lt;em>Muda&lt;/em> in the
Toyota 3M model), as the shallow tree enables faster executions of Terraform
(less data to sync and compare).&lt;/p>
&lt;p>The granularity level will be defined for each specific case (no silver bullet)
balancing the smallest and most feasible composition possible.&lt;/p>
&lt;h3 id="product-areas--a-dot-k-dot-a-dot-business-capabilities--structure-and-ownership">Product areas (a.k.a. Business capabilities) structure and ownership&lt;/h3>
&lt;p>Ideally, the
&lt;a href="https://www.terraform-best-practices.com/key-concepts#composition" target="_blank" rel="noopener">composition level&lt;/a> would be organized around Product Areas (either
squads/crews or guilds) with a fallback to shared technologies (e.g. vpc,
databases). Therefore, Terraform compositions are designed around what Martin
Fowler
&lt;a href="https://youtu.be/wgdBVIX9ifA?t=388" target="_blank" rel="noopener">calls &amp;ldquo;Business capabilities&amp;rdquo;&lt;/a> in micro-services terminology, ideally the
Terraform composition will follow the organizational structure so that each team
&amp;ldquo;owns&amp;rdquo; (in both senses: ownership and freedom) its own state.&lt;/p>
&lt;p>The main goal here is to structure the Terraform code as a reflection of the
organization so that is fosters self-service Ops. If the Infrastructure as Code
is mature enough to the point of having well-described Terraform modules,
everyone should be empowered to define these modules by setting the parameters
according to their needs, without centralizing power on a Operations team.&lt;/p>
&lt;p>The resource composition must gravitate towards the following (ordered by
priority from higher to lower):&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Product Areas (ownership) directory structure:&lt;/p>
&lt;ol>
&lt;li>squad/crew OR guild;&lt;/li>
&lt;li>product.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Shared resources, around technologies.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Looking on the structure from bottom-up it starts from the product and then
attributes the product to a crew through the directory tree.&lt;/p>
&lt;p>e.g.:&lt;/p>
&lt;pre>&lt;code class="language-text"># Squad or Crew
red-team
└── payment # Product (i.e. micro-service) name
└── main.tf # Any resource used by the payment product
# Guild (organized around technology)
back-end
└─ monolith # Shared application in terms of ownership
   └── main.tf # Cloud resources used by the monolith
&lt;/code>&lt;/pre>
&lt;p>On the example above, we can&amp;rsquo;t ignore that &lt;code>monolith&lt;/code> is a product with shared
ownership among back-end developers and therefore it is organized to follow the
business structure.&lt;/p>
&lt;p>The structure is inspired
&lt;a href="https://terragrunt.gruntwork.io/docs/getting-started/quick-start/#promote-immutable-versioned-terraform-modules-across-environments" target="_blank" rel="noopener">on Terragrunt&amp;rsquo;s best-practices&lt;/a> to some extend.
However, it distinct from Terragrunt proposal in the way resources are divided,
rather than organizing resources exclusively around technologies.&lt;/p>
&lt;h3 id="shared-resources-organized-around-technologies">Shared resources, organized around technologies&lt;/h3>
&lt;p>Oftentimes in organizations we will face shared resources among products, there
is no way around reality. e.g. a shared VPC or SQL database.&lt;/p>
&lt;p>However, these situations should be the exception and not the norm. Dealt
similar to the organization of Terraform compositions around
guilds/technologies.&lt;/p>
&lt;pre>&lt;code class="language-text">platform # as in Platform Engineering
└── vpc
└── main.tf
back-end
└── database
└── main.tf
&lt;/code>&lt;/pre>
&lt;h3 id="files-inside-the-composition">Files inside the composition?&lt;/h3>
&lt;p>Ideally the files in the sub-directory (which specify the composition) are going
to partially
&lt;a href="https://www.terraform-best-practices.com/code-structure#getting-started-with-structuring-of-terraform-configurations" target="_blank" rel="noopener">follow this spec&lt;/a> and include &lt;code>data.tf&lt;/code>, &lt;code>terraform.tf&lt;/code> and
&lt;code>providers.tf&lt;/code> on top of that.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>main.tf:&lt;/strong> contains locals, module and resource definitions;&lt;/li>
&lt;li>&lt;strong>variables.tf:&lt;/strong> contains declarations of variables (i.e. inputs/parameters)
used in main.tf;&lt;/li>
&lt;li>&lt;strong>data.tf:&lt;/strong> contains data-resources for input data used in main.tf;&lt;/li>
&lt;li>&lt;strong>outputs.tf:&lt;/strong> contains outputs from the resources created in main.tf;&lt;/li>
&lt;li>&lt;strong>providers.tf:&lt;/strong> contains provider and provider&amp;rsquo;s versions definitions;&lt;/li>
&lt;li>&lt;strong>terraform.tf:&lt;/strong> contains the terraform back-end (e.g. remote state)
definition;&lt;/li>
&lt;/ul>
&lt;h3 id="what-about-terraform-modules">What about Terraform modules?&lt;/h3>
&lt;p>
&lt;a href="https://www.terraform.io/docs/language/modules/index.html" target="_blank" rel="noopener">Terraform modules&lt;/a> are containers for multiple resources that are used together
to achieve a shared goal. Modules can be used to create lightweight
abstractions, facilitating reusability and distribution of Terraform code.&lt;/p>
&lt;p>Therefore, we assume that the following are anti-patterns that make Terraform
modules' reusability difficult:&lt;/p>
&lt;ul>
&lt;li>Configuration of Terraform Providers inside a module;&lt;/li>
&lt;li>Implementation of Business logic and/or hard-coded parameters in a
module;&lt;/li>
&lt;li>Default values are specified in optional variables instead of
hard-coding;&lt;/li>
&lt;li>Modules should be self-contained and provide a clear contract.
Dependencies (pre-existing resources) must be specified through required
variables.&lt;/li>
&lt;li>Modules must serve to a singular purpose. Multiple purpose must be
achieved through composability of modules and not by &amp;ldquo;monolithic&amp;rdquo; modules.&lt;/li>
&lt;/ul>
&lt;p>Modules are abstractions that should be used to reduce the amount of code
duplication, implementing the
&lt;a href="https://en.wikipedia.org/wiki/Don%27t%5Frepeat%5Fyourself" target="_blank" rel="noopener">DRY (don&amp;rsquo;t repeat yourself) principle&lt;/a>.&lt;/p>
&lt;p>On top of that, modules are an important factor to reduce the parity among
environments, which helps to better address the
&lt;a href="https://12factor.net/" target="_blank" rel="noopener">Twelve-Factor App model&lt;/a> in
regards to
&lt;a href="https://12factor.net/dev-prod-parity" target="_blank" rel="noopener">Factor X (ten)&lt;/a>.&lt;/p></description></item><item><title>Quickstart: Apache Spark on Kubernetes</title><link>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</link><pubDate>Thu, 21 May 2020 23:00:57 +0200</pubDate><guid>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="the-apache-spark-operator-for-kubernetes">The Apache Spark Operator for Kubernetes&lt;/h2>
&lt;p>Since its launch in 2014 by Google, Kubernetes has gained a lot of
popularity along with Docker itself and since 2016 has become the &lt;em>de
facto Container Orchestrator&lt;/em>, established as a market standard.
Having cloud-managed versions available in &lt;strong>all&lt;/strong> the &lt;em>major Clouds&lt;/em>.
&lt;a href="https://cloud.google.com/kubernetes-engine/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://docs.microsoft.com/en-us/azure/aks/" target="_blank" rel="noopener">[3]&lt;/a> (including
&lt;a href="https://www.digitalocean.com/products/kubernetes/" target="_blank" rel="noopener">Digital Ocean&lt;/a> and
&lt;a href="https://www.alibabacloud.com/product/kubernetes" target="_blank" rel="noopener">Alibaba&lt;/a>).&lt;/p>
&lt;p>With this popularity came various implementations and &lt;em>use-cases&lt;/em> of
the orchestrator, among them the execution of
&lt;a href="https://kubernetes.io/docs/tutorials/stateful-application/" target="_blank" rel="noopener">Stateful
applications&lt;/a>
including
&lt;a href="https://vitess.io/zh/docs/get-started/kubernetes/" target="_blank" rel="noopener">databases using containers&lt;/a>.&lt;/p>
&lt;p>What would be the motivation to host an orchestrated database? That&amp;rsquo;s
a great question. But let&amp;rsquo;s focus on the
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md" target="_blank" rel="noopener">Spark Operator&lt;/a>
running workloads on Kubernetes.&lt;/p>
&lt;p>A native Spark Operator
&lt;a href="https://github.com/kubernetes/kubernetes/issues/34377" target="_blank" rel="noopener">idea came out&lt;/a>
in 2016, before that you couldn&amp;rsquo;t run Spark jobs natively except
some &lt;em>hacky alternatives&lt;/em>, like
&lt;a href="https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/" target="_blank" rel="noopener">running Apache Zeppelin&lt;/a>
inside Kubernetes or creating your
&lt;a href="https://github.com/kubernetes/examples/tree/master/staging/spark" target="_blank" rel="noopener">Apache Spark cluster inside
Kubernetes (from the official Kubernetes organization on GitHub)&lt;/a>
referencing the
&lt;a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Spark workers in Stand-alone mode&lt;/a>.&lt;/p>
&lt;p>However, the native execution would be far more interesting for taking
advantage of
&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank" rel="noopener">Kubernetes Scheduler&lt;/a>
responsible for taking action of allocating resources, giving
elasticity and an simpler interface to manage Apache Spark workloads.&lt;/p>
&lt;p>Considering that,
&lt;a href="https://issues.apache.org/jira/browse/SPARK-18278" target="_blank" rel="noopener">Apache Spark Operator development got attention&lt;/a>,
merged and released into
&lt;a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">Spark version 2.3.0&lt;/a>
launched in
&lt;a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">February, 2018&lt;/a>.&lt;/p>
&lt;p>If you&amp;rsquo;re eager for reading more regarding the Apache Spark proposal,
you can head to the
&lt;a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target="_blank" rel="noopener">design document published in Google Docs.&lt;/a>&lt;/p>
&lt;h2 id="why-kubernetes">Why Kubernetes?&lt;/h2>
&lt;p>As companies are currently seeking to
&lt;a href="https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html" target="_blank" rel="noopener">reinvent themselves through the
widely spoken digital transformation&lt;/a>
in order for them to be competitive and, above all, to survive in an
increasingly dynamic market, it is common to see approaches that
include Big Data, Artificial Intelligence and Cloud Computing
&lt;a href="https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/" target="_blank" rel="noopener">[3]&lt;/a>.&lt;/p>
&lt;p>An interesting comparison between the benefits of using Cloud Computing in the
context of Big Data instead of On-premises' servers can be read at
&lt;a href="https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html" target="_blank" rel="noopener">Databricks
blog&lt;/a>,
which is the company
&lt;a href="https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/" target="_blank" rel="noopener">founded by the creators of Apache Spark&lt;/a>.&lt;/p>
&lt;p>As we see a widespread adoption of Cloud Computing (even by companies
that would be able to afford the hardware and run on-premises), we
notice that most of these Cloud implementations don&amp;rsquo;t have an
&lt;a href="https://hadoop.apache.org/" target="_blank" rel="noopener">Apache
Hadoop&lt;/a> since the Data Teams (BI/Data
Science/Analytics) increasingly choose to use tools like
&lt;a href="https://cloud.google.com/bigquery/" target="_blank" rel="noopener">Google
BigQuery&lt;/a> or
&lt;a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">AWS Redshift&lt;/a>.
Therefore, it doesn&amp;rsquo;t make sense to spin-up a Hadoop with the only intention to
use
&lt;a href="https://hortonworks.com/apache/yarn/" target="_blank" rel="noopener">YARN&lt;/a> as the resources manager.&lt;/p>
&lt;p>An alternative is the use of Hadoop cluster providers such as
&lt;a href="https://cloud.google.com/dataproc" target="_blank" rel="noopener">Google
DataProc&lt;/a> or
&lt;a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener">AWS EMR&lt;/a>
for the creation of ephemeral clusters. Just to name a few options.&lt;/p>
&lt;p>To better understand the design of Spark Operator, the doc from
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operatoR/blob/master/docs/design.md#the-crd-controller" target="_blank" rel="noopener">GCP on GitHub&lt;/a>
is a no-brainer.&lt;/p>
&lt;h1 id="lets-get-hands-on">Let&amp;rsquo;s get hands-on!&lt;/h1>
&lt;h2 id="warming-up-the-engine">Warming up the engine&lt;/h2>
&lt;p>Now that the word has been spread, let&amp;rsquo;s get our hands on it to show
the engine running. For that, let&amp;rsquo;s use:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://www.docker.com/" target="_blank" rel="noopener">Docker&lt;/a> as the container engine for
Kubernetes
&lt;a href="https://docs.docker.com/install/" target="_blank" rel="noopener">(installation guide)&lt;/a>;&lt;/li>
&lt;li>Minikube
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">(installation guide)&lt;/a>
to facilitate the provisioning of the Kubernetes (yes, it will be
a local execution);&lt;/li>
&lt;li>For interaction with the Kubernetes API it is necessary to have
&lt;code>kubectl&lt;/code> installed,
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">if you don&amp;rsquo;t have it, follow instructions
here&lt;/a>.&lt;/li>
&lt;li>a compiled version of Apache Spark larger than 2.3.0.
&lt;ol>
&lt;li>you can either compile
&lt;a href="https://github.com/apache/spark" target="_blank" rel="noopener">source code&lt;/a>,
which will took &lt;em>some hours&lt;/em> to finish, or&lt;/li>
&lt;li>download a compiled version
&lt;a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">here&lt;/a>
(recommended).&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>Once the necessary tools are installed, it&amp;rsquo;s necessary to
include Apache Spark path in &lt;code>PATH&lt;/code> environment variable, to ease the
invocation of Apache Spark executables. Simply run:&lt;/p>
&lt;pre>&lt;code class="language-bash">export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin
&lt;/code>&lt;/pre>
&lt;h2 id="creating-the-minikube-cluster">Creating the Minikube &amp;ldquo;cluster&amp;rdquo;&lt;/h2>
&lt;p>At last, to have a Kubernetes &amp;ldquo;cluster&amp;rdquo; we will start a &lt;code>minikube&lt;/code>
with the intention of running an example from
&lt;a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala" target="_blank" rel="noopener">Spark
repository&lt;/a>
called &lt;code>SparkPi&lt;/code> just as a demonstration.&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube start --cpus=2 \
--memory=4g
&lt;/code>&lt;/pre>
&lt;h2 id="building-the-docker-image">Building the Docker image&lt;/h2>
&lt;p>Let&amp;rsquo;s use the Minikube Docker daemon to not depend on an external registry (and
only generate Docker image layers on the VM, facilitating garbage disposal
later). Minikube has a wrapper that makes our life easier:&lt;/p>
&lt;pre>&lt;code class="language-bash">eval $(minikube docker-env)
&lt;/code>&lt;/pre>
&lt;p>After having the daemon environment variables configured, we need a
Docker image to run the jobs. There is a
&lt;a href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh" target="_blank" rel="noopener">shell script in the Spark
repository&lt;/a>
to help with this. Considering that our &lt;code>PATH&lt;/code> was properly
configured, just run:&lt;/p>
&lt;pre>&lt;code class="language-bash">docker-image-tool.sh -m -t latest build
&lt;/code>&lt;/pre>
&lt;p>&lt;em>FYI:&lt;/em> The &lt;code>-m&lt;/code> parameter here indicates a minikube build.&lt;/p>
&lt;p>Let&amp;rsquo;s take the highway to execute SparkPi, using the same command
that would be used for a Hadoop Spark cluster
&lt;a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">spark-submit&lt;/a>.&lt;/p>
&lt;p>However, Spark Operator supports defining jobs in the &amp;ldquo;Kubernetes
dialect&amp;rdquo; using
&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">CRD&lt;/a>,
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples" target="_blank" rel="noopener">here are some examples&lt;/a> - for later.&lt;/p>
&lt;h1 id="fire-in-the-hole">Fire in the hole!&lt;/h1>
&lt;p>Mid the gap between the Scala version and .jar when you&amp;rsquo;re
parameterizing with your Apache Spark version:&lt;/p>
&lt;pre>&lt;code class="language-bash">spark-submit --master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=2 \
--executor-memory 1024m \
--conf spark.kubernetes.container.image=spark:latest \
local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # here
&lt;/code>&lt;/pre>
&lt;p>What&amp;rsquo;s new is:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--master&lt;/code>: Accepts a prefix &lt;code>k8s://&lt;/code> in the URL, for the
Kubernetes master API endpoint, exposed by the command
&lt;code>https://$(minikube ip):8443&lt;/code>. BTW, in case you want to
know, it&amp;rsquo;s a
&lt;a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html" target="_blank" rel="noopener">shell command substitution&lt;/a>;&lt;/li>
&lt;li>&lt;code>--conf spark.kubernetes.container.image=&lt;/code>: Configures the Docker
image to run in Kubernetes.&lt;/li>
&lt;/ul>
&lt;p>Sample output:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&amp;gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&amp;gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
&lt;/code>&lt;/pre>
&lt;p>To see the job result (and the whole execution) we can run a
&lt;code>kubectl logs&lt;/code> passing the name of the driver pod as a parameter:&lt;/p>
&lt;pre>&lt;code class="language-bash">kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')
&lt;/code>&lt;/pre>
&lt;p>Which brings the output (omitted some entries), similar to:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
...
&lt;/code>&lt;/pre>
&lt;p>The result appears in:&lt;/p>
&lt;pre>&lt;code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
&lt;/code>&lt;/pre>
&lt;p>Finally, let&amp;rsquo;s delete the VM that Minikube generates, to clean up the
environment (unless you want to keep playing with it):&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube delete
&lt;/code>&lt;/pre>
&lt;h2 id="last-words">Last words&lt;/h2>
&lt;p>I hope your curiosity got &lt;em>sparked&lt;/em> and some ideas for further
development have raised for your Big Data workloads. If you have any
doubt or suggestion, don&amp;rsquo;t hesitate to share on the comment section.&lt;/p></description></item><item><title>DevOps: Benefits</title><link>https://macunha.me/en/post/2019/01/devops-benefits/</link><pubDate>Fri, 11 Jan 2019 22:00:00 +0000</pubDate><guid>https://macunha.me/en/post/2019/01/devops-benefits/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Main benefits that a company generally expects and finds in the adoption of
culture:&lt;/p>
&lt;h2 id="faster-and-cheaper-releases">Faster and Cheaper Releases&lt;/h2>
&lt;p>Since releases will be continuous and frequent, deliverables will turn into
small changes with the benefit of increasing speed in the development cycle
(delivering always).&lt;/p>
&lt;h2 id="improved-operational-support-with-quick-fixed">Improved Operational support with quick fixed&lt;/h2>
&lt;p>If there is a failure during delivery, the impact is minimal because the amount
of modifications is small, just as the rollback is faster. Having a simple
inspection and debugging.&lt;/p>
&lt;h2 id="better-time-to-market-ttm">Better Time-to-market (TTM)&lt;/h2>
&lt;p>The software will be delivered much earlier when it&amp;rsquo;s still an MVP. Customers
will be integrated as part of the development process, bringing insights and
feedback to the development team. Thus allowing for a higher launch speed in the
market.&lt;/p>
&lt;h2 id="superior-quality-products">Superior quality products&lt;/h2>
&lt;p>As has been said before, early failures prevent defects from being delivered to
production, because:&lt;/p>
&lt;ul>
&lt;li>Reduces the volume of defects in the product as a whole;&lt;/li>
&lt;li>Increases frequency of new features and releases;&lt;/li>
&lt;li>Appropriate development processes in teams, including automation.&lt;/li>
&lt;/ul>
&lt;h1 id="now-we-understood-why-lets-talk-about-how">Now we understood WHY, let&amp;rsquo;s talk about HOW&lt;/h1>
&lt;h2 id="continuous-releases-integration-delivery-deployment">Continuous releases (integration, delivery, deployment)&lt;/h2>
&lt;p>Usually follows a code versioning approach (through Git) using specific branches
for each environment (e.g.: feature branches with git flow).&lt;/p>
&lt;h2 id="continuous-integration">Continuous integration&lt;/h2>
&lt;p>Automatic execution of unit tests, integration tests and code quality analysis
against a git branch, to ensure that there was no disruption of the modified
piece of code.&lt;/p>
&lt;h2 id="continuous-delivery">Continuous delivery&lt;/h2>
&lt;p>Packaging the software that is tested and approved, to deliver it somewhere that
it is possible to use in a deploy later. Examples are libs delivered in
repositories to be integrated into the code during the next update and code
deploy.&lt;/p>
&lt;h2 id="continuous-deployment">Continuous deployment&lt;/h2>
&lt;p>Once you have completed all of the above steps, you can do automated deployments
right in the environments, when the team is more confident about the tools they
are testing, as well as the risk they&amp;rsquo;re taking and also understanding that
there is a possibility of failure in a tests environment without worrying that
it&amp;rsquo;s going to be divergent from production.&lt;/p>
&lt;h2 id="configuration-andor-infrastructure-as-code">Configuration (and/or Infrastructure) as code&lt;/h2>
&lt;p>To be able to test software with assertiveness, and to understand that it will
transit between environments without changing behavior, it is essential that the
configurations are also expressed in code. This allows the settings to be also
versioned, following the code. Also guaranteeing a uniformity among the
environments, which enables:&lt;/p>
&lt;ul>
&lt;li>Reduction in maintenance costs, having a single point to look at and
understand the operation of the system;&lt;/li>
&lt;li>Easy to recreate the infrastructure, if it is necessary to move everything to
another place, this can happen with a few manual interactions;&lt;/li>
&lt;li>Allows for a code review of infrastructure and configurations, which
consequently brings a culture of collaboration in the development, sharing of
knowledge and increases the democratization of the infra;&lt;/li>
&lt;li>Documentation as code, helping new team members get a faster warm up.&lt;/li>
&lt;/ul>
&lt;p>These points were well-stressed by the Heroku team and gave rise to the famous
paper:
&lt;a href="https://12factor.net/" target="_blank" rel="noopener">The Twelve-Factor App&lt;/a>. It&amp;rsquo;s an excellent reading
for the explanation of the benefits of configuration management.&lt;/p>
&lt;h2 id="observability-monitoring-and-self-healing">Observability, Monitoring, and self-healing&lt;/h2>
&lt;p>At the end of the delivery process, the software must be monitored. Avoiding to
wait for an external report of failures, ensuring that the actions are proactive
rather than reactive.&lt;/p>
&lt;p>With mature monitoring, it&amp;rsquo;s possible to create trigger against alerts, creating
a self-healing system in which actions (scripts) are performed to &lt;strong>fix known&lt;/strong>
failures in the infrastructure so that everyone can sleep peacefully at night,
without having to worry about the on-call schedule that makes you read some
documentation at dawn. (If you have had experience with this, you know for sure
how bad it is).&lt;/p>
&lt;p>Scaling up only those cases that are extreme exceptions (mistakes not
known/expected) in the process for the employee to act, ensuring higher health
in operation.&lt;/p>
&lt;h2 id="processes-automation">Processes automation&lt;/h2>
&lt;p>All processes that cause Muda should be addressed with automation, allowing
people to work more quickly. Good examples of processes that are usually
automated are:&lt;/p>
&lt;ul>
&lt;li>Deployment;&lt;/li>
&lt;li>Self-healing (system resilience in response to anomalies);&lt;/li>
&lt;li>Renewal of Certificates;&lt;/li>
&lt;li>Execution of tests (unitary, integration, functional, etc.);&lt;/li>
&lt;li>Monitoring (with auto-discovery);&lt;/li>
&lt;li>User Governance;&lt;/li>
&lt;/ul>
&lt;h1 id="devops-toolchainhttpsenwikipediaorgwikidevops_toolchain">
&lt;a href="https://en.wikipedia.org/wiki/DevOps_toolchain" target="_blank" rel="noopener">DevOps toolchain&lt;/a>&lt;/h1>
&lt;p>A combination of tools to facilitate the maintenance and operation of the
system, with the flow:&lt;/p>
&lt;p>&lt;img src="https://macunha.me/img/content/devops-lifecycle.png" alt="Development Cycle Using DevOps">&lt;/p>
&lt;blockquote>
&lt;p>Note: Any similarity to the PDCA is pure certainty.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Plan&lt;/strong>: Project planning phase, in which feedbacks are collected for
requirements survey, and backlog creation;&lt;/li>
&lt;li>&lt;strong>Create&lt;/strong>: Creation of a deliverable (to validate a hypothesis), such as an
MVP;&lt;/li>
&lt;li>&lt;strong>Verify&lt;/strong>: Pass the deliverable to the test phase;&lt;/li>
&lt;li>&lt;strong>Package:&lt;/strong> Package the build to be able to put it in some testing
environment;&lt;/li>
&lt;li>&lt;strong>Release&lt;/strong>: Deploy packaged deliverable;&lt;/li>
&lt;li>&lt;strong>Configure&lt;/strong>: Perform the configuration of the deliverable in the testing
environment, trying to get as close as possible to the twelve-factor app.&lt;/li>
&lt;li>&lt;strong>Monitor&lt;/strong>: After deploying to the environment, track business metrics and
infrastructure to ensure everything is working as expected.&lt;/li>
&lt;/ul>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>During the implementation of these techniques it is possible to observe
improvements in the development process, the most notable gains are:&lt;/p>
&lt;ul>
&lt;li>Increase in team engagement;&lt;/li>
&lt;li>Knowledge sharing;&lt;/li>
&lt;li>Reduction of bottlenecks;&lt;/li>
&lt;li>More free time to do work that really matters (adds value to the user
experience or generates impact);&lt;/li>
&lt;li>Greater confidence in delivering software.&lt;/li>
&lt;/ul></description></item><item><title>DevOps: The Genesis</title><link>https://macunha.me/en/post/2019/01/devops-genesis/</link><pubDate>Fri, 11 Jan 2019 21:00:00 +0000</pubDate><guid>https://macunha.me/en/post/2019/01/devops-genesis/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>First of all, it&amp;rsquo;s all about agile.&lt;/p>
&lt;p>The DevOps methodology was created on top of agile methods, to deliver a higher
value inside software releases, automating feature release through pipelines,
that can test hypothesis faster allowing higher adaptability using &amp;ldquo;fail-fast&amp;rdquo;
approaches. Those changes are more cultural than technical, so it&amp;rsquo;s normal to
see DevOps being called culture.&lt;/p>
&lt;p>The implementation of DevOps happens through processes automation, having a
strong sense of processes re-engineering inside the company. Comparing to the
cultural change, the technical is easy to implement. Therefore the role that a
&amp;ldquo;DevOps Engineer/Analyst&amp;rdquo; performs is very confusing, which enables many
SysAdmins and Infra Analysts assuming the role of &amp;ldquo;DevOps.&amp;rdquo;&lt;/p>
&lt;h2 id="lean-is-the-basis-of-agile">Lean is the basis of Agile&lt;/h2>
&lt;p>Reality is not as happy as it sounds. After World War II, Japan was destroyed
and under-resourced after losing the war. With a limited amount of resources,
the country needed to reinvent itself and survive after a time of severe
depression. During that time two guys gained attention inside a company that
later gave its name after the methodology.&lt;/p>
&lt;p>Those guys were Eiji Toyoda and Taiichi Ohno, inside Toyota Motor Corporation.
They&amp;rsquo;re the founders of the &amp;ldquo;Toyota production model&amp;rdquo; also known as Toyotism.&lt;/p>
&lt;h2 id="toyota-gave-birth-to-lean">Toyota gave birth to Lean&lt;/h2>
&lt;p>Lean teaches how to optimize the end-to-end process, focusing on processes that
create value for customers. Bottlenecks in the process must be removed, and
wasteful activities need to be identified and avoided. Both explained and
defined by LEAN 3M: Muda, Mura, and Muri.&lt;/p>
&lt;p>Also teaches to improve yourself day after day and always focus on quality
through Kaizen (continuous improvement).&lt;/p>
&lt;p>Japanese culture truly believes that quality is the main objective to deliver
value to customers since quality is what brings your clients back.&lt;/p>
&lt;h2 id="kaizen">Kaizen&lt;/h2>
&lt;p>A mindset that helps to look at each part of the process exclusively and think
about the improvements. Involving the people who are part of the process,
encourage the inclusion of these people in the decisions of change, since:&lt;/p>
&lt;ul>
&lt;li>It is much easier to accept a change when it is not imposed (top-down);&lt;/li>
&lt;li>There is a greater absorption of change by people when they&amp;rsquo;re included in the
planning;&lt;/li>
&lt;li>The people who are involved in the process bring their concerns and
suggestions, which contribute positively to the evolution of the change,
making the idea more robust.&lt;/li>
&lt;/ul>
&lt;p>The process of defining improvements through Kaizen happens (usually) in the
following order:&lt;/p>
&lt;ol>
&lt;li>Define data-driven objectives;&lt;/li>
&lt;li>Review the current state and develop an improvement plan;&lt;/li>
&lt;li>Implement improvement;&lt;/li>
&lt;li>Review the implementation and improve what does not work;&lt;/li>
&lt;li>Report the results and determine the items to be monitored.&lt;/li>
&lt;/ol>
&lt;p>This process is also called &lt;strong>PDCA: Plain-Do-Control-Act&lt;/strong>, which is summarized
in:&lt;/p>
&lt;ul>
&lt;li>Plan (develop the hypothesis);&lt;/li>
&lt;li>Do (experiment);&lt;/li>
&lt;li>Check (validate results);&lt;/li>
&lt;li>Act (refine the experiment and start over).&lt;/li>
&lt;/ul>
&lt;h1 id="3m-muda-mura-muri">3M: Muda, Mura, Muri&lt;/h1>
&lt;h2 id="muda-waste">Muda (waste)&lt;/h2>
&lt;p>Any activity that consumes time without adding value to the final consumer. e.g.:&lt;/p>
&lt;ul>
&lt;li>over-production;&lt;/li>
&lt;li>idle time in the process;&lt;/li>
&lt;li>products with a defect.&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s important to remember that there are different levels of Muda that can be
removed quickly or not, and the classification depends on the time for removal.&lt;/p>
&lt;p>An example of a more time-consuming Muda is the discontinuation of legacy
software that ends up with longer release cycles, causing teams to be idle,
followed by an often long or manual test routine.&lt;/p>
&lt;h2 id="mura-unevenness">Mura (unevenness)&lt;/h2>
&lt;p>Unevenness in operation, caused by activities that are very changeable and
unpredictable, generating different results in all executions. e.g., the
execution of tasks that were not well planned and ended up arriving with strict
deadlines. The team runs in the rush, generating exhaustion, despair, and
moreover, when finished leaves the people who have performed these tasks waiting
(for feedback, or confirmation that it is completed).&lt;/p>
&lt;h2 id="muri-overload">Muri (overload)&lt;/h2>
&lt;p>Overburdening equipment or operators by requiring them to run at a higher or
harder pace beyond the limit, to achieve some goal or expectation, causing
fatigue and consequently failures during the process. These failures are usually
human errors caused by fatigue during overwork.&lt;/p>
&lt;h2 id="back-to-agile">Back to Agile&lt;/h2>
&lt;p>In 2000 a group of 17 people met at a resort in Oregon to talk about ideas that
could improve the flow of software development. After a year of mature ideas,
these people met again and published the ideas, which we now know as &lt;strong>Agile
Manifesto&lt;/strong>.&lt;/p>
&lt;p>Main points are:&lt;/p>
&lt;p>&lt;strong>Individuals and interactions&lt;/strong> over processes and tools &lt;strong>Working software&lt;/strong>
over comprehensive documentation &lt;strong>Customer collaboration&lt;/strong> over contract
negotiation &lt;strong>Responding to change&lt;/strong> over following a plan&lt;/p>
&lt;p>I will restrict the explanation of these points with the DevOps point of view,
keeping on track (now).&lt;/p>
&lt;h2 id="individuals-and-interactions">Individuals and interactions&lt;/h2>
&lt;p>&lt;em>over processes and tools&lt;/em>&lt;/p>
&lt;p>First comes the individuals, they should receive the necessary tooling to work
with, and then be empowered to do their jobs. Interactions between people are
greatly encouraged, for sharing knowledge and also for facilitating creative
flow within development teams.&lt;/p>
&lt;p>An excellent example of interaction encouraged through DevOps is the code review
habit. Considering that small parts of the software will be iterated and
approved in the pipeline passing through different environments, automatically,
the best way to prevent defects is through code review.&lt;/p>
&lt;p>This habit brings benefits such as:&lt;/p>
&lt;ul>
&lt;li>Knowledge sharing;&lt;/li>
&lt;li>Observation of the problem from a different point of view;&lt;/li>
&lt;li>Team engagement;&lt;/li>
&lt;li>Lesser bugs.&lt;/li>
&lt;/ul>
&lt;h2 id="working-software">Working software&lt;/h2>
&lt;p>&lt;em>over comprehensive documentation&lt;/em>&lt;/p>
&lt;p>Here&amp;rsquo;s a trick in &amp;ldquo;working software,&amp;rdquo; software that works is not code that
compiles. The software that works is what meets the requirements of the user;
i.e., the software that solves the problem and the pains of the user.&lt;/p>
&lt;p>As the market is very dynamic, and evolves with high speed, often during the
software development project the requirements change due to external factors.
Therefore, knowing that it is not possible to predict all the elements, many
&amp;ldquo;workarounds&amp;rdquo; are made during development and documented. Passing the
responsibility to the user to handle the faults, and perform the workarounds,
expending more effort than would be required to perform the tasks using the
software.&lt;/p>
&lt;blockquote>
&lt;p>Deliver a working software frequently, ranging from a few weeks to a few months, considering shorter time-scale. - Agile Manifesto&lt;/p>
&lt;/blockquote>
&lt;p>Encouraging as many deployments as possible, so that failures happen as early as
possible, thus allowing their impact to be much less.&lt;/p>
&lt;h1 id="fail-fast">Fail-fast!&lt;/h1>
&lt;p>Failures are understood and encouraged because it&amp;rsquo;s part of the mindset. Because:&lt;/p>
&lt;ul>
&lt;li>Only those who &lt;strong>do&lt;/strong> make mistakes;&lt;/li>
&lt;li>Failures are the best opportunity for learning and evolving;&lt;/li>
&lt;li>Shit happens.&lt;/li>
&lt;/ul>
&lt;p>Nothing like quoting Murphy&amp;rsquo;s law to contextualize&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;Anything that can possibly go wrong, does.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;p>Therefore, it&amp;rsquo;s best for failures to occur early, while the cost of correction
is still low. Failing a controlled testing environment allows the fix to be much
faster (and cheaper) than it would if the fix were already in production.&lt;/p>
&lt;p>For this approach to succeed, there is a premise that environments are
production copies, or at least as close as possible. Otherwise, there will be
behavioral changes in the software between the environments, making the test
environment unfeasible.&lt;/p>
&lt;p>If the environments are divergent, the promotion of bugs for production will be
very frequent, causing late failures, which are expensive failures.&lt;/p>
&lt;h2 id="customer--collaboration">Customer collaboration&lt;/h2>
&lt;p>&lt;em>over contract negotiation&lt;/em>&lt;/p>
&lt;p>Know your client! Including it in the process is the best approach to have
working software. After iterating over deliverables, it&amp;rsquo;s essential to create a
positive feedback loop with your client, bringing it as close as possible to the
development of the tools that he/she is going to use.&lt;/p>
&lt;p>We can describe this situation with:&lt;/p>
&lt;ul>
&lt;li>From point A it is possible to see only point B;&lt;/li>
&lt;li>From point B it is possible to see point C;&lt;/li>
&lt;/ul>
&lt;p>Therefore there is a great incentive for the software to be delivered in parts,
continuously. Thus gathering user feedback on the next steps, following the
concepts of evolutionary prototyping, which were widely publicized through
&lt;a href="http://theleanstartup.com/book" target="_blank" rel="noopener">&lt;em>The
Lean Startup&lt;/em>&lt;/a>.&lt;/p>
&lt;p>This point contrasts sharply with the previous one about continuous release, so
that it is possible to present the prototype and evolve it throughout the
project.&lt;/p>
&lt;p>Learn who your customer/consumer/user is, and whom you are making the software
for, as this is the only way you can deliver value to that customer. An
essential part of the software development process is to be empathic with user
problems, and to truly understand what the problem is to be solved, and the
result of the impact on software development (value creation for the user).&lt;/p>
&lt;h2 id="responding-to-change">Responding to change&lt;/h2>
&lt;p>&lt;em>over following a plan&lt;/em>&lt;/p>
&lt;p>Redesigning the requirements overtime is part of the job, and a necessary step
to success. If you want to build something useful that is going to grow and have
absorption, it&amp;rsquo;s a key feature to include your client in the implementation
process.&lt;/p>
&lt;p>It will be the only way to bring all the problems of the user to the table and
create the best solution for all these problems because the user is the only
person that knows the real challenges he faces in their routine dealing with
software.&lt;/p>
&lt;p>With continuous delivery of software along with monitoring results, the process
of collecting feedback is much simpler and faster.&lt;/p>
&lt;h1 id="devops-devops-devops">DevOps, DevOps, DevOps&lt;/h1>
&lt;p>With the popularization of DevOps, a lot of disagreement came out there followed
by a significant confusion about the subject. It is very common to come across
different interpretations of &lt;strong>what is DevOps&lt;/strong>. There is a lot of euphemism in
the area, and gourmetization on LinkedIn, with many SysAdmins calling themselves
DevOps since they learned to code shell script inside Python.&lt;/p>
&lt;p>Do you want to keep reading?
&lt;a href="https://macunha.me/en/post/2019/01/devops-benefits/">Here are the benefits of adopting DevOps techniques.&lt;/a>&lt;/p></description></item><item><title>Insights from a perfectionist about Over-Engineering</title><link>https://macunha.me/en/post/2018/10/insights-from-a-perfectionist-about-over-engineering/</link><pubDate>Sun, 28 Oct 2018 00:00:00 +0200</pubDate><guid>https://macunha.me/en/post/2018/10/insights-from-a-perfectionist-about-over-engineering/</guid><description>&lt;h2 id="foreword">Foreword&lt;/h2>
&lt;p>NOTE: This article is an open letter for me to keep reminding myself about what
to prioritize when developing software, I am as much of a sinner in this aspect
as the next person.&lt;/p>
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>We as software engineers are always trying to do our best when it comes to being
innovative, improving our systems to work better and faster, perhaps with a
better design, or a more comprehensive codebase. We all have some preference
when it comes to doing our bests which we try to achieve at all times.&lt;/p>
&lt;p>The main drive of this motivation is our necessity as &amp;ldquo;digital craftspeople&amp;rdquo; to
express ourselves through quality work, along with the personal realization we
feel by doing a great job, with great quality, that challenges us and takes
ourselves further. It&amp;rsquo;s motivating, isn&amp;rsquo;t it? Assuming risks and getting out of
the comfort zone is incredibly funny, our brain&amp;rsquo;s reward system goes crazy with
unpredictability.&lt;/p>
&lt;p>To help achieve that challenge, innovation, and quality in Software Engineering
we usually think that we need the best tools available, so we&amp;rsquo;ll have fewer
things to worry about, and can concentrate our efforts in the process of
creating great products. On top of that, having the best tools could improve our
quality of life (allowing us not to work under pressure, avoids overwork, and
also helps us to sleep better at night). Furthermore, &amp;ldquo;the right set of tools&amp;rdquo;
could even enhance our productivity through self-satisfaction with work,
everyone has their preoccupations and is willing to create something to be proud
of.&lt;/p>
&lt;p>Many times during the design and development of products we take unmeasured
solutions for a simple problem. After all, we want to have not just the right
set of tools, but the &lt;strong>best&lt;/strong> right? How can we be ground-breaking, innovative,
disruptive, and pick-your-buzzword-poison otherwise? Well, as Nathan Marz
(creator of Apache Storm) puts better in his
&lt;a href="http://nathanmarz.com/blog/suffering-oriented-programming.html" target="_blank" rel="noopener">suffering-oriented programming&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>[…] don&amp;rsquo;t build technology unless you feel the pain of not having it. It applies
to the big, architectural decisions as well as the smaller everyday programming
decisions. Suffering-oriented programming greatly reduces risk by ensuring that
you&amp;rsquo;re always working on something important, and it ensures that you are
well-versed in a problem space before attempting a large investment.&lt;/p>
&lt;/blockquote>
&lt;p>This method describes a good way to think about LEAN and evolving products
through
&lt;a href="https://dzone.com/articles/what-is-minimum-viable-product-and-how-to-build-it" target="_blank" rel="noopener">an MVP concept&lt;/a>, helping to keep track of what &lt;strong>really&lt;/strong> matters when it
comes to a good balance between Product and Engineering efforts.&lt;/p>
&lt;p>As we&amp;rsquo;re daily overfed with information, it&amp;rsquo;s easy to make mistakes trying to
choose the right set of tools to work with. From picking Frameworks to Operating
Systems, and even the cloud provider to host our systems and products. It&amp;rsquo;s OK
to make mistakes, we all have a great first impression about all choices we
could have done, if you read AWS or GCP documentation you&amp;rsquo;ll be impressed with
their magical solutions to your problems, where you can just throw everything in
(including your credit card), and everything will be fine, right? The magic
cloud will solve &lt;strong>all of your&lt;/strong> problems. Yeah, &lt;em>maybe&lt;/em>.&lt;/p>
&lt;h2 id="what-is-the-problem-i-am-trying-to-solve-here">What is the problem I am trying to solve here?&lt;/h2>
&lt;p>One good example of the current hype, when it comes to applications is Docker
containers and Kubernetes. Kubernetes is the open-source version of Google&amp;rsquo;s
Borg, a great Linux containers orchestration tool developed to orchestrate
applications on Google&amp;rsquo;s data center.&lt;/p>
&lt;p>Kubernetes is great, but the hype goes too far sometimes with companies running
even Production transactional databases on it, as well as entire monoliths and
Stateful services. At this point, we have to look back and ask ourselves: &amp;ldquo;What
problem I&amp;rsquo;m trying to solve here?&amp;rdquo;. Because, if you take a second look, these
decisions are kind of a &amp;ldquo;Hydra&amp;rdquo; solution, &amp;ldquo;for every head chopped off, the Hydra
would regrow two heads&amp;rdquo;, or even better: these solutions are creating more
problems, by trying to solve problems &lt;strong>that may not even exist&lt;/strong>.&lt;/p>
&lt;p>Yeah, Google orchestrated MySQL instance deployment using Borg. The first
&lt;a href="https://sre.google/sre-book/automation-at-google/" target="_blank" rel="noopener">version (POC) was released in 2008 and finished by 2009&lt;/a> at that time the revenue
of the Ad service was
&lt;a href="https://www.statista.com/statistics/266249/advertising-revenue-of-google/" target="_blank" rel="noopener">estimated at USD ~22.9 Bi&lt;/a>. Ask yourself, do your database
serves a &lt;strong>USD 22.9 BILLION service&lt;/strong>? Do you &lt;em>really need&lt;/em> orchestration there?
Chances are, and let&amp;rsquo;s face it,
&lt;a href="https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb" target="_blank" rel="noopener">You Are Not Google&lt;/a>. This is an extreme example
but it serves to illustrate the main concept of &lt;em>suffering-oriented
programming&lt;/em>:&lt;/p>
&lt;blockquote>
&lt;p>[&amp;hellip;] don&amp;rsquo;t build technology unless you &lt;strong>feel the pain&lt;/strong> of not having it.&lt;/p>
&lt;/blockquote>
&lt;p>A nice quote from &amp;ldquo;You Are Not Google&amp;rdquo; to sink in:&lt;/p>
&lt;blockquote>
&lt;p>Don’t even start considering solutions until you &lt;strong>understand&lt;/strong> the problem. Your
goal should be to “solve” the problem mostly within the problem domain, not the
solution domain.&lt;/p>
&lt;/blockquote>
&lt;p>Otherwise, in case we insist on the inappropriate (not necessarily wrong)
solution, we&amp;rsquo;re going to spend some extra time dealing with the consequences
(i.e. chopping additional Hydra heads). Worth noting that dealing with the
consequences is not something bad, as long as you have the resources (time and
money) to invest into learning and rework, investing some resources into
inappropriate software solutions could even be seen as a way of training with
higher outcomes (learnings) than conferences, courses, and books. There is a lot
of lessons and knowledge to be extracted from these experiments.&lt;/p>
&lt;p>Learning from our experiences is the only path to success, and failures teach
best. Failures were also the motivation for writing this article to keep
reminding myself (:&lt;/p>
&lt;p>As Software Engineers the problem space analysis oftentimes fail due to an
underrated aspect, mostly unnoticed: on the other side of the line is a user of
this software.&lt;/p>
&lt;h2 id="and-guess-what">And guess what?&lt;/h2>
&lt;p>He doesn&amp;rsquo;t care if you&amp;rsquo;re running Elixir inside a container on Kubernetes, using
Container OS or Core OS, which you provisioned with your bare hands, and have
polished bit by bit to be XYZ ms faster than the Vanilla version. As long as you
respond to their requests, and &lt;strong>don&amp;rsquo;t break things&lt;/strong>.&lt;/p>
&lt;p>Innovation has nothing to do with the fact that you want to use cutting-edge
technology, and it&amp;rsquo;s not about how fast you spend money on those solutions
either. It&amp;rsquo;s about delivering value to your customers, and enrich their
experience from the interactions with your product.&lt;/p>
&lt;p>If you&amp;rsquo;re going through some orchestration problems, having 10+ micro-services
with some asynchronous task-based workers (e.g. Python&amp;rsquo;s Celery). Then, &lt;em>maybe&lt;/em>
it&amp;rsquo;s time to use Kubernetes. But, as an engineer you should know that the best
path is to put some solutions on the table, run some benchmarks and compare
them, so you&amp;rsquo;ll have data to help in your decision, and choose what&amp;rsquo;s the right
solution for your problem, &lt;strong>at the right time&lt;/strong>. We just have to keep asking
ourselves: &lt;em>&amp;ldquo;What is the problem I&amp;rsquo;m trying to solve here?&amp;quot;&lt;/em>.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>There&amp;rsquo;s a quote from a great investor called Benjamin Graham that says:&lt;/p>
&lt;blockquote>
&lt;p>If you are looking for investments*, choose them the way you would buy
groceries, not the way you would buy perfume.
&amp;ndash; Graham, The Intelligent Investor (1973)&lt;/p>
&lt;/blockquote>
&lt;p>We should carefully look to where we&amp;rsquo;re going with our choices. So we don&amp;rsquo;t
overspend and keep things going for more time, thus we go further.&lt;/p>
&lt;p>NOTE: The original text is: &amp;ldquo;If you are shopping for common stocks [&amp;hellip;]&amp;rdquo;. But, as
a Software Engineer, I just switched the syntax so we could adapt it to more
use-cases (:&lt;/p>
&lt;p>I learned from my own experience that over-engineered decisions end up bringing
more pain than solving problems, and it currently happens through early
improvements on the system, timing really matters. Many times we try to solve
all problems at once (even those we don&amp;rsquo;t have), and it brings more problems,
like high costs of maintenance and infrastructure, or under-utilization of the
resources.&lt;/p>
&lt;p>Sooner or later, the Over-Engineering bill will come as Hydra heads keep growing
up accumulating technical debt. Be mindful when analyzing the problem space,
pick the right tool for the job that eases your real pain (not the imaginary
one).&lt;/p></description></item></channel></rss>