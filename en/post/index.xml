<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | It's me, Macunha!</title><link>https://macunha.me/en/post/</link><atom:link href="https://macunha.me/en/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 21 May 2020 23:00:57 +0200</lastBuildDate><image><url>https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_512x512_fill_lanczos_center_2.png</url><title>Posts</title><link>https://macunha.me/en/post/</link></image><item><title>Quickstart: Apache Spark on Kubernetes</title><link>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</link><pubDate>Thu, 21 May 2020 23:00:57 +0200</pubDate><guid>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="the-apache-spark-operator-for-kubernetes">The Apache Spark Operator for Kubernetes&lt;/h2>
&lt;p>Since its launch in 2014 by Google, Kubernetes has gained a lot of
popularity along with Docker itself and since 2016 has become the &lt;em>de
facto Container Orchestrator&lt;/em>, established as a market standard.
Having cloud-managed versions available in &lt;strong>all&lt;/strong> the &lt;em>major Clouds&lt;/em>.
&lt;a href="https://cloud.google.com/kubernetes-engine/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://docs.microsoft.com/en-us/azure/aks/" target="_blank" rel="noopener">[3]&lt;/a> (including
&lt;a href="https://www.digitalocean.com/products/kubernetes/" target="_blank" rel="noopener">Digital Ocean&lt;/a> and
&lt;a href="https://www.alibabacloud.com/product/kubernetes" target="_blank" rel="noopener">Alibaba&lt;/a>).&lt;/p>
&lt;p>With this popularity came various implementations and &lt;em>use-cases&lt;/em> of
the orchestrator, among them the execution of
&lt;a href="https://kubernetes.io/docs/tutorials/stateful-application/" target="_blank" rel="noopener">Stateful
applications&lt;/a>
including
&lt;a href="https://vitess.io/zh/docs/get-started/kubernetes/" target="_blank" rel="noopener">databases using containers&lt;/a>.&lt;/p>
&lt;p>What would be the motivation to host an orchestrated database? That&amp;rsquo;s
a great question. But let&amp;rsquo;s focus on the
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md" target="_blank" rel="noopener">Spark Operator&lt;/a>
running workloads on Kubernetes.&lt;/p>
&lt;p>A native Spark Operator
&lt;a href="https://github.com/kubernetes/kubernetes/issues/34377" target="_blank" rel="noopener">idea came out&lt;/a>
in 2016, before that you couldn&amp;rsquo;t run Spark jobs natively except
some &lt;em>hacky alternatives&lt;/em>, like
&lt;a href="https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/" target="_blank" rel="noopener">running Apache Zeppelin&lt;/a>
inside Kubernetes or creating your
&lt;a href="https://github.com/kubernetes/examples/tree/master/staging/spark" target="_blank" rel="noopener">Apache Spark cluster inside
Kubernetes (from the official Kubernetes organization on GitHub)&lt;/a>
referencing the
&lt;a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Spark workers in Stand-alone mode&lt;/a>.&lt;/p>
&lt;p>However, the native execution would be far more interesting for taking
advantage of
&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank" rel="noopener">Kubernetes Scheduler&lt;/a>
responsible for taking action of allocating resources, giving
elasticity and an simpler interface to manage Apache Spark workloads.&lt;/p>
&lt;p>Considering that,
&lt;a href="https://issues.apache.org/jira/browse/SPARK-18278" target="_blank" rel="noopener">Apache Spark Operator development got attention&lt;/a>,
merged and released into
&lt;a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">Spark version 2.3.0&lt;/a>
launched in
&lt;a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">February, 2018&lt;/a>.&lt;/p>
&lt;p>If you&amp;rsquo;re eager for reading more regarding the Apache Spark proposal,
you can head to the
&lt;a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target="_blank" rel="noopener">design document published in Google Docs.&lt;/a>&lt;/p>
&lt;h2 id="why-kubernetes">Why Kubernetes?&lt;/h2>
&lt;p>As companies are currently seeking to
&lt;a href="https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html" target="_blank" rel="noopener">reinvent themselves through the
widely spoken digital transformation&lt;/a>
in order for them to be competitive and, above all, to survive in an
increasingly dynamic market, it is common to see approaches that
include Big Data, Artificial Intelligence and Cloud Computing
&lt;a href="https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/" target="_blank" rel="noopener">[3]&lt;/a>.&lt;/p>
&lt;p>Among the benefits of using Cloud instead of On-premises
we can list:&lt;/p>
&lt;p>An interesting comparison can be read at
&lt;a href="https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html" target="_blank" rel="noopener">Databricks blog&lt;/a>
which is the company
&lt;a href="https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/" target="_blank" rel="noopener">founded by the creators of Apache Spark&lt;/a>.&lt;/p>
&lt;p>As we see a widespread adoption of Cloud Computing (even by companies
that would be able to afford the hardware and run on-premises), we
notice that most of these Cloud implementations don&amp;rsquo;t have an
&lt;a href="https://hadoop.apache.org/" target="_blank" rel="noopener">Apache
Hadoop&lt;/a> since the Data Teams (BI/Data
Science/Analytics) increasingly choose to use tools like
&lt;a href="https://cloud.google.com/bigquery/" target="_blank" rel="noopener">Google
BigQuery&lt;/a> or
&lt;a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">AWS Redshift&lt;/a>,
it doesn&amp;rsquo;t make sense to climb a Hadoop only to use
&lt;a href="https://hortonworks.com/apache/yarn/" target="_blank" rel="noopener">YARN&lt;/a>
to manage the resources.&lt;/p>
&lt;p>An alternative is the use of Hadoop cluster providers such as
&lt;a href="https://cloud.google.com/dataproc" target="_blank" rel="noopener">Google
DataProc&lt;/a> or
&lt;a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener">AWS EMR&lt;/a>
for the creation of ephemeral clusters. Just to name a few options.&lt;/p>
&lt;p>To better understand the design of Spark Operator, the doc from
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operatoR/blob/master/docs/design.md#the-crd-controller" target="_blank" rel="noopener">GCP on GitHub&lt;/a>
as a no-brainer.&lt;/p>
&lt;h1 id="lets-get-our-hands-on">Let&amp;rsquo;s get our hands-on!&lt;/h1>
&lt;h2 id="warming-up-the-engine">Warming up the engine&lt;/h2>
&lt;p>Now that the word has been spread, let&amp;rsquo;s get our hands on it to show
the engine running. For that, let&amp;rsquo;s use:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://www.docker.com/" target="_blank" rel="noopener">Docker&lt;/a> as the container engine for
Kubernetes
&lt;a href="https://docs.docker.com/install/" target="_blank" rel="noopener">(installation guide)&lt;/a>;&lt;/li>
&lt;li>Minikube
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">(installation guide)&lt;/a>
to facilitate the provisioning of the Kubernetes (yes, it will be
a local execution);&lt;/li>
&lt;li>For interaction with the Kubernetes API it is necessary to have
&lt;code>kubectl&lt;/code> installed,
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">if you don&amp;rsquo;t have it, follow instructions
here&lt;/a>.&lt;/li>
&lt;li>a compiled version of Apache Spark larger than 2.3.0.
&lt;ol>
&lt;li>you can either compile
&lt;a href="https://github.com/apache/spark" target="_blank" rel="noopener">source code&lt;/a>,
which will took &lt;em>some hours&lt;/em> to finish, or&lt;/li>
&lt;li>download a compiled version
&lt;a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">here&lt;/a>
(recommended).&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>Once the necessary tools are installed, it&amp;rsquo;s necessary to
include Apache Spark path in &lt;code>PATH&lt;/code> environment variable, to ease the
invokation of Apache Spark executables. Simply run:&lt;/p>
&lt;pre>&lt;code class="language-bash">export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin
&lt;/code>&lt;/pre>
&lt;h2 id="creating-the-minikube-cluster">Creating the Minikube &amp;ldquo;cluster&amp;rdquo;&lt;/h2>
&lt;p>At last, to have a Kubernetes &amp;ldquo;cluster&amp;rdquo; we will start a &lt;code>minikube&lt;/code>
with the intention of running a example from
&lt;a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala" target="_blank" rel="noopener">Spark
repository&lt;/a>
called &lt;code>SparkPi&lt;/code> just as a demonstration.&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube start --cpus=2 \
--memory=4g
&lt;/code>&lt;/pre>
&lt;h2 id="building-the-docker-image">Building the Docker image&lt;/h2>
&lt;p>Let&amp;rsquo;s use the Minikube Docker daemon to not depend on an external
registry (and only generate Docker image layers on the VM,
facilitating a garbage disposal later). Minikube has a wrapper that
makes our life easier:&lt;/p>
&lt;pre>&lt;code class="language-bash">eval $(minikube docker-env)
&lt;/code>&lt;/pre>
&lt;p>After having the daemon environment variables configured, we need a
Docker image to run the jobs. There is a
&lt;a href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh" target="_blank" rel="noopener">shell script in the Spark
repository&lt;/a>
to help with this. Considering that our &lt;code>PATH&lt;/code> was properly
configured, just run:&lt;/p>
&lt;pre>&lt;code class="language-bash">docker-image-tool.sh -m -t latest build
&lt;/code>&lt;/pre>
&lt;p>&lt;em>FYI:&lt;/em> The &lt;code>-m&lt;/code> parameter here indicates a minikube build.&lt;/p>
&lt;p>Let&amp;rsquo;s take the highway to execute SparkPi, using the same command
that would be used for a Hadoop Spark cluster
&lt;a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">spark-submit&lt;/a>.&lt;/p>
&lt;p>However, Spark Operator supports defining jobs in the &amp;ldquo;Kubernetes
language&amp;rdquo; using
&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">CRD&lt;/a>,
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples" target="_blank" rel="noopener">here are some examples&lt;/a> - for later.&lt;/p>
&lt;h1 id="fire-in-the-hole">Fire in the hole!&lt;/h1>
&lt;p>Mid the gap between the Scala version and .jar when you&amp;rsquo;re
parameterizing with your Apache Spark version:&lt;/p>
&lt;pre>&lt;code class="language-bash">spark-submit --master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=2 \
--executor-memory 1024m \
--conf spark.kubernetes.container.image=spark:latest \
local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # here
&lt;/code>&lt;/pre>
&lt;p>What&amp;rsquo;s new is:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--master&lt;/code>: Accepts a prefix &lt;code>k8s://&lt;/code> in the URL, for the
Kubernetes master API endpoint, exposed by the command
&lt;code>https://$(minikube ip):8443&lt;/code>. BTW, in case you want to
know, it&amp;rsquo;s a
&lt;a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html" target="_blank" rel="noopener">shell command substitution&lt;/a>;&lt;/li>
&lt;li>&lt;code>--conf spark.kubernetes.container.image=&lt;/code>: Configures the Docker
image to run in Kubernetes.&lt;/li>
&lt;/ul>
&lt;p>Sample output:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&amp;gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&amp;gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
&lt;/code>&lt;/pre>
&lt;p>To see the job result (and the whole execution) we can run a
&lt;code>kubectl logs&lt;/code> passing the name of the driver pod as a parameter:&lt;/p>
&lt;pre>&lt;code class="language-bash">kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')
&lt;/code>&lt;/pre>
&lt;p>Which brings the output (omitted some entries), similar to:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
...
&lt;/code>&lt;/pre>
&lt;p>The result appears in:&lt;/p>
&lt;pre>&lt;code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
&lt;/code>&lt;/pre>
&lt;p>Finally, let&amp;rsquo;s delete the VM that Minikube generates, to clean up the
environment (unless you want to keep playing with it):&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube delete
&lt;/code>&lt;/pre>
&lt;h2 id="last-words">Last words&lt;/h2>
&lt;p>I hope your curiosity got &lt;em>sparked&lt;/em> and some ideas for further
development have raised for your Big Data workloads. If you have any
doubt or suggestion, don&amp;rsquo;t hesitate to share on the comment section.&lt;/p></description></item><item><title>DevOps: Benefits</title><link>https://macunha.me/en/post/2019/01/devops-benefits/</link><pubDate>Fri, 11 Jan 2019 22:00:00 +0000</pubDate><guid>https://macunha.me/en/post/2019/01/devops-benefits/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Main benefits that a company generally expects and finds in the adoption of culture:&lt;/p>
&lt;h2 id="faster-and-cheaper-releases">Faster and Cheaper Releases&lt;/h2>
&lt;p>Since releases will be continuous and frequent, deliverables will turn into small changes with the benefit of increasing speed in the development cycle (delivering always).&lt;/p>
&lt;h2 id="improved-operational-support-with-quick-fixed">Improved Operational support with quick fixed&lt;/h2>
&lt;p>If there is a failure during delivery, the impact is minimal because the amount of modifications is small, just as the rollback is faster. Having a simple inspection and debugging.&lt;/p>
&lt;h2 id="better-time-to-market-ttm">Better Time-to-market (TTM)&lt;/h2>
&lt;p>The software will be delivered much earlier when it&amp;rsquo;s still an MVP. Customers will be integrated as part of the development process, bringing insights and feedback to the development team. Thus allowing for a higher launch speed in the market.&lt;/p>
&lt;h2 id="superior-quality-products">Superior quality products&lt;/h2>
&lt;p>As has been said before, early failures prevent defects from being delivered to production, because:&lt;/p>
&lt;ul>
&lt;li>Reduces the volume of defects in the product as a whole;&lt;/li>
&lt;li>Increases frequency of new features and releases;&lt;/li>
&lt;li>Appropriate development processes in teams, including automation.&lt;/li>
&lt;/ul>
&lt;h1 id="now-we-understood-why-lets-talk-about-how">Now we understood WHY, let&amp;rsquo;s talk about HOW&lt;/h1>
&lt;h2 id="continuous-releases-integration-delivery-deployment">Continuous releases (integration, delivery, deployment)&lt;/h2>
&lt;p>Usually follows a code versioning approach (through Git) using specific branches for each environment (e.g.: feature branches with git flow).&lt;/p>
&lt;h2 id="continuous-integration">Continuous integration&lt;/h2>
&lt;p>Automatic execution of unit tests, integration tests and code quality analysis against a git branch, to ensure that there was no disruption of the modified piece of code.&lt;/p>
&lt;h2 id="continuous-delivery">Continuous delivery&lt;/h2>
&lt;p>Packaging the software that is tested and approved, to deliver it somewhere that it is possible to use in a deploy later. Examples are libs delivered in repositories to be integrated into the code during the next update and code deploy.&lt;/p>
&lt;h2 id="continuous-deployment">Continuous deployment&lt;/h2>
&lt;p>Once you have completed all of the above steps, you can do automated deployments right in the environments, when the team is more confident about the tools they are testing, as well as the risk they&amp;rsquo;re taking and also understanding that there is a possibility of failure in a tests environment without worrying that it&amp;rsquo;s going to be divergent from production.&lt;/p>
&lt;h2 id="configuration-andor-infrastructure-as-code">Configuration (and/or Infrastructure) as code&lt;/h2>
&lt;p>To be able to test software with assertiveness, and to understand that it will transit between environments without changing behavior, it is essential that the configurations are also expressed in code. This allows the settings to be also versioned, following the code. Also guaranteeing a uniformity among the environments, which enables:&lt;/p>
&lt;ul>
&lt;li>Reduction in maintenance costs, having a single point to look at and understand the operation of the system;&lt;/li>
&lt;li>Easy to recreate the infrastructure, if it is necessary to move everything to another place, this can happen with a few manual interactions;&lt;/li>
&lt;li>Allows for a code review of infrastructure and configurations, which consequently brings a culture of collaboration in the development, sharing of knowledge and increases the democratization of the infra;&lt;/li>
&lt;li>Documentation as code, helping new team members get a faster warm up.&lt;/li>
&lt;/ul>
&lt;p>These points were well-stressed by the Heroku team and gave rise to the famous paper: [The Twelve-Factor App] (&lt;a href="https://12factor.net/)">https://12factor.net/)&lt;/a>. It&amp;rsquo;s an excellent reading for the explanation of the benefits of configuration management.&lt;/p>
&lt;h2 id="observability-monitoring-and-self-healing">Observability, Monitoring, and self-healing&lt;/h2>
&lt;p>At the end of the delivery process, the software must be monitored. Avoiding to wait for an external report of failures, ensuring that the actions are proactive rather than reactive.&lt;/p>
&lt;p>With mature monitoring, it&amp;rsquo;s possible to create trigger against alerts, creating a self-healing system in which actions (scripts) are performed to &lt;strong>fix known&lt;/strong> failures in the infrastructure so that everyone can sleep peacefully at night, without having to worry about the on-call schedule that makes you read some documentation at dawn. (If you have had experience with this, you know for sure how bad it is).&lt;/p>
&lt;p>Scaling up only those cases that are extreme exceptions (mistakes not known/expected) in the process for the employee to act, ensuring higher health in operation.&lt;/p>
&lt;h2 id="processes-automation">Processes automation&lt;/h2>
&lt;p>All processes that cause Muda should be addressed with automation, allowing people to work more quickly. Good examples of processes that are usually automated are:&lt;/p>
&lt;ul>
&lt;li>Deployment;&lt;/li>
&lt;li>Self-healing (system resilience in response to anomalies);&lt;/li>
&lt;li>Renewal of Certificates;&lt;/li>
&lt;li>Execution of tests (unitary, integration, functional, etc.);&lt;/li>
&lt;li>Monitoring (with auto-discovery);&lt;/li>
&lt;li>User Governance;&lt;/li>
&lt;/ul>
&lt;h1 id="devops-toolchainhttpsenwikipediaorgwikidevops_toolchain">
&lt;a href="https://en.wikipedia.org/wiki/DevOps_toolchain" target="_blank" rel="noopener">DevOps toolchain&lt;/a>&lt;/h1>
&lt;p>A combination of tools to facilitate the maintenance and operation of the system, with the flow:&lt;/p>
&lt;p>&lt;img src="https://macunha.me/img/content/devops-lifecycle.png" alt="Development Cycle Using DevOps">&lt;/p>
&lt;blockquote>
&lt;p>Note: Any similarity to the PDCA is pure certainty.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Plan&lt;/strong>: Project planning phase, in which feedbacks are collected for requirements survey, and backlog creation;&lt;/li>
&lt;li>&lt;strong>Create&lt;/strong>: Creation of a deliverable (to validate a hypothesis), such as an MVP;&lt;/li>
&lt;li>&lt;strong>Verify&lt;/strong>: Pass the deliverable to the test phase;&lt;/li>
&lt;li>&lt;strong>Package:&lt;/strong> Package the build to be able to put it in some testing environment;&lt;/li>
&lt;li>&lt;strong>Release&lt;/strong>: Deploy packaged deliverable;&lt;/li>
&lt;li>&lt;strong>Configure&lt;/strong>: Perform the configuration of the deliverable in the testing environment, trying to get as close as possible to the twelve-factor app.&lt;/li>
&lt;li>&lt;strong>Monitor&lt;/strong>: After deploying to the environment, track business metrics and infrastructure to ensure everything is working as expected.&lt;/li>
&lt;/ul>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>During the implementation of these techniques it is possible to observe improvements in the development process, the most notable gains are:&lt;/p>
&lt;ul>
&lt;li>Increase in team engagement;&lt;/li>
&lt;li>Knowledge sharing;&lt;/li>
&lt;li>Reduction of bottlenecks;&lt;/li>
&lt;li>More free time to do work that really matters (adds value to the user experience or generates impact);&lt;/li>
&lt;li>Greater confidence in delivering software.&lt;/li>
&lt;/ul></description></item><item><title>DevOps: The Genesis</title><link>https://macunha.me/en/post/2019/01/devops-genesis/</link><pubDate>Fri, 11 Jan 2019 21:00:00 +0000</pubDate><guid>https://macunha.me/en/post/2019/01/devops-genesis/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>First of all, it&amp;rsquo;s all about agile.&lt;/p>
&lt;p>The DevOps methodology was created on top of agile methods, to deliver a higher value inside software releases, automating feature release through pipelines, that can test hypothesis faster allowing higher adaptability using &amp;ldquo;fail-fast&amp;rdquo; approaches. Those changes are more cultural than technical, so it&amp;rsquo;s normal to see DevOps being called culture.&lt;/p>
&lt;p>The implementation of DevOps happens through processes automation, having a strong sense of processes re-engineering inside the company. Comparing to the cultural change, the technical is easy to implement. Therefore the role that a &amp;ldquo;DevOps Engineer/Analyst&amp;rdquo; performs is very confusing, which enables many SysAdmins and Infra Analysts assuming the role of &amp;ldquo;DevOps.&amp;rdquo;&lt;/p>
&lt;h2 id="lean-is-the-basis-of-agile">Lean is the basis of Agile&lt;/h2>
&lt;p>Reality is not as happy as it sounds. After World War II, Japan was destroyed and under-resourced after losing the war. With a limited amount of resources, the country needed to reinvent itself and survive after a time of severe depression. During that time two guys gained attention inside a company that later gave its name after the methodology.&lt;/p>
&lt;p>Those guys were Eiji Toyoda and Taiichi Ohno, inside Toyota Motor Corporation. They&amp;rsquo;re the founders of the &amp;ldquo;Toyota production model&amp;rdquo; also known as Toyotism.&lt;/p>
&lt;h2 id="toyota-gave-birth-to-lean">Toyota gave birth to Lean&lt;/h2>
&lt;p>Lean teaches how to optimize the end-to-end process, focusing on processes that create value for customers. Bottlenecks in the process must be removed, and wasteful activities need to be identified and avoided. Both explained and defined by LEAN 3M: Muda, Mura, and Muri.&lt;/p>
&lt;p>Also teaches to improve yourself day after day and always focus on quality through Kaizen (continuous improvement).&lt;/p>
&lt;p>Japanese culture truly believes that quality is the main objective to deliver value to customers since quality is what brings your clients back.&lt;/p>
&lt;h2 id="kaizen">Kaizen&lt;/h2>
&lt;p>A mindset that helps to look at each part of the process exclusively and think about the improvements. Involving the people who are part of the process, encourage the inclusion of these people in the decisions of change, since:&lt;/p>
&lt;ul>
&lt;li>It is much easier to accept a change when it is not imposed (top-down);&lt;/li>
&lt;li>There is a greater absorption of change by people when they&amp;rsquo;re included in the planning;&lt;/li>
&lt;li>The people who are involved in the process bring their concerns and suggestions, which contribute positively to the evolution of the change, making the idea more robust.&lt;/li>
&lt;/ul>
&lt;p>The process of defining improvements through Kaizen happens (usually) in the following order:&lt;/p>
&lt;ol>
&lt;li>Define data-driven objectives;&lt;/li>
&lt;li>Review the current state and develop an improvement plan;&lt;/li>
&lt;li>Implement improvement;&lt;/li>
&lt;li>Review the implementation and improve what does not work;&lt;/li>
&lt;li>Report the results and determine the items to be monitored.&lt;/li>
&lt;/ol>
&lt;p>This process is also called &lt;strong>PDCA: Plain-Do-Control-Act&lt;/strong>, which is summarized in:&lt;/p>
&lt;ul>
&lt;li>Plan (develop the hypothesis);&lt;/li>
&lt;li>Do (experiment);&lt;/li>
&lt;li>Check (validate results);&lt;/li>
&lt;li>Act (refine the experiment and start over).&lt;/li>
&lt;/ul>
&lt;h1 id="3m-muda-mura-muri">3M: Muda, Mura, Muri&lt;/h1>
&lt;h2 id="muda-waste">Muda (waste)&lt;/h2>
&lt;p>Any activity that consumes time without adding value to the final consumer. e.g.:&lt;/p>
&lt;ul>
&lt;li>over-production;&lt;/li>
&lt;li>idle time in the process;&lt;/li>
&lt;li>products with a defect.&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s important to remember that there are different levels of Muda that can be removed quickly or not, and the classification depends on the time for removal.&lt;/p>
&lt;p>An example of a more time-consuming Muda is the discontinuation of legacy software that ends up with longer release cycles, causing teams to be idle, followed by an often long or manual test routine.&lt;/p>
&lt;h2 id="mura-unevenness">Mura (unevenness)&lt;/h2>
&lt;p>Unevenness in operation, caused by activities that are very changeable and unpredictable, generating different results in all executions. e.g., the execution of tasks that were not well planned and ended up arriving with strict deadlines. The team runs in the rush, generating exhaustion, despair, and moreover, when finished leaves the people who have performed these tasks waiting (for feedback, or confirmation that it is completed).&lt;/p>
&lt;h2 id="muri-overload">Muri (overload)&lt;/h2>
&lt;p>Overburdening equipment or operators by requiring them to run at a higher or harder pace beyond the limit, to achieve some goal or expectation, causing fatigue and consequently failures during the process. These failures are usually human errors caused by fatigue during overwork.&lt;/p>
&lt;h2 id="back-to-agile">Back to Agile&lt;/h2>
&lt;p>In 2000 a group of 17 people met at a resort in Oregon to talk about ideas that could improve the flow of software development. After a year of mature ideas, these people met again and published the ideas, which we now know as &lt;strong>Agile Manifesto&lt;/strong>.&lt;/p>
&lt;p>Main points are:&lt;/p>
&lt;p>&lt;strong>Individuals and interactions&lt;/strong> over processes and tools
&lt;strong>Working software&lt;/strong> over comprehensive documentation
&lt;strong>Customer collaboration&lt;/strong> over contract negotiation
&lt;strong>Responding to change&lt;/strong> over following a plan&lt;/p>
&lt;p>I will restrict the explanation of these points with the DevOps point of view, keeping on track (now).&lt;/p>
&lt;h2 id="individuals-and-interactions">Individuals and interactions&lt;/h2>
&lt;p>&lt;em>over processes and tools&lt;/em>&lt;/p>
&lt;p>First comes the individuals, they should receive the necessary tooling to work with, and then be empowered to do their jobs. Interactions between people are greatly encouraged, for sharing knowledge and also for facilitating creative flow within development teams.&lt;/p>
&lt;p>An excellent example of interaction encouraged through DevOps is the code review habit. Considering that small parts of the software will be iterated and approved in the pipeline passing through different environments, automatically, the best way to prevent defects is through code review.&lt;/p>
&lt;p>This habit brings benefits such as:&lt;/p>
&lt;ul>
&lt;li>Knowledge sharing;&lt;/li>
&lt;li>Observation of the problem from a different point of view;&lt;/li>
&lt;li>Team engagement;&lt;/li>
&lt;li>Lesser bugs.&lt;/li>
&lt;/ul>
&lt;h2 id="working-software">Working software&lt;/h2>
&lt;p>&lt;em>over comprehensive documentation&lt;/em>&lt;/p>
&lt;p>Here&amp;rsquo;s a trick in &amp;ldquo;working software,&amp;rdquo; software that works is not code that compiles. The software that works is what meets the requirements of the user; i.e., the software that solves the problem and the pains of the user.&lt;/p>
&lt;p>As the market is very dynamic, and evolves with high speed, often during the software development project the requirements change due to external factors. Therefore, knowing that it is not possible to predict all the elements, many &amp;ldquo;workarounds&amp;rdquo; are made during development and documented. Passing the responsibility to the user to handle the faults, and perform the workarounds, expending more effort than would be required to perform the tasks using the software.&lt;/p>
&lt;blockquote>
&lt;p>Deliver a working software frequently, ranging from a few weeks to a few months, considering shorter time-scale. - Agile Manifesto&lt;/p>
&lt;/blockquote>
&lt;p>Encouraging as many deployments as possible, so that failures happen as early as possible, thus allowing their impact to be much less.&lt;/p>
&lt;h1 id="fail-fast">Fail-fast!&lt;/h1>
&lt;p>Failures are understood and encouraged because it&amp;rsquo;s part of the mindset. Because:&lt;/p>
&lt;ul>
&lt;li>Only those who &lt;strong>do&lt;/strong> make mistakes;&lt;/li>
&lt;li>Shit happens.&lt;/li>
&lt;/ul>
&lt;p>Therefore, it&amp;rsquo;s best for failures to occur early, while the cost of correction is still low. Failing a controlled testing environment allows the fix to be much faster (and cheaper) than it would if the fix were already in production.&lt;/p>
&lt;p>For this approach to succeed, there is a premise that environments are production copies, or at least as close as possible. Otherwise, there will be behavioral changes in the software between the environments, making the test environment unfeasible.&lt;/p>
&lt;p>If the environments are divergent, the promotion of bugs for production will be very frequent, causing late failures, which are expensive failures.&lt;/p>
&lt;h2 id="customer--collaboration">Customer collaboration&lt;/h2>
&lt;p>&lt;em>over contract negotiation&lt;/em>&lt;/p>
&lt;p>Know your client! Including it in the process is the best approach to have working software. After iterating over deliverables, it&amp;rsquo;s essential to create a positive feedback loop with your client, bringing it as close as possible to the development of the tools that he/she is going to use.&lt;/p>
&lt;p>We can describe this situation with:&lt;/p>
&lt;ul>
&lt;li>From point A it is possible to see only point B;&lt;/li>
&lt;li>From point B it is possible to see point C;&lt;/li>
&lt;/ul>
&lt;p>Therefore there is a great incentive for the software to be delivered in parts, continuously. Thus gathering user feedback on the next steps, following the concepts of evolutionary prototyping, which were widely publicized through
&lt;a href="http://theleanstartup.com/book" target="_blank" rel="noopener">&lt;em>The Lean Startup&lt;/em>&lt;/a>.&lt;/p>
&lt;p>This point contrasts sharply with the previous one about continuous release, so that it is possible to present the prototype and evolve it throughout the project.&lt;/p>
&lt;p>Learn who your customer/consumer/user is, and whom you are making the software for, as this is the only way you can deliver value to that customer. An essential part of the software development process is to be empathic with user problems, and to truly understand what the problem is to be solved, and the result of the impact on software development (value creation for the user).&lt;/p>
&lt;h2 id="responding-to-change">Responding to change&lt;/h2>
&lt;p>&lt;em>over following a plan&lt;/em>&lt;/p>
&lt;p>Redesigning the requirements overtime is part of the job, and a necessary step to success. If you want to build something useful that is going to grow and have absorption, it&amp;rsquo;s a key feature to include your client in the implementation process.&lt;/p>
&lt;p>It will be the only way to bring all the problems of the user to the table and create the best solution for all these problems because the user is the only person that knows the real challenges he faces in their routine dealing with software.&lt;/p>
&lt;p>With continuous delivery of software along with monitoring results, the process of collecting feedback is much simpler and faster.&lt;/p>
&lt;h1 id="devops-devops-devops">DevOps, DevOps, DevOps&lt;/h1>
&lt;p>With the popularization of DevOps, a lot of disagreement came out there followed by a significant confusion about the subject. It is very common to come across different interpretations of &lt;strong>what is DevOps&lt;/strong>. There is a lot of euphemism in the area, and gourmetization on LinkedIn, with many SysAdmins calling themselves DevOps since they learned to code shell script inside Python.&lt;/p>
&lt;p>Do you want to keep reading?
&lt;a href="https://macunha.me/en/post/2019/01/devops-benefits/">Here are the benefits of adopting DevOps techniques.&lt;/a>&lt;/p></description></item></channel></rss>