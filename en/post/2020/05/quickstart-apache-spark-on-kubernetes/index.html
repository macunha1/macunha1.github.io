<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Matheus Cunha"><meta name=description content="Using Apache Spark Operator in Kubernetes to streamline your Big Data workflows with a cloud-native approach without relying on a Hadoop cluster."><link rel=alternate hreflang=pt href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/><link rel=alternate hreflang=en-us href=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-dark disabled><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=https://macunha.me/css/academic.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-143794949-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-143794949-1',{'anonymize_ip':true});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=https://macunha.me/en/index.webmanifest><link rel=icon type=image/png href=https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@_macunha1"><meta property="twitter:creator" content="@_macunha1"><meta property="og:site_name" content="It's me, Macunha!"><meta property="og:url" content="https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/"><meta property="og:title" content="Quickstart: Apache Spark on Kubernetes | It's me, Macunha!"><meta property="og:description" content="Using Apache Spark Operator in Kubernetes to streamline your Big Data workflows with a cloud-native approach without relying on a Hadoop cluster."><meta property="og:image" content="https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/featured.png"><meta property="twitter:image" content="https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-05-21T23:00:57+02:00"><meta property="article:modified_time" content="2020-06-18T07:19:15+02:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/"},"headline":"Quickstart: Apache Spark on Kubernetes","image":["https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/featured.png"],"datePublished":"2020-05-21T23:00:57+02:00","dateModified":"2020-06-18T07:19:15+02:00","author":{"@type":"Person","name":"Matheus Cunha"},"publisher":{"@type":"Organization","name":"It's me, Macunha!","logo":{"@type":"ImageObject","url":"https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_192x192_fill_lanczos_center_2.png"}},"description":"Using Apache Spark Operator in Kubernetes to streamline your Big Data workflows with a cloud-native approach without relying on a Hadoop cluster."}</script><script src=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({"palette":{"popup":{"background":"#2962ff","text":"#fff"},"button":{"background":"#fff","text":"#2962ff"}},"theme":"classic","content":{"message":"This website uses cookies to ensure you get the best experience on our website.","dismiss":"Got it!","link":"Learn more","href":"https://www.cookiesandyou.com"}})});</script><title>Quickstart: Apache Spark on Kubernetes | It's me, Macunha!</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=https://macunha.me/en/>It's me, Macunha!</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=https://macunha.me/en/>It's me, Macunha!</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=https://macunha.me/en/project/><span>Projects</span></a></li><li class=nav-item><a class="nav-link active" href=https://macunha.me/en/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=https://macunha.me/en/author/matheus-cunha/><span>About</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">English</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>English</span></div><a class=dropdown-item href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/><span>PortuguÃªs</span></a></div></li></ul></div></nav><article class=article><div class="article-container pt-3"><h1>Quickstart: Apache Spark on Kubernetes</h1><p class=page-subtitle>Running Apache Spark Operator on Kubernetes</p><div class=article-metadata><span class=article-date>Last updated on
18/06/2020</span>
<span class=middot-divider></span><span class=article-reading-time>6 min read</span>
<span class=middot-divider></span><a href=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/#disqus_thread></a><span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=https://macunha.me/en/category/tutorials/>Tutorials</a></span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:480px><div style=position:relative><img src=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/featured_hu41e40b398e8bb52ece3d3b259cd36487_2617045_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><div class=article-style><h1 id=introduction>Introduction</h1><h2 id=the-apache-spark-operator-for-kubernetes>The Apache Spark Operator for Kubernetes</h2><p>Since its launch in 2014 by Google, Kubernetes has gained a lot of
popularity along with Docker itself and since 2016 has become the <em>de
facto Container Orchestrator</em>, established as a market standard.
Having cloud-managed versions available in <strong>all</strong> the <em>major Clouds</em>.
<a href=https://cloud.google.com/kubernetes-engine/ target=_blank rel=noopener>[1]</a>
<a href=https://aws.amazon.com/eks/ target=_blank rel=noopener>[2]</a>
<a href=https://docs.microsoft.com/en-us/azure/aks/ target=_blank rel=noopener>[3]</a> (including
<a href=https://www.digitalocean.com/products/kubernetes/ target=_blank rel=noopener>Digital Ocean</a> and
<a href=https://www.alibabacloud.com/product/kubernetes target=_blank rel=noopener>Alibaba</a>).</p><p>With this popularity came various implementations and <em>use-cases</em> of
the orchestrator, among them the execution of
<a href=https://kubernetes.io/docs/tutorials/stateful-application/ target=_blank rel=noopener>Stateful
applications</a>
including
<a href=https://vitess.io/zh/docs/get-started/kubernetes/ target=_blank rel=noopener>databases using containers</a>.</p><p>What would be the motivation to host an orchestrated database? That&rsquo;s
a great question. But let&rsquo;s focus on the
<a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md target=_blank rel=noopener>Spark Operator</a>
running workloads on Kubernetes.</p><p>A native Spark Operator
<a href=https://github.com/kubernetes/kubernetes/issues/34377 target=_blank rel=noopener>idea came out</a>
in 2016, before that you couldn&rsquo;t run Spark jobs natively except
some <em>hacky alternatives</em>, like
<a href=https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/ target=_blank rel=noopener>running Apache Zeppelin</a>
inside Kubernetes or creating your
<a href=https://github.com/kubernetes/examples/tree/master/staging/spark target=_blank rel=noopener>Apache Spark cluster inside
Kubernetes (from the official Kubernetes organization on GitHub)</a>
referencing the
<a href=http://spark.apache.org/docs/latest/spark-standalone.html target=_blank rel=noopener>Spark workers in Stand-alone mode</a>.</p><p>However, the native execution would be far more interesting for taking
advantage of
<a href=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ target=_blank rel=noopener>Kubernetes Scheduler</a>
responsible for taking action of allocating resources, giving
elasticity and an simpler interface to manage Apache Spark workloads.</p><p>Considering that,
<a href=https://issues.apache.org/jira/browse/SPARK-18278 target=_blank rel=noopener>Apache Spark Operator development got attention</a>,
merged and released into
<a href=https://spark.apache.org/releases/spark-release-2-3-0.html target=_blank rel=noopener>Spark version 2.3.0</a>
launched in
<a href=https://spark.apache.org/news/index.html target=_blank rel=noopener>February, 2018</a>.</p><p>If you&rsquo;re eager for reading more regarding the Apache Spark proposal,
you can head to the
<a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target=_blank rel=noopener>design document published in Google Docs.</a></p><h2 id=why-kubernetes>Why Kubernetes?</h2><p>As companies are currently seeking to
<a href=https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html target=_blank rel=noopener>reinvent themselves through the
widely spoken digital transformation</a>
in order for them to be competitive and, above all, to survive in an
increasingly dynamic market, it is common to see approaches that
include Big Data, Artificial Intelligence and Cloud Computing
<a href=https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/ target=_blank rel=noopener>[1]</a>
<a href=https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/ target=_blank rel=noopener>[2]</a>
<a href=https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/ target=_blank rel=noopener>[3]</a>.</p><p>An interesting comparison between the benefits of using Cloud Computing in the
context of Big Data instead of On-premises' servers can be read at
<a href=https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html target=_blank rel=noopener>Databricks
blog</a>,
which is the company
<a href=https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/ target=_blank rel=noopener>founded by the creators of Apache Spark</a>.</p><p>As we see a widespread adoption of Cloud Computing (even by companies
that would be able to afford the hardware and run on-premises), we
notice that most of these Cloud implementations don&rsquo;t have an
<a href=https://hadoop.apache.org/ target=_blank rel=noopener>Apache
Hadoop</a> since the Data Teams (BI/Data
Science/Analytics) increasingly choose to use tools like
<a href=https://cloud.google.com/bigquery/ target=_blank rel=noopener>Google
BigQuery</a> or
<a href=https://aws.amazon.com/redshift/ target=_blank rel=noopener>AWS Redshift</a>.
Therefore, it doesn&rsquo;t make sense to spin-up a Hadoop with the only intention to
use
<a href=https://hortonworks.com/apache/yarn/ target=_blank rel=noopener>YARN</a> as the resources manager.</p><p>An alternative is the use of Hadoop cluster providers such as
<a href=https://cloud.google.com/dataproc target=_blank rel=noopener>Google
DataProc</a> or
<a href=https://aws.amazon.com/emr/ target=_blank rel=noopener>AWS EMR</a>
for the creation of ephemeral clusters. Just to name a few options.</p><p>To better understand the design of Spark Operator, the doc from
<a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operatoR/blob/master/docs/design.md#the-crd-controller target=_blank rel=noopener>GCP on GitHub</a>
is a no-brainer.</p><h1 id=lets-get-hands-on>Let&rsquo;s get hands-on!</h1><h2 id=warming-up-the-engine>Warming up the engine</h2><p>Now that the word has been spread, let&rsquo;s get our hands on it to show
the engine running. For that, let&rsquo;s use:</p><ul><li><a href=https://www.docker.com/ target=_blank rel=noopener>Docker</a> as the container engine for
Kubernetes
<a href=https://docs.docker.com/install/ target=_blank rel=noopener>(installation guide)</a>;</li><li>Minikube
<a href=https://kubernetes.io/docs/tasks/tools/install-minikube/ target=_blank rel=noopener>(installation guide)</a>
to facilitate the provisioning of the Kubernetes (yes, it will be
a local execution);</li><li>For interaction with the Kubernetes API it is necessary to have
<code>kubectl</code> installed,
<a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/ target=_blank rel=noopener>if you don&rsquo;t have it, follow instructions
here</a>.</li><li>a compiled version of Apache Spark larger than 2.3.0.<ol><li>you can either compile
<a href=https://github.com/apache/spark target=_blank rel=noopener>source code</a>,
which will took <em>some hours</em> to finish, or</li><li>download a compiled version
<a href=https://spark.apache.org/downloads.html target=_blank rel=noopener>here</a>
(recommended).</li></ol></li></ul><p>Once the necessary tools are installed, it&rsquo;s necessary to
include Apache Spark path in <code>PATH</code> environment variable, to ease the
invocation of Apache Spark executables. Simply run:</p><pre><code class=language-bash>export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin
</code></pre><h2 id=creating-the-minikube-cluster>Creating the Minikube &ldquo;cluster&rdquo;</h2><p>At last, to have a Kubernetes &ldquo;cluster&rdquo; we will start a <code>minikube</code>
with the intention of running an example from
<a href=https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala target=_blank rel=noopener>Spark
repository</a>
called <code>SparkPi</code> just as a demonstration.</p><pre><code class=language-bash>minikube start --cpus=2 \
    --memory=4g
</code></pre><h2 id=building-the-docker-image>Building the Docker image</h2><p>Let&rsquo;s use the Minikube Docker daemon to not depend on an external registry (and
only generate Docker image layers on the VM, facilitating garbage disposal
later). Minikube has a wrapper that makes our life easier:</p><pre><code class=language-bash>eval $(minikube docker-env)
</code></pre><p>After having the daemon environment variables configured, we need a
Docker image to run the jobs. There is a
<a href=https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh target=_blank rel=noopener>shell script in the Spark
repository</a>
to help with this. Considering that our <code>PATH</code> was properly
configured, just run:</p><pre><code class=language-bash>docker-image-tool.sh -m -t latest build
</code></pre><p><em>FYI:</em> The <code>-m</code> parameter here indicates a minikube build.</p><p>Let&rsquo;s take the highway to execute SparkPi, using the same command
that would be used for a Hadoop Spark cluster
<a href=https://spark.apache.org/docs/latest/submitting-applications.html target=_blank rel=noopener>spark-submit</a>.</p><p>However, Spark Operator supports defining jobs in the &ldquo;Kubernetes
dialect&rdquo; using
<a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ target=_blank rel=noopener>CRD</a>,
<a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples target=_blank rel=noopener>here are some examples</a> - for later.</p><h1 id=fire-in-the-hole>Fire in the hole!</h1><p>Mid the gap between the Scala version and .jar when you&rsquo;re
parameterizing with your Apache Spark version:</p><pre><code class=language-bash>spark-submit --master k8s://https://$(minikube ip):8443 \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=2 \
    --executor-memory 1024m \
    --conf spark.kubernetes.container.image=spark:latest \
    local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # here
</code></pre><p>What&rsquo;s new is:</p><ul><li><code>--master</code>: Accepts a prefix <code>k8s://</code> in the URL, for the
Kubernetes master API endpoint, exposed by the command
<code>https://$(minikube ip):8443</code>. BTW, in case you want to
know, it&rsquo;s a
<a href=https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html target=_blank rel=noopener>shell command substitution</a>;</li><li><code>--conf spark.kubernetes.container.image=</code>: Configures the Docker
image to run in Kubernetes.</li></ul><p>Sample output:</p><pre><code>...

19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]

19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
</code></pre><p>To see the job result (and the whole execution) we can run a
<code>kubectl logs</code> passing the name of the driver pod as a parameter:</p><pre><code class=language-bash>kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')
</code></pre><p>Which brings the output (omitted some entries), similar to:</p><pre><code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
...
</code></pre><p>The result appears in:</p><pre><code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
</code></pre><p>Finally, let&rsquo;s delete the VM that Minikube generates, to clean up the
environment (unless you want to keep playing with it):</p><pre><code class=language-bash>minikube delete
</code></pre><h2 id=last-words>Last words</h2><p>I hope your curiosity got <em>sparked</em> and some ideas for further
development have raised for your Big Data workloads. If you have any
doubt or suggestion, don&rsquo;t hesitate to share on the comment section.</p></div><div class=article-tags><a class="badge badge-light" href=https://macunha.me/en/tag/data-engineering/>data-engineering</a>
<a class="badge badge-light" href=https://macunha.me/en/tag/kubernetes/>kubernetes</a></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=https://macunha.me/en/author/matheus-cunha/avatar_hu538abb559021e5e818cc3fcb6b905317_34815_270x270_fill_q90_lanczos_center.jpg alt="Matheus Cunha"><div class=media-body><h5 class=card-title><a href=https://macunha.me>Matheus Cunha</a></h5><h6 class=card-subtitle>Systems Engineer and Magician</h6><p class=card-text>Just a technology lover empowering business with high-tech computing to help innovation (:</p><ul class=network-icon aria-hidden=true><li><a href=https://github.com/macunha1/ target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://gitlab.com/macunha/ target=_blank rel=noopener><i class="fab fa-gitlab"></i></a></li><li><a href=https://twitter.com/_macunha1 target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.linkedin.com/in/macunha/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://medium.com/@macunha1 target=_blank rel=noopener><i class="fab fa-medium"></i></a></li></ul></div></div><section id=comments><div id=disqus_thread></div><script>let disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='https://'+"macunha1-me"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/en/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script id=dsq-count-scr src=https://macunha1-me.disqus.com/count.js async></script><script src=https://macunha.me/js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-sa/4.0 rel="noopener noreferrer" target=_blank><img src=https://search.creativecommons.org/static/img/cc_icon.svg alt="CC icon">
<img src=https://search.creativecommons.org/static/img/cc-by_icon.svg alt="CC by icon">
<img src=https://search.creativecommons.org/static/img/cc-nc_icon.svg alt="CC NC icon">
<img src=https://search.creativecommons.org/static/img/cc-nd_icon.svg alt="CC ND icon">
<img src=https://search.creativecommons.org/static/img/cc-sa_icon.svg alt="CC SA icon"></a></p><p class=powered-by>Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>