<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>It's me, Macunha!</title><link>https://macunha.me/en/</link><atom:link href="https://macunha.me/en/index.xml" rel="self" type="application/rss+xml"/><description>It's me, Macunha!</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 21 May 2020 23:00:57 +0200</lastBuildDate><image><url>https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_512x512_fill_lanczos_center_2.png</url><title>It's me, Macunha!</title><link>https://macunha.me/en/</link></image><item><title>Quickstart: Apache Spark on Kubernetes</title><link>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</link><pubDate>Thu, 21 May 2020 23:00:57 +0200</pubDate><guid>https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="the-apache-spark-operator-for-kubernetes">The Apache Spark Operator for Kubernetes&lt;/h2>
&lt;p>Since its launch in 2014 by Google, Kubernetes has gained a lot of
popularity along with Docker itself and since 2016 has become the &lt;em>de
facto Container Orchestrator&lt;/em>, established as a market standard.
Having cloud-managed versions available in &lt;strong>all&lt;/strong> the &lt;em>major Clouds&lt;/em>.
&lt;a href="https://cloud.google.com/kubernetes-engine/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://docs.microsoft.com/en-us/azure/aks/" target="_blank" rel="noopener">[3]&lt;/a> (including
&lt;a href="https://www.digitalocean.com/products/kubernetes/" target="_blank" rel="noopener">Digital Ocean&lt;/a> and
&lt;a href="https://www.alibabacloud.com/product/kubernetes" target="_blank" rel="noopener">Alibaba&lt;/a>).&lt;/p>
&lt;p>With this popularity came various implementations and &lt;em>use-cases&lt;/em> of
the orchestrator, among them the execution of
&lt;a href="https://kubernetes.io/docs/tutorials/stateful-application/" target="_blank" rel="noopener">Stateful
applications&lt;/a>
including
&lt;a href="https://vitess.io/zh/docs/get-started/kubernetes/" target="_blank" rel="noopener">databases using containers&lt;/a>.&lt;/p>
&lt;p>What would be the motivation to host an orchestrated database? That&amp;rsquo;s
a great question. But let&amp;rsquo;s focus on the
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md" target="_blank" rel="noopener">Spark Operator&lt;/a>
running workloads on Kubernetes.&lt;/p>
&lt;p>A native Spark Operator
&lt;a href="https://github.com/kubernetes/kubernetes/issues/34377" target="_blank" rel="noopener">idea came out&lt;/a>
in 2016, before that you couldn&amp;rsquo;t run Spark jobs natively except
some &lt;em>hacky alternatives&lt;/em>, like
&lt;a href="https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/" target="_blank" rel="noopener">running Apache Zeppelin&lt;/a>
inside Kubernetes or creating your
&lt;a href="https://github.com/kubernetes/examples/tree/master/staging/spark" target="_blank" rel="noopener">Apache Spark cluster inside
Kubernetes (from the official Kubernetes organization on GitHub)&lt;/a>
referencing the
&lt;a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Spark workers in Stand-alone mode&lt;/a>.&lt;/p>
&lt;p>However, the native execution would be far more interesting for taking
advantage of
&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank" rel="noopener">Kubernetes Scheduler&lt;/a>
responsible for taking action of allocating resources, giving
elasticity and an simpler interface to manage Apache Spark workloads.&lt;/p>
&lt;p>Considering that,
&lt;a href="https://issues.apache.org/jira/browse/SPARK-18278" target="_blank" rel="noopener">Apache Spark Operator development got attention&lt;/a>,
merged and released into
&lt;a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">Spark version 2.3.0&lt;/a>
launched in
&lt;a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">February, 2018&lt;/a>.&lt;/p>
&lt;p>If you&amp;rsquo;re eager for reading more regarding the Apache Spark proposal,
you can head to the
&lt;a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target="_blank" rel="noopener">design document published in Google Docs.&lt;/a>&lt;/p>
&lt;h2 id="why-kubernetes">Why Kubernetes?&lt;/h2>
&lt;p>As companies are currently seeking to
&lt;a href="https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html" target="_blank" rel="noopener">reinvent themselves through the
widely spoken digital transformation&lt;/a>
in order for them to be competitive and, above all, to survive in an
increasingly dynamic market, it is common to see approaches that
include Big Data, Artificial Intelligence and Cloud Computing
&lt;a href="https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/" target="_blank" rel="noopener">[3]&lt;/a>.&lt;/p>
&lt;p>An interesting comparison between the benefits of using Cloud Computing in the
context of Big Data instead of On-premises&amp;rsquo; servers can be read at
&lt;a href="https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html" target="_blank" rel="noopener">Databricks
blog&lt;/a>,
which is the company
&lt;a href="https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/" target="_blank" rel="noopener">founded by the creators of Apache Spark&lt;/a>.&lt;/p>
&lt;p>As we see a widespread adoption of Cloud Computing (even by companies
that would be able to afford the hardware and run on-premises), we
notice that most of these Cloud implementations don&amp;rsquo;t have an
&lt;a href="https://hadoop.apache.org/" target="_blank" rel="noopener">Apache
Hadoop&lt;/a> since the Data Teams (BI/Data
Science/Analytics) increasingly choose to use tools like
&lt;a href="https://cloud.google.com/bigquery/" target="_blank" rel="noopener">Google
BigQuery&lt;/a> or
&lt;a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">AWS Redshift&lt;/a>.
Therefore, it doesn&amp;rsquo;t make sense to spin-up a Hadoop with the only intention to
use
&lt;a href="https://hortonworks.com/apache/yarn/" target="_blank" rel="noopener">YARN&lt;/a> as the resources manager.&lt;/p>
&lt;p>An alternative is the use of Hadoop cluster providers such as
&lt;a href="https://cloud.google.com/dataproc" target="_blank" rel="noopener">Google
DataProc&lt;/a> or
&lt;a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener">AWS EMR&lt;/a>
for the creation of ephemeral clusters. Just to name a few options.&lt;/p>
&lt;p>To better understand the design of Spark Operator, the doc from
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operatoR/blob/master/docs/design.md#the-crd-controller" target="_blank" rel="noopener">GCP on GitHub&lt;/a>
is a no-brainer.&lt;/p>
&lt;h1 id="lets-get-hands-on">Let&amp;rsquo;s get hands-on!&lt;/h1>
&lt;h2 id="warming-up-the-engine">Warming up the engine&lt;/h2>
&lt;p>Now that the word has been spread, let&amp;rsquo;s get our hands on it to show
the engine running. For that, let&amp;rsquo;s use:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://www.docker.com/" target="_blank" rel="noopener">Docker&lt;/a> as the container engine for
Kubernetes
&lt;a href="https://docs.docker.com/install/" target="_blank" rel="noopener">(installation guide)&lt;/a>;&lt;/li>
&lt;li>Minikube
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">(installation guide)&lt;/a>
to facilitate the provisioning of the Kubernetes (yes, it will be
a local execution);&lt;/li>
&lt;li>For interaction with the Kubernetes API it is necessary to have
&lt;code>kubectl&lt;/code> installed,
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">if you don&amp;rsquo;t have it, follow instructions
here&lt;/a>.&lt;/li>
&lt;li>a compiled version of Apache Spark larger than 2.3.0.
&lt;ol>
&lt;li>you can either compile
&lt;a href="https://github.com/apache/spark" target="_blank" rel="noopener">source code&lt;/a>,
which will took &lt;em>some hours&lt;/em> to finish, or&lt;/li>
&lt;li>download a compiled version
&lt;a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">here&lt;/a>
(recommended).&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>Once the necessary tools are installed, it&amp;rsquo;s necessary to
include Apache Spark path in &lt;code>PATH&lt;/code> environment variable, to ease the
invocation of Apache Spark executables. Simply run:&lt;/p>
&lt;pre>&lt;code class="language-bash">export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin
&lt;/code>&lt;/pre>
&lt;h2 id="creating-the-minikube-cluster">Creating the Minikube &amp;ldquo;cluster&amp;rdquo;&lt;/h2>
&lt;p>At last, to have a Kubernetes &amp;ldquo;cluster&amp;rdquo; we will start a &lt;code>minikube&lt;/code>
with the intention of running an example from
&lt;a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala" target="_blank" rel="noopener">Spark
repository&lt;/a>
called &lt;code>SparkPi&lt;/code> just as a demonstration.&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube start --cpus=2 \
--memory=4g
&lt;/code>&lt;/pre>
&lt;h2 id="building-the-docker-image">Building the Docker image&lt;/h2>
&lt;p>Let&amp;rsquo;s use the Minikube Docker daemon to not depend on an external registry (and
only generate Docker image layers on the VM, facilitating garbage disposal
later). Minikube has a wrapper that makes our life easier:&lt;/p>
&lt;pre>&lt;code class="language-bash">eval $(minikube docker-env)
&lt;/code>&lt;/pre>
&lt;p>After having the daemon environment variables configured, we need a
Docker image to run the jobs. There is a
&lt;a href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh" target="_blank" rel="noopener">shell script in the Spark
repository&lt;/a>
to help with this. Considering that our &lt;code>PATH&lt;/code> was properly
configured, just run:&lt;/p>
&lt;pre>&lt;code class="language-bash">docker-image-tool.sh -m -t latest build
&lt;/code>&lt;/pre>
&lt;p>&lt;em>FYI:&lt;/em> The &lt;code>-m&lt;/code> parameter here indicates a minikube build.&lt;/p>
&lt;p>Let&amp;rsquo;s take the highway to execute SparkPi, using the same command
that would be used for a Hadoop Spark cluster
&lt;a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">spark-submit&lt;/a>.&lt;/p>
&lt;p>However, Spark Operator supports defining jobs in the &amp;ldquo;Kubernetes
dialect&amp;rdquo; using
&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">CRD&lt;/a>,
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples" target="_blank" rel="noopener">here are some examples&lt;/a> - for later.&lt;/p>
&lt;h1 id="fire-in-the-hole">Fire in the hole!&lt;/h1>
&lt;p>Mid the gap between the Scala version and .jar when you&amp;rsquo;re
parameterizing with your Apache Spark version:&lt;/p>
&lt;pre>&lt;code class="language-bash">spark-submit --master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=2 \
--executor-memory 1024m \
--conf spark.kubernetes.container.image=spark:latest \
local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # here
&lt;/code>&lt;/pre>
&lt;p>What&amp;rsquo;s new is:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--master&lt;/code>: Accepts a prefix &lt;code>k8s://&lt;/code> in the URL, for the
Kubernetes master API endpoint, exposed by the command
&lt;code>https://$(minikube ip):8443&lt;/code>. BTW, in case you want to
know, it&amp;rsquo;s a
&lt;a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html" target="_blank" rel="noopener">shell command substitution&lt;/a>;&lt;/li>
&lt;li>&lt;code>--conf spark.kubernetes.container.image=&lt;/code>: Configures the Docker
image to run in Kubernetes.&lt;/li>
&lt;/ul>
&lt;p>Sample output:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&amp;gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&amp;gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
&lt;/code>&lt;/pre>
&lt;p>To see the job result (and the whole execution) we can run a
&lt;code>kubectl logs&lt;/code> passing the name of the driver pod as a parameter:&lt;/p>
&lt;pre>&lt;code class="language-bash">kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')
&lt;/code>&lt;/pre>
&lt;p>Which brings the output (omitted some entries), similar to:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
...
&lt;/code>&lt;/pre>
&lt;p>The result appears in:&lt;/p>
&lt;pre>&lt;code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
&lt;/code>&lt;/pre>
&lt;p>Finally, let&amp;rsquo;s delete the VM that Minikube generates, to clean up the
environment (unless you want to keep playing with it):&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube delete
&lt;/code>&lt;/pre>
&lt;h2 id="last-words">Last words&lt;/h2>
&lt;p>I hope your curiosity got &lt;em>sparked&lt;/em> and some ideas for further
development have raised for your Big Data workloads. If you have any
doubt or suggestion, don&amp;rsquo;t hesitate to share on the comment section.&lt;/p></description></item><item><title>ReclameAQUI Data Lake</title><link>https://macunha.me/en/project/2019/reclameaqui-data-lake/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2019/reclameaqui-data-lake/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>ReclameAQUI (Portuguese for &amp;ldquo;complain here&amp;rdquo;) is an interesting and unique
business. They&amp;rsquo;re a content aggregator for customers&amp;rsquo; experience sharing
(especially bad experiences) about shopping (online and offline). However, it
goes further than a mere &amp;ldquo;complaints website&amp;rdquo; offering an interface for
companies to answers complaints, helping customers with their issues.&lt;/p>
&lt;p>The service is simply the biggest in this regard (worldwide) receiving 600K
unique visitors each day, searching for a company&amp;rsquo;s reputation before closing a
deal/purchase.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Even though they are already advanced in the digital approach to business,
having most services hosted on Cloud computing and analytical culture, their
data lake needed some upgrades. The most relevant motivator of this project was
the sky-high bills from GCP especially related to BigQuery data consumption.&lt;/p>
&lt;p>Apart from the cost-reduction tasks and data ingestion process optimization, we
took the opportunity to implement data cryptograph at-rest, governance, and
obfuscation during query executions against the data lake. Making data
accessible by everyone in the company, controlling identity access and
management through LDAP (auditing each access, to be fully compliant with
GDPR), we could offer a self-service data lake so different business actors
could satisfy their needs &amp;ldquo;drinking&amp;rdquo; from the lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="tech-implementation">Tech implementation&lt;/h2>
&lt;p>Key objectives were cost-optimization of the existing Data Lake, improvement
(and extension) of existing data ingestion pipelines, and security enhancements.&lt;/p>
&lt;p>Starting from Data Lake&amp;rsquo;s cost optimization, we redesigned the data ingestion,
using a &amp;ldquo;landing&amp;rdquo; area for raw data, making data transformations later to suit
the desired data models. Saving the results in other Data Lake layers to achieve
greater performance in queries.&lt;/p>
&lt;p>We shifted away from the Streaming inserts in BigQuery by adding a step to load
data at the end of the ingestion pipeline. Apache NiFi was the main software
responsible for orchestrating and executing the pipeline, covering also the
improvements in data ingestion through processes re-engineering.&lt;/p>
&lt;p>Auditing in the Data Lake was managed through Apache Ranger. In order to have
it fully supported we implemented a JDBC driver using a component from Apache
Calcite called Avatica. Authentication for Apache Ranger went through a custom
plugin (also developed during the project) for LDAP consuming user info from
Google Cloud Identity, reflecting the existing organization&amp;rsquo;s users and groups
from Google Suite.&lt;/p>
&lt;p>To make the game more interesting, we containerized the workflow and heavily
used Kubernetes (GKE) to manage these components. Most of the Apache projects
didn&amp;rsquo;t have Helm Charts at the time and we developed and made some
of them open-source.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>During project time we could measure an estimative of roughly 56% in Data Lake
cost-optimization through reengineering of processes and resources, especially
the removal of streaming inserts to BigQuery.&lt;/p>
&lt;p>We made relevant progress in security and governance during the project with the
introduction of Apache Ranger and Data Lake auditing for access and usage,
providing advanced security capabilities to ReclameAQUI, which anticipated itself
towards GDPR and data privacy concerns.&lt;/p></description></item><item><title>DevOps: Benefits</title><link>https://macunha.me/en/post/2019/01/devops-benefits/</link><pubDate>Fri, 11 Jan 2019 22:00:00 +0000</pubDate><guid>https://macunha.me/en/post/2019/01/devops-benefits/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Main benefits that a company generally expects and finds in the adoption of culture:&lt;/p>
&lt;h2 id="faster-and-cheaper-releases">Faster and Cheaper Releases&lt;/h2>
&lt;p>Since releases will be continuous and frequent, deliverables will turn into small changes with the benefit of increasing speed in the development cycle (delivering always).&lt;/p>
&lt;h2 id="improved-operational-support-with-quick-fixed">Improved Operational support with quick fixed&lt;/h2>
&lt;p>If there is a failure during delivery, the impact is minimal because the amount of modifications is small, just as the rollback is faster. Having a simple inspection and debugging.&lt;/p>
&lt;h2 id="better-time-to-market-ttm">Better Time-to-market (TTM)&lt;/h2>
&lt;p>The software will be delivered much earlier when it&amp;rsquo;s still an MVP. Customers will be integrated as part of the development process, bringing insights and feedback to the development team. Thus allowing for a higher launch speed in the market.&lt;/p>
&lt;h2 id="superior-quality-products">Superior quality products&lt;/h2>
&lt;p>As has been said before, early failures prevent defects from being delivered to production, because:&lt;/p>
&lt;ul>
&lt;li>Reduces the volume of defects in the product as a whole;&lt;/li>
&lt;li>Increases frequency of new features and releases;&lt;/li>
&lt;li>Appropriate development processes in teams, including automation.&lt;/li>
&lt;/ul>
&lt;h1 id="now-we-understood-why-lets-talk-about-how">Now we understood WHY, let&amp;rsquo;s talk about HOW&lt;/h1>
&lt;h2 id="continuous-releases-integration-delivery-deployment">Continuous releases (integration, delivery, deployment)&lt;/h2>
&lt;p>Usually follows a code versioning approach (through Git) using specific branches for each environment (e.g.: feature branches with git flow).&lt;/p>
&lt;h2 id="continuous-integration">Continuous integration&lt;/h2>
&lt;p>Automatic execution of unit tests, integration tests and code quality analysis against a git branch, to ensure that there was no disruption of the modified piece of code.&lt;/p>
&lt;h2 id="continuous-delivery">Continuous delivery&lt;/h2>
&lt;p>Packaging the software that is tested and approved, to deliver it somewhere that it is possible to use in a deploy later. Examples are libs delivered in repositories to be integrated into the code during the next update and code deploy.&lt;/p>
&lt;h2 id="continuous-deployment">Continuous deployment&lt;/h2>
&lt;p>Once you have completed all of the above steps, you can do automated deployments right in the environments, when the team is more confident about the tools they are testing, as well as the risk they&amp;rsquo;re taking and also understanding that there is a possibility of failure in a tests environment without worrying that it&amp;rsquo;s going to be divergent from production.&lt;/p>
&lt;h2 id="configuration-andor-infrastructure-as-code">Configuration (and/or Infrastructure) as code&lt;/h2>
&lt;p>To be able to test software with assertiveness, and to understand that it will transit between environments without changing behavior, it is essential that the configurations are also expressed in code. This allows the settings to be also versioned, following the code. Also guaranteeing a uniformity among the environments, which enables:&lt;/p>
&lt;ul>
&lt;li>Reduction in maintenance costs, having a single point to look at and understand the operation of the system;&lt;/li>
&lt;li>Easy to recreate the infrastructure, if it is necessary to move everything to another place, this can happen with a few manual interactions;&lt;/li>
&lt;li>Allows for a code review of infrastructure and configurations, which consequently brings a culture of collaboration in the development, sharing of knowledge and increases the democratization of the infra;&lt;/li>
&lt;li>Documentation as code, helping new team members get a faster warm up.&lt;/li>
&lt;/ul>
&lt;p>These points were well-stressed by the Heroku team and gave rise to the famous paper: [The Twelve-Factor App] (&lt;a href="https://12factor.net/)">https://12factor.net/)&lt;/a>. It&amp;rsquo;s an excellent reading for the explanation of the benefits of configuration management.&lt;/p>
&lt;h2 id="observability-monitoring-and-self-healing">Observability, Monitoring, and self-healing&lt;/h2>
&lt;p>At the end of the delivery process, the software must be monitored. Avoiding to wait for an external report of failures, ensuring that the actions are proactive rather than reactive.&lt;/p>
&lt;p>With mature monitoring, it&amp;rsquo;s possible to create trigger against alerts, creating a self-healing system in which actions (scripts) are performed to &lt;strong>fix known&lt;/strong> failures in the infrastructure so that everyone can sleep peacefully at night, without having to worry about the on-call schedule that makes you read some documentation at dawn. (If you have had experience with this, you know for sure how bad it is).&lt;/p>
&lt;p>Scaling up only those cases that are extreme exceptions (mistakes not known/expected) in the process for the employee to act, ensuring higher health in operation.&lt;/p>
&lt;h2 id="processes-automation">Processes automation&lt;/h2>
&lt;p>All processes that cause Muda should be addressed with automation, allowing people to work more quickly. Good examples of processes that are usually automated are:&lt;/p>
&lt;ul>
&lt;li>Deployment;&lt;/li>
&lt;li>Self-healing (system resilience in response to anomalies);&lt;/li>
&lt;li>Renewal of Certificates;&lt;/li>
&lt;li>Execution of tests (unitary, integration, functional, etc.);&lt;/li>
&lt;li>Monitoring (with auto-discovery);&lt;/li>
&lt;li>User Governance;&lt;/li>
&lt;/ul>
&lt;h1 id="devops-toolchainhttpsenwikipediaorgwikidevops_toolchain">
&lt;a href="https://en.wikipedia.org/wiki/DevOps_toolchain" target="_blank" rel="noopener">DevOps toolchain&lt;/a>&lt;/h1>
&lt;p>A combination of tools to facilitate the maintenance and operation of the system, with the flow:&lt;/p>
&lt;p>&lt;img src="https://macunha.me/img/content/devops-lifecycle.png" alt="Development Cycle Using DevOps">&lt;/p>
&lt;blockquote>
&lt;p>Note: Any similarity to the PDCA is pure certainty.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Plan&lt;/strong>: Project planning phase, in which feedbacks are collected for requirements survey, and backlog creation;&lt;/li>
&lt;li>&lt;strong>Create&lt;/strong>: Creation of a deliverable (to validate a hypothesis), such as an MVP;&lt;/li>
&lt;li>&lt;strong>Verify&lt;/strong>: Pass the deliverable to the test phase;&lt;/li>
&lt;li>&lt;strong>Package:&lt;/strong> Package the build to be able to put it in some testing environment;&lt;/li>
&lt;li>&lt;strong>Release&lt;/strong>: Deploy packaged deliverable;&lt;/li>
&lt;li>&lt;strong>Configure&lt;/strong>: Perform the configuration of the deliverable in the testing environment, trying to get as close as possible to the twelve-factor app.&lt;/li>
&lt;li>&lt;strong>Monitor&lt;/strong>: After deploying to the environment, track business metrics and infrastructure to ensure everything is working as expected.&lt;/li>
&lt;/ul>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>During the implementation of these techniques it is possible to observe improvements in the development process, the most notable gains are:&lt;/p>
&lt;ul>
&lt;li>Increase in team engagement;&lt;/li>
&lt;li>Knowledge sharing;&lt;/li>
&lt;li>Reduction of bottlenecks;&lt;/li>
&lt;li>More free time to do work that really matters (adds value to the user experience or generates impact);&lt;/li>
&lt;li>Greater confidence in delivering software.&lt;/li>
&lt;/ul></description></item><item><title>DevOps: The Genesis</title><link>https://macunha.me/en/post/2019/01/devops-genesis/</link><pubDate>Fri, 11 Jan 2019 21:00:00 +0000</pubDate><guid>https://macunha.me/en/post/2019/01/devops-genesis/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>First of all, it&amp;rsquo;s all about agile.&lt;/p>
&lt;p>The DevOps methodology was created on top of agile methods, to deliver a higher value inside software releases, automating feature release through pipelines, that can test hypothesis faster allowing higher adaptability using &amp;ldquo;fail-fast&amp;rdquo; approaches. Those changes are more cultural than technical, so it&amp;rsquo;s normal to see DevOps being called culture.&lt;/p>
&lt;p>The implementation of DevOps happens through processes automation, having a strong sense of processes re-engineering inside the company. Comparing to the cultural change, the technical is easy to implement. Therefore the role that a &amp;ldquo;DevOps Engineer/Analyst&amp;rdquo; performs is very confusing, which enables many SysAdmins and Infra Analysts assuming the role of &amp;ldquo;DevOps.&amp;rdquo;&lt;/p>
&lt;h2 id="lean-is-the-basis-of-agile">Lean is the basis of Agile&lt;/h2>
&lt;p>Reality is not as happy as it sounds. After World War II, Japan was destroyed and under-resourced after losing the war. With a limited amount of resources, the country needed to reinvent itself and survive after a time of severe depression. During that time two guys gained attention inside a company that later gave its name after the methodology.&lt;/p>
&lt;p>Those guys were Eiji Toyoda and Taiichi Ohno, inside Toyota Motor Corporation. They&amp;rsquo;re the founders of the &amp;ldquo;Toyota production model&amp;rdquo; also known as Toyotism.&lt;/p>
&lt;h2 id="toyota-gave-birth-to-lean">Toyota gave birth to Lean&lt;/h2>
&lt;p>Lean teaches how to optimize the end-to-end process, focusing on processes that create value for customers. Bottlenecks in the process must be removed, and wasteful activities need to be identified and avoided. Both explained and defined by LEAN 3M: Muda, Mura, and Muri.&lt;/p>
&lt;p>Also teaches to improve yourself day after day and always focus on quality through Kaizen (continuous improvement).&lt;/p>
&lt;p>Japanese culture truly believes that quality is the main objective to deliver value to customers since quality is what brings your clients back.&lt;/p>
&lt;h2 id="kaizen">Kaizen&lt;/h2>
&lt;p>A mindset that helps to look at each part of the process exclusively and think about the improvements. Involving the people who are part of the process, encourage the inclusion of these people in the decisions of change, since:&lt;/p>
&lt;ul>
&lt;li>It is much easier to accept a change when it is not imposed (top-down);&lt;/li>
&lt;li>There is a greater absorption of change by people when they&amp;rsquo;re included in the planning;&lt;/li>
&lt;li>The people who are involved in the process bring their concerns and suggestions, which contribute positively to the evolution of the change, making the idea more robust.&lt;/li>
&lt;/ul>
&lt;p>The process of defining improvements through Kaizen happens (usually) in the following order:&lt;/p>
&lt;ol>
&lt;li>Define data-driven objectives;&lt;/li>
&lt;li>Review the current state and develop an improvement plan;&lt;/li>
&lt;li>Implement improvement;&lt;/li>
&lt;li>Review the implementation and improve what does not work;&lt;/li>
&lt;li>Report the results and determine the items to be monitored.&lt;/li>
&lt;/ol>
&lt;p>This process is also called &lt;strong>PDCA: Plain-Do-Control-Act&lt;/strong>, which is summarized in:&lt;/p>
&lt;ul>
&lt;li>Plan (develop the hypothesis);&lt;/li>
&lt;li>Do (experiment);&lt;/li>
&lt;li>Check (validate results);&lt;/li>
&lt;li>Act (refine the experiment and start over).&lt;/li>
&lt;/ul>
&lt;h1 id="3m-muda-mura-muri">3M: Muda, Mura, Muri&lt;/h1>
&lt;h2 id="muda-waste">Muda (waste)&lt;/h2>
&lt;p>Any activity that consumes time without adding value to the final consumer. e.g.:&lt;/p>
&lt;ul>
&lt;li>over-production;&lt;/li>
&lt;li>idle time in the process;&lt;/li>
&lt;li>products with a defect.&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s important to remember that there are different levels of Muda that can be removed quickly or not, and the classification depends on the time for removal.&lt;/p>
&lt;p>An example of a more time-consuming Muda is the discontinuation of legacy software that ends up with longer release cycles, causing teams to be idle, followed by an often long or manual test routine.&lt;/p>
&lt;h2 id="mura-unevenness">Mura (unevenness)&lt;/h2>
&lt;p>Unevenness in operation, caused by activities that are very changeable and unpredictable, generating different results in all executions. e.g., the execution of tasks that were not well planned and ended up arriving with strict deadlines. The team runs in the rush, generating exhaustion, despair, and moreover, when finished leaves the people who have performed these tasks waiting (for feedback, or confirmation that it is completed).&lt;/p>
&lt;h2 id="muri-overload">Muri (overload)&lt;/h2>
&lt;p>Overburdening equipment or operators by requiring them to run at a higher or harder pace beyond the limit, to achieve some goal or expectation, causing fatigue and consequently failures during the process. These failures are usually human errors caused by fatigue during overwork.&lt;/p>
&lt;h2 id="back-to-agile">Back to Agile&lt;/h2>
&lt;p>In 2000 a group of 17 people met at a resort in Oregon to talk about ideas that could improve the flow of software development. After a year of mature ideas, these people met again and published the ideas, which we now know as &lt;strong>Agile Manifesto&lt;/strong>.&lt;/p>
&lt;p>Main points are:&lt;/p>
&lt;p>&lt;strong>Individuals and interactions&lt;/strong> over processes and tools
&lt;strong>Working software&lt;/strong> over comprehensive documentation
&lt;strong>Customer collaboration&lt;/strong> over contract negotiation
&lt;strong>Responding to change&lt;/strong> over following a plan&lt;/p>
&lt;p>I will restrict the explanation of these points with the DevOps point of view, keeping on track (now).&lt;/p>
&lt;h2 id="individuals-and-interactions">Individuals and interactions&lt;/h2>
&lt;p>&lt;em>over processes and tools&lt;/em>&lt;/p>
&lt;p>First comes the individuals, they should receive the necessary tooling to work with, and then be empowered to do their jobs. Interactions between people are greatly encouraged, for sharing knowledge and also for facilitating creative flow within development teams.&lt;/p>
&lt;p>An excellent example of interaction encouraged through DevOps is the code review habit. Considering that small parts of the software will be iterated and approved in the pipeline passing through different environments, automatically, the best way to prevent defects is through code review.&lt;/p>
&lt;p>This habit brings benefits such as:&lt;/p>
&lt;ul>
&lt;li>Knowledge sharing;&lt;/li>
&lt;li>Observation of the problem from a different point of view;&lt;/li>
&lt;li>Team engagement;&lt;/li>
&lt;li>Lesser bugs.&lt;/li>
&lt;/ul>
&lt;h2 id="working-software">Working software&lt;/h2>
&lt;p>&lt;em>over comprehensive documentation&lt;/em>&lt;/p>
&lt;p>Here&amp;rsquo;s a trick in &amp;ldquo;working software,&amp;rdquo; software that works is not code that compiles. The software that works is what meets the requirements of the user; i.e., the software that solves the problem and the pains of the user.&lt;/p>
&lt;p>As the market is very dynamic, and evolves with high speed, often during the software development project the requirements change due to external factors. Therefore, knowing that it is not possible to predict all the elements, many &amp;ldquo;workarounds&amp;rdquo; are made during development and documented. Passing the responsibility to the user to handle the faults, and perform the workarounds, expending more effort than would be required to perform the tasks using the software.&lt;/p>
&lt;blockquote>
&lt;p>Deliver a working software frequently, ranging from a few weeks to a few months, considering shorter time-scale. - Agile Manifesto&lt;/p>
&lt;/blockquote>
&lt;p>Encouraging as many deployments as possible, so that failures happen as early as possible, thus allowing their impact to be much less.&lt;/p>
&lt;h1 id="fail-fast">Fail-fast!&lt;/h1>
&lt;p>Failures are understood and encouraged because it&amp;rsquo;s part of the mindset. Because:&lt;/p>
&lt;ul>
&lt;li>Only those who &lt;strong>do&lt;/strong> make mistakes;&lt;/li>
&lt;li>Shit happens.&lt;/li>
&lt;/ul>
&lt;p>Therefore, it&amp;rsquo;s best for failures to occur early, while the cost of correction is still low. Failing a controlled testing environment allows the fix to be much faster (and cheaper) than it would if the fix were already in production.&lt;/p>
&lt;p>For this approach to succeed, there is a premise that environments are production copies, or at least as close as possible. Otherwise, there will be behavioral changes in the software between the environments, making the test environment unfeasible.&lt;/p>
&lt;p>If the environments are divergent, the promotion of bugs for production will be very frequent, causing late failures, which are expensive failures.&lt;/p>
&lt;h2 id="customer--collaboration">Customer collaboration&lt;/h2>
&lt;p>&lt;em>over contract negotiation&lt;/em>&lt;/p>
&lt;p>Know your client! Including it in the process is the best approach to have working software. After iterating over deliverables, it&amp;rsquo;s essential to create a positive feedback loop with your client, bringing it as close as possible to the development of the tools that he/she is going to use.&lt;/p>
&lt;p>We can describe this situation with:&lt;/p>
&lt;ul>
&lt;li>From point A it is possible to see only point B;&lt;/li>
&lt;li>From point B it is possible to see point C;&lt;/li>
&lt;/ul>
&lt;p>Therefore there is a great incentive for the software to be delivered in parts, continuously. Thus gathering user feedback on the next steps, following the concepts of evolutionary prototyping, which were widely publicized through
&lt;a href="http://theleanstartup.com/book" target="_blank" rel="noopener">&lt;em>The Lean Startup&lt;/em>&lt;/a>.&lt;/p>
&lt;p>This point contrasts sharply with the previous one about continuous release, so that it is possible to present the prototype and evolve it throughout the project.&lt;/p>
&lt;p>Learn who your customer/consumer/user is, and whom you are making the software for, as this is the only way you can deliver value to that customer. An essential part of the software development process is to be empathic with user problems, and to truly understand what the problem is to be solved, and the result of the impact on software development (value creation for the user).&lt;/p>
&lt;h2 id="responding-to-change">Responding to change&lt;/h2>
&lt;p>&lt;em>over following a plan&lt;/em>&lt;/p>
&lt;p>Redesigning the requirements overtime is part of the job, and a necessary step to success. If you want to build something useful that is going to grow and have absorption, it&amp;rsquo;s a key feature to include your client in the implementation process.&lt;/p>
&lt;p>It will be the only way to bring all the problems of the user to the table and create the best solution for all these problems because the user is the only person that knows the real challenges he faces in their routine dealing with software.&lt;/p>
&lt;p>With continuous delivery of software along with monitoring results, the process of collecting feedback is much simpler and faster.&lt;/p>
&lt;h1 id="devops-devops-devops">DevOps, DevOps, DevOps&lt;/h1>
&lt;p>With the popularization of DevOps, a lot of disagreement came out there followed by a significant confusion about the subject. It is very common to come across different interpretations of &lt;strong>what is DevOps&lt;/strong>. There is a lot of euphemism in the area, and gourmetization on LinkedIn, with many SysAdmins calling themselves DevOps since they learned to code shell script inside Python.&lt;/p>
&lt;p>Do you want to keep reading?
&lt;a href="https://macunha.me/en/post/2019/01/devops-benefits/">Here are the benefits of adopting DevOps techniques.&lt;/a>&lt;/p></description></item><item><title>Clipping Service News OCR</title><link>https://macunha.me/en/project/2018/clipping-service-news-ocr/</link><pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2018/clipping-service-news-ocr/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>As the owner of the biggest Brazilian media data set, the media monitoring
leader Clipping Service was having issues with scalability, getting close to
their data center maximum capacity.&lt;/p>
&lt;p>Clipping Service operates on a huge scale, receiving around ~4.5K media press
pages per day from roughly 300 newspapers in both digital and printed versions.
Previously employees called &amp;ldquo;readers&amp;rdquo; were responsible for reading and clipping
(adding highlight into the targeted content) to be later passed onto the
&amp;ldquo;reviewers&amp;rdquo; team.&lt;/p>
&lt;p>As if the burden of reading countless pages a day were not enough, the readers&amp;rsquo;
operation begins around 4:30 a.m. when the &amp;ldquo;first reading&amp;rdquo; begins (i.e., the
delivery of the morning papers).&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>For over 20 years this content has been ingested by so-called &amp;ldquo;readers&amp;rdquo;. But due
to the advent of the internet and the digital press boom at the end of the 90&amp;rsquo;s,
and nowadays of social media, companies are transferring their clipping
investments to monitoring other areas. Therefore requiring a Clipping Service
action to remain competitive in the market.&lt;/p>
&lt;p>Through news reading automation using OCR, NLP, and artificial intelligence to
categorize media, the plan was to achieve a higher throughput during ingestion,
giving readers more free time to review the content. Consequently achieving a
higher quality in the content, since we as humans aren&amp;rsquo;t good at doing
repetitive tasks, especially when it comes to reading endless pages searching
for names and words.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="tech-implementation">Tech implementation&lt;/h2>
&lt;p>After spending some time researching and benchmarking the alternatives at hand
we decided to use Python as the implementation language for handling texts,
OCR, and NLP (using NLTK). Given its extended API and libraries for NLP and
image processing.&lt;/p>
&lt;p>As the cloud provider we choose AWS due to it&amp;rsquo;s stability and consistency over
other vendors, the conclusion at the time was: AWS price estimative 14.67%
greater than GCP. However, AWS&amp;rsquo;s popularity is greater than GCP and proven in
terms of stability, support, and integrity. Making a safer choice for a
slightly higher price.&lt;/p>
&lt;p>The tech stack was: Python 3 using Dramatiq as the task processing library,
running Tesseract OCR jobs, processing text with NLTK and images with Pillow
(ImageMagick wrapper). Redis was the message broker for Dramatiq, a simple
Postgres database stored metrics regarding the execution and we had an
Elasticsearch storing the processed content.&lt;/p>
&lt;p>Requests coming from the data center reached an API Gateway, responsible for
executing a Lambda function, and delivering the content result.&lt;/p>
&lt;p>The best part of the design? We stored and served the content via AWS S3. Each
part was designed with fault tolerance, and we simply turned off the entire
cloud infrastructure after the operation, to turn on only the next day.&lt;/p>
&lt;p>Operating only from 4am to 2pm, a &amp;ldquo;serverless&amp;rdquo; and ephemeral project benefiting
from an aggressive cloud cost reduction.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Clipping Service reduced its reading team workforce by ~78%, offering internal
hiring for other areas of the company and a voluntary dismissal plan with
benefits, making the process as human as possible for the former employees.&lt;/p>
&lt;p>Using automation for reading tasks, Clipping Service could reach considerable
improvements in the media press ingestion throughput (around 20 times faster),
offering higher quality in press clipping service for its customers and saw the
opportunity in creating a self-service press clipping service later, since the
operational cost decreased significantly.&lt;/p></description></item><item><title>Dotz Data Labs</title><link>https://macunha.me/en/project/2017/dotz-data-labs/</link><pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2017/dotz-data-labs/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>In order to be able to innovate and maintain itself in a highly changing
and evolving market, Dotz went through a process of digital
transformation and had the help of some consultants along the way.&lt;/p>
&lt;p>Among the steps to get closer to a digital model, the implementation of
a Data Lake emerged, with the requirements of being
&lt;a href="https://en.wikipedia.org/wiki/Serverless_computing" target="_blank" rel="noopener">serverless&lt;/a> and
cloud-native to support the decision-making process and shorten
time-to-market during the launch of products.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Dotz is one of the largest companies in the field of loyalty program in
Brazil and they&amp;rsquo;d face a high number of issues with data disconnection
making it difficult to analyze their users' behaviors. Since they
received data from numerous supermarkets and stores, it&amp;rsquo;s difficult to
clusterizate products, since the name is different depending on the
source. To help with this analysis, they decided to build a Data Lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>We built and deployed a Big Data's managed architecture using Google's
Cloud Platform (GCP) to support this strategy and allow a 360-degree
view of customers (users with points a.k.a. Dotz) and partners (the
supermarkets offering the loyalty program).&lt;/p>
&lt;p>The design was focused on cloud-managed services and serverless
computing offered by Google, serving the core competencies of a Data
Lake such as scalable storage using Google Cloud Storage, and Google
BigQuery. With part of the process running inside Kubernetes,
responsible for data cleansing ETL flow management.&lt;/p>
&lt;p>We streamed data with Apache Beam running under Google DataFlow,
parallel mass processing with Apache Spark jobs running on Google
DataProc, exploratory analysis with Google DataLab, Machine Learning
Analysis with Google ML and Data visualization in Google Data Studio.&lt;/p>
&lt;p>Data is transported using an event-driven model, where every data is
collected using a streaming model, even the ETL (which runs on a
micro-batch, to enable near real-time exploration). These data goes
through the data pipeline using Google Pub/Sub message-oriented
middleware, and every message is serialized using Avro format, which
reduces the payload and allows transportation to be cost-effective, fast
and reliable.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>It all allowed Dotz to have a better structure on their analytical
platform, previously managed inside a large MS SQL Server instance,
which moved to a Data Lake with layers allowing data categorization,
governance, quality and security.&lt;/p>
&lt;p>Supporting analytical processes of users data, faster exploration and
monetization of their knowledge on customers' behavior.&lt;/p></description></item><item><title>Easynvest Data Platform</title><link>https://macunha.me/en/project/2017/easynvest-data-platform/</link><pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2017/easynvest-data-platform/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Within Easynvest&amp;rsquo;s annual planning, an investment in the expansion of
data &amp;amp; analytics team aimed at shortening decision making process and
in delivering a higher quality to customers through a low cost operational
process.&lt;/p>
&lt;p>Among the main objectives of this project, we had the automation of
credit analysis (executed during the approval of customer registration,
using Machine Learning), a process that until then was long and manual,
being handled by the back office.&lt;/p>
&lt;p>Followed by a better offer of products to the client, carrying out the
categorization according to the profile of each customer, allowing
suggestions of more attractive products, in line with personal preferences,
as well as according to the profile of each investor (conservative,
moderate or aggressive).&lt;/p>
&lt;p>Last but not least, the intelligent detection of money laundry and
reporting to the responsible authorities.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>However, there were limitations in the data tools, mainly due to the
fact that they were proprietary software (with limited licenses) and
designed for usage inside data centers. In addition, the analytical
database was modeled for traditional Business Intelligence models
(OLAP, etc), making the decision making process heavy, due to the
demanding amount of interactions during ETL.&lt;/p>
&lt;p>Previously for a client to be approved, the process took 10 to 15 days.
Gathering all necessary information, providing a complete perspective of
the profile, including credit analysis. After collecting the information,
the back office generated an internal credit analysis score.&lt;/p>
&lt;p>In most cases, the client was not notified of updates regarding the
process and did not receive feedback at the end (if refused) unless
explicitly requested (by contacting support via chat or email, for
example) which made the process time consuming and costly. Not to
mention the countless amounts of customers lost to the competition during
this long wait.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>To make it possible, we built an hybrid-cloud implementation using AWS
cloud-based components (mainly AWS S3, EMR and ECS), to extend the data
centers' capability, implementing a cloud-first Hadoop ecosystem
(replacing the proprietary software components with open-source
equivalents). Giving Easynvest the possibility to grow the Data Lake
exponentially.&lt;/p>
&lt;p>The Data Lake design was robust, aimed to handle the execution of heavy
analytical processes through Machine Learning models, with support for
data quality, metadata governance, information security and data serving
(data owners could share data with consumers from other areas within the
company, allowing to self-service their analytical data).&lt;/p>
&lt;p>A Chatbot was also used to reduce the operational load in the
environment, this bot is responsible for maintaining and updating
infrastructure components. From triggering deployments to generating
encryption keys on-demand for data security and governance. Implemented
with the Errbot framework for Python interacting with Slack.&lt;/p>
&lt;p>Going further, we implemented the best practices in DevOps, using Jenkins
as a tool for CI/CD of the developed components alongside with Ansible for
Configuration Management.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Thanks to the utilization of layers in the Data Lake and the
implementation of data pipelines, we were able to reduce the data
ingestion time by 78% and to include metadata and data catalog, in
addition to automating much of the work that used to be done manually.&lt;/p>
&lt;p>Thus bringing positive results, especially reducing the registration
approval time for consumers from roughly 10 days to 1 day. It also made
the data platform more democratic, providing relevant information that
facilitates the analysis of areas such as risk (credit analysis) and
support, without having to give up security.&lt;/p></description></item><item><title>Movida Rent A DevOps</title><link>https://macunha.me/en/project/2016/movida-rent-a-devops/</link><pubDate>Tue, 07 Feb 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2016/movida-rent-a-devops/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>JSL Holdings Ltd, holder of Julio Simes Logistica (biggest logistics
players in LATAM) bought Movida Rent a Car in 2013 to expand the
portfolio and open new market opportunities on car rental and selling
markets.&lt;/p>
&lt;p>JSL invested around R$1.8 billion in Movida, and multiplied its annual
revenues by 21 times from BRL 58m to BRL 1.2b, in 3 years. Based on
these successful results, JSL Holdings Ltd planned an IPO for Movida.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>In order to be public traded, Movida had to pass through an audition.
However, the software solution did not comply with some security
standards.&lt;/p>
&lt;p>The project started on December 2016, planning to implement an automated
software release process adopting DevOps on their data center. With the
goals:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>security&lt;/strong>; no person would need to access the Linux servers. and&lt;/li>
&lt;li>&lt;strong>productivity&lt;/strong>; releasing features faster to shorten their
time-to-market.&lt;/li>
&lt;/ol>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>Our first goal was to implement the CI/CD pipeline using Jenkins,
responsible to pack new features, create a release, and deploy it on
their data center. Apart from the production deployment, the pipeline
also supported the creation of ephemeral on-demand environments for
feature homologation and feedback retrieval from users.&lt;/p>
&lt;p>To have a faster and more controlled release cycle, we migrated the Git
server from a cloud-hosted to the data center. Through this action we
reduced in 5 minutes the deployment overall time and increased the
control over accesses in their repositories.&lt;/p>
&lt;p>The CI/CD implementation used Jenkins to control the CI/CD flow, GitLab
with LDAP authentication, and Ansible as a Configuration Manager. A
complete deployment took around 2 minutes from the git push to having
code running on production.&lt;/p>
&lt;p>Apart from the CI/CD deployment process, we also had to work in a
self-service strategy for running jobs without directly SSH access to
servers. Rundeck came into place, with RBAC configurations and
visibility over the history of executed jobs.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Movida went through audition early on January 2017, by the end of
January 2017 they received the approval.&lt;/p>
&lt;p>Two weeks later, in February 2017 Movida launched their IPO, marked as
the first Brazilian IPO of 2017. Movida went public on the 8th of
February, 2017, raising BRL 645m.&lt;/p></description></item><item><title>Nextel Digital Release</title><link>https://macunha.me/en/project/2016/nextel-digital-release/</link><pubDate>Wed, 30 Nov 2016 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/2016/nextel-digital-release/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Nextel planed to develop a mobile application to reduce their
contact rate and operational costs with call centers. &amp;ldquo;Nextel Digital&amp;rdquo;
was the name given to the project responsible for releasing this
application.&lt;/p>
&lt;p>&amp;ldquo;Nextel Digital&amp;rdquo; absorbed more goals like improving the User
experience, and turned into a new product called &amp;ldquo;Happy&amp;rdquo;, a digital
cell phone operator. Nextel Happy allows users to manage their plans and
data entirely from the mobile app, from activating your SIM to managing
your family plan.&lt;/p>
&lt;p>This project helped Nextel to increase their customers base, improved
the users&amp;rsquo; experience, and decrease operational costs (in 16%) all at
once.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Nextel Brazil executive team decided to work with outsourcing on the
development of this product to absorb the knowledge from digital
companies and to complement their internal capabilities. Also to bring
different perspectives into play, improving the creative process.&lt;/p>
&lt;p>Our team assumed the responsibility to architect and to implement the
Cloud infrastructure ensuring high-availability, resilience and
consistency of the software.&lt;/p>
&lt;p>We were also responsible for the data synchronization between Nextel
data center and the cloud. Securely moving tons of GB of users&amp;rsquo; data to
the cloud daily without data loss or duplication.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical implementation&lt;/h2>
&lt;p>We choose GlusterFS to ensure consistency, installed between Nextel Data
Center and AWS. Users&amp;rsquo; data (e.g. data plan consumption, minutes of
call) synchronization went through GlusterFS to AWS.
Nextel IT operations inserted data into GlusterFS directly from cell
phone towers in near-real-time.&lt;/p>
&lt;p>Once the data is available at AWS mounted volumes, the Celery
implementation comes into play. At the core of the architecture, Celery
(implemented in Python 3) using Redis as the message broker, running
asynchronous jobs inspects events on the GlusterFS. When Celery detects
a new file it parses the content and starts the multi-part upload to AWS
S3, then compares the checksums to ensure consistency (and retries in
case it&amp;rsquo;s inconsistent).&lt;/p>
&lt;p>After reaching AWS S3 the object event triggers a AWS Lambda function to
parse the content and index it on Elasticsearch, whose are later served
to clients through an REST API.&lt;/p>
&lt;p>The entire infrastructure setup was immutable, to facilitate the
evolution and reliability, using Ansible as a Configuration Manager and
AWS CloudFormation as the Cloud Provisioner. In just a couple minutes it
is possible to recreate everything with minimum effort.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>The entire process of making data from a cell phone tower available to
end users time went down from 1 day to 5 minutes. This reduced
in ~56% the contact rate on Nextel call centers, due to a self-service
alternative provided in the mobile app.&lt;/p>
&lt;p>In addition, users can manage their call history and
plan consumption directly on the mobile phone, with updates in
near-real-time. Providing consistent and interactive feedback.&lt;/p></description></item></channel></rss>