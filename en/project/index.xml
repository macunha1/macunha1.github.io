<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | It's me, Macunha!</title><link>https://macunha.me/en/project/</link><atom:link href="https://macunha.me/en/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Matheus Cunha</copyright><image><url>https://macunha.me/media/icon_hu176de0364afaeda8922c372b574c3cbf_6946_512x512_fill_lanczos_center_2.png</url><title>Projects</title><link>https://macunha.me/en/project/</link></image><item><title>ReclameAQUI Data Lake</title><link>https://macunha.me/en/project/reclameaqui-data-lake/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/reclameaqui-data-lake/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>ReclameAQUI (Portuguese for &amp;ldquo;complain here&amp;rdquo;) is an interesting and unique
business. They&amp;rsquo;re a content aggregator for customers' experience sharing
(especially bad experiences) about shopping (online and offline). However, it
goes further than a mere &amp;ldquo;complaints website&amp;rdquo; offering an interface for
companies to answers complaints, helping customers with their issues.&lt;/p>
&lt;p>The service is simply the biggest in this regard (worldwide) receiving 600K
unique visitors each day, searching for a company&amp;rsquo;s reputation before closing a
deal/purchase.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Even though they are already advanced in the digital approach to business,
having most services hosted on Cloud computing and analytical culture, their
data lake needed some upgrades. The most relevant motivator of this project was
the sky-high bills from GCP especially related to BigQuery data consumption.&lt;/p>
&lt;p>Apart from the cost-reduction tasks and data ingestion process optimization, we
took the opportunity to implement data cryptograph at-rest, governance, and
obfuscation during query executions against the data lake. Making data
accessible by everyone in the company, controlling identity access and
management through LDAP (auditing each access, to be fully compliant with
GDPR), we could offer a self-service data lake so different business actors
could satisfy their needs &amp;ldquo;drinking&amp;rdquo; from the lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="tech-implementation">Tech implementation&lt;/h2>
&lt;p>Key objectives were cost-optimization of the existing Data Lake, improvement
(and extension) of existing data ingestion pipelines, and security enhancements.&lt;/p>
&lt;p>Starting from Data Lake&amp;rsquo;s cost optimization, we redesigned the data ingestion,
using a &amp;ldquo;landing&amp;rdquo; area for raw data, making data transformations later to suit
the desired data models. Saving the results in other Data Lake layers to achieve
greater performance in queries.&lt;/p>
&lt;p>We shifted away from the Streaming inserts in BigQuery by adding a step to load
data at the end of the ingestion pipeline. Apache NiFi was the main software
responsible for orchestrating and executing the pipeline, covering also the
improvements in data ingestion through processes re-engineering.&lt;/p>
&lt;p>Auditing in the Data Lake was managed through Apache Ranger. In order to have
it fully supported we implemented a JDBC driver using a component from Apache
Calcite called Avatica. Authentication for Apache Ranger went through a custom
plugin (also developed during the project) for LDAP consuming user info from
Google Cloud Identity, reflecting the existing organization&amp;rsquo;s users and groups
from Google Suite.&lt;/p>
&lt;p>To make the game more interesting, we containerized the workflow and heavily
used Kubernetes (GKE) to manage these components. Most of the Apache projects
didn&amp;rsquo;t have Helm Charts at the time and we developed and made some
of them open-source.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>During project time we could measure an estimative of roughly 56% in Data Lake
cost-optimization through reengineering of processes and resources, especially
the removal of streaming inserts to BigQuery.&lt;/p>
&lt;p>We made relevant progress in security and governance during the project with the
introduction of Apache Ranger and Data Lake auditing for access and usage,
providing advanced security capabilities to ReclameAQUI, which anticipated itself
towards GDPR and data privacy concerns.&lt;/p></description></item><item><title>Clipping Service News OCR</title><link>https://macunha.me/en/project/clipping-service-news-ocr/</link><pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/clipping-service-news-ocr/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>As the owner of the biggest Brazilian media data set, the media monitoring
leader Clipping Service was having issues with scalability, getting close to
their data center maximum capacity.&lt;/p>
&lt;p>Clipping Service operates on a huge scale, receiving around ~4.5K media press
pages per day from roughly 300 newspapers in both digital and printed versions.
Previously employees called &amp;ldquo;readers&amp;rdquo; were responsible for reading and clipping
(adding highlight into the targeted content) to be later passed onto the
&amp;ldquo;reviewers&amp;rdquo; team.&lt;/p>
&lt;p>As if the burden of reading countless pages a day were not enough, the readers'
operation begins around 4:30 a.m. when the &amp;ldquo;first reading&amp;rdquo; begins (i.e., the
delivery of the morning papers).&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>For over 20 years this content has been ingested by so-called &amp;ldquo;readers&amp;rdquo;. But due
to the advent of the internet and the digital press boom at the end of the 90&amp;rsquo;s,
and nowadays of social media, companies are transferring their clipping
investments to monitoring other areas. Therefore requiring a Clipping Service
action to remain competitive in the market.&lt;/p>
&lt;p>Through news reading automation using OCR, NLP, and artificial intelligence to
categorize media, the plan was to achieve a higher throughput during ingestion,
giving readers more free time to review the content. Consequently achieving a
higher quality in the content, since we as humans aren&amp;rsquo;t good at doing
repetitive tasks, especially when it comes to reading endless pages searching
for names and words.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="tech-implementation">Tech implementation&lt;/h2>
&lt;p>After spending some time researching and benchmarking the alternatives at hand
we decided to use Python as the implementation language for handling texts,
OCR, and NLP (using NLTK). Given its extended API and libraries for NLP and
image processing.&lt;/p>
&lt;p>As the cloud provider we choose AWS due to it&amp;rsquo;s stability and consistency over
other vendors, the conclusion at the time was: AWS price estimative 14.67%
greater than GCP. However, AWS&amp;rsquo;s popularity is greater than GCP and proven in
terms of stability, support, and integrity. Making a safer choice for a
slightly higher price.&lt;/p>
&lt;p>The tech stack was: Python 3 using Dramatiq as the task processing library,
running Tesseract OCR jobs, processing text with NLTK and images with Pillow
(ImageMagick wrapper). Redis was the message broker for Dramatiq, a simple
Postgres database stored metrics regarding the execution and we had an
Elasticsearch storing the processed content.&lt;/p>
&lt;p>Requests coming from the data center reached an API Gateway, responsible for
executing a Lambda function, and delivering the content result.&lt;/p>
&lt;p>The best part of the design? We stored and served the content via AWS S3. Each
part was designed with fault tolerance, and we simply turned off the entire
cloud infrastructure after the operation, to turn on only the next day.&lt;/p>
&lt;p>Operating only from 4am to 2pm, a &amp;ldquo;serverless&amp;rdquo; and ephemeral project benefiting
from an aggressive cloud cost reduction.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Clipping Service reduced its reading team workforce by ~78%, offering internal
hiring for other areas of the company and a voluntary dismissal plan with
benefits, making the process as human as possible for the former employees.&lt;/p>
&lt;p>Using automation for reading tasks, Clipping Service could reach considerable
improvements in the media press ingestion throughput (around 20 times faster),
offering higher quality in press clipping service for its customers and saw the
opportunity in creating a self-service press clipping service later, since the
operational cost decreased significantly.&lt;/p></description></item><item><title>Dotz Data Labs</title><link>https://macunha.me/en/project/dotz-data-labs/</link><pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/dotz-data-labs/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>In order to be able to innovate and maintain itself in a highly changing
and evolving market, Dotz went through a process of digital
transformation and had the help of some consultants along the way.&lt;/p>
&lt;p>Among the steps to get closer to a digital model, the implementation of
a Data Lake emerged, with the requirements of being
&lt;a href="https://en.wikipedia.org/wiki/Serverless_computing" target="_blank" rel="noopener">serverless&lt;/a> and
cloud-native to support the decision-making process and shorten
time-to-market during the launch of products.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Dotz is one of the largest companies in the field of loyalty program in
Brazil and they&amp;rsquo;d face a high number of issues with data disconnection
making it difficult to analyze their users' behaviors. Since they
received data from numerous supermarkets and stores, it&amp;rsquo;s difficult to
clusterizate products, since the name is different depending on the
source. To help with this analysis, they decided to build a Data Lake.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>We built and deployed a Big Data's managed architecture using Google's
Cloud Platform (GCP) to support this strategy and allow a 360-degree
view of customers (users with points a.k.a. Dotz) and partners (the
supermarkets offering the loyalty program).&lt;/p>
&lt;p>The design was focused on cloud-managed services and serverless
computing offered by Google, serving the core competencies of a Data
Lake such as scalable storage using Google Cloud Storage, and Google
BigQuery. With part of the process running inside Kubernetes,
responsible for data cleansing ETL flow management.&lt;/p>
&lt;p>We streamed data with Apache Beam running under Google DataFlow,
parallel mass processing with Apache Spark jobs running on Google
DataProc, exploratory analysis with Google DataLab, Machine Learning
Analysis with Google ML and Data visualization in Google Data Studio.&lt;/p>
&lt;p>Data is transported using an event-driven model, where every data is
collected using a streaming model, even the ETL (which runs on a
micro-batch, to enable near real-time exploration). These data goes
through the data pipeline using Google Pub/Sub message-oriented
middleware, and every message is serialized using Avro format, which
reduces the payload and allows transportation to be cost-effective, fast
and reliable.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>It all allowed Dotz to have a better structure on their analytical
platform, previously managed inside a large MS SQL Server instance,
which moved to a Data Lake with layers allowing data categorization,
governance, quality and security.&lt;/p>
&lt;p>Supporting analytical processes of users data, faster exploration and
monetization of their knowledge on customers' behavior.&lt;/p></description></item><item><title>Easynvest Data Platform</title><link>https://macunha.me/en/project/easynvest-data-platform/</link><pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/easynvest-data-platform/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Within Easynvest&amp;rsquo;s annual planning, an investment in the expansion of
data &amp;amp; analytics team aimed at shortening decision making process and
in delivering a higher quality to customers through a low cost operational
process.&lt;/p>
&lt;p>Among the main objectives of this project, we had the automation of
credit analysis (executed during the approval of customer registration,
using Machine Learning), a process that until then was long and manual,
being handled by the back office.&lt;/p>
&lt;p>Followed by a better offer of products to the client, carrying out the
categorization according to the profile of each customer, allowing
suggestions of more attractive products, in line with personal preferences,
as well as according to the profile of each investor (conservative,
moderate or aggressive).&lt;/p>
&lt;p>Last but not least, the intelligent detection of money laundry and
reporting to the responsible authorities.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>However, there were limitations in the data tools, mainly due to the
fact that they were proprietary software (with limited licenses) and
designed for usage inside data centers. In addition, the analytical
database was modeled for traditional Business Intelligence models
(OLAP, etc), making the decision making process heavy, due to the
demanding amount of interactions during ETL.&lt;/p>
&lt;p>Previously for a client to be approved, the process took 10 to 15 days.
Gathering all necessary information, providing a complete perspective of
the profile, including credit analysis. After collecting the information,
the back office generated an internal credit analysis score.&lt;/p>
&lt;p>In most cases, the client was not notified of updates regarding the
process and did not receive feedback at the end (if refused) unless
explicitly requested (by contacting support via chat or email, for
example) which made the process time consuming and costly. Not to
mention the countless amounts of customers lost to the competition during
this long wait.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>To make it possible, we built an hybrid-cloud implementation using AWS
cloud-based components (mainly AWS S3, EMR and ECS), to extend the data
centers' capability, implementing a cloud-first Hadoop ecosystem
(replacing the proprietary software components with open-source
equivalents). Giving Easynvest the possibility to grow the Data Lake
exponentially.&lt;/p>
&lt;p>The Data Lake design was robust, aimed to handle the execution of heavy
analytical processes through Machine Learning models, with support for
data quality, metadata governance, information security and data serving
(data owners could share data with consumers from other areas within the
company, allowing to self-service their analytical data).&lt;/p>
&lt;p>A Chatbot was also used to reduce the operational load in the
environment, this bot is responsible for maintaining and updating
infrastructure components. From triggering deployments to generating
encryption keys on-demand for data security and governance. Implemented
with the Errbot framework for Python interacting with Slack.&lt;/p>
&lt;p>Going further, we implemented the best practices in DevOps, using Jenkins
as a tool for CI/CD of the developed components alongside with Ansible for
Configuration Management.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Thanks to the utilization of layers in the Data Lake and the
implementation of data pipelines, we were able to reduce the data
ingestion time by 78% and to include metadata and data catalog, in
addition to automating much of the work that used to be done manually.&lt;/p>
&lt;p>Thus bringing positive results, especially reducing the registration
approval time for consumers from roughly 10 days to 1 day. It also made
the data platform more democratic, providing relevant information that
facilitates the analysis of areas such as risk (credit analysis) and
support, without having to give up security.&lt;/p></description></item><item><title>Movida Rent A DevOps</title><link>https://macunha.me/en/project/movida-rent-a-devops/</link><pubDate>Tue, 07 Feb 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/movida-rent-a-devops/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>JSL Holdings Ltd, holder of Julio SimÃµes Logistica (biggest logistics
players in LATAM) bought Movida Rent a Car in 2013 to expand the
portfolio and open new market opportunities on car rental and selling
markets.&lt;/p>
&lt;p>JSL invested around R$1.8 billion in Movida, and multiplied its annual
revenues by 21 times from BRL 58m to BRL 1.2b, in 3 years. Based on
these successful results, JSL Holdings Ltd planned an IPO for Movida.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>In order to be public traded, Movida had to pass through an audition.
However, the software solution did not comply with some security
standards.&lt;/p>
&lt;p>The project started on December 2016, planning to implement an automated
software release process adopting DevOps on their data center. With the
goals:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>security&lt;/strong>; no person would need to access the Linux servers. and&lt;/li>
&lt;li>&lt;strong>productivity&lt;/strong>; releasing features faster to shorten their
time-to-market.&lt;/li>
&lt;/ol>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical Implementation&lt;/h2>
&lt;p>Our first goal was to implement the CI/CD pipeline using Jenkins,
responsible to pack new features, create a release, and deploy it on
their data center. Apart from the production deployment, the pipeline
also supported the creation of ephemeral on-demand environments for
feature homologation and feedback retrieval from users.&lt;/p>
&lt;p>To have a faster and more controlled release cycle, we migrated the Git
server from a cloud-hosted to the data center. Through this action we
reduced in 5 minutes the deployment overall time and increased the
control over accesses in their repositories.&lt;/p>
&lt;p>The CI/CD implementation used Jenkins to control the CI/CD flow, GitLab
with LDAP authentication, and Ansible as a Configuration Manager. A
complete deployment took around 2 minutes from the git push to having
code running on production.&lt;/p>
&lt;p>Apart from the CI/CD deployment process, we also had to work in a
self-service strategy for running jobs without directly SSH access to
servers. Rundeck came into place, with RBAC configurations and
visibility over the history of executed jobs.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>Movida went through audition early on January 2017, by the end of
January 2017 they received the approval.&lt;/p>
&lt;p>Two weeks later, in February 2017 Movida launched their IPO, marked as
the first Brazilian IPO of 2017. Movida went public on the 8th of
February, 2017, raising BRL 645m.&lt;/p></description></item><item><title>Nextel Digital Release</title><link>https://macunha.me/en/project/nextel-digital-release/</link><pubDate>Wed, 30 Nov 2016 00:00:00 +0000</pubDate><guid>https://macunha.me/en/project/nextel-digital-release/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Nextel planed to develop a mobile application to reduce their
contact rate and operational costs with call centers. &amp;ldquo;Nextel Digital&amp;rdquo;
was the name given to the project responsible for releasing this
application.&lt;/p>
&lt;p>&amp;ldquo;Nextel Digital&amp;rdquo; absorbed more goals like improving the User
experience, and turned into a new product called &amp;ldquo;Happy&amp;rdquo;, a digital
cell phone operator. Nextel Happy allows users to manage their plans and
data entirely from the mobile app, from activating your SIM to managing
your family plan.&lt;/p>
&lt;p>This project helped Nextel to increase their customers base, improved
the users' experience, and decrease operational costs (in 16%) all at
once.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Nextel Brazil executive team decided to work with outsourcing on the
development of this product to absorb the knowledge from digital
companies and to complement their internal capabilities. Also to bring
different perspectives into play, improving the creative process.&lt;/p>
&lt;p>Our team assumed the responsibility to architect and to implement the
Cloud infrastructure ensuring high-availability, resilience and
consistency of the software.&lt;/p>
&lt;p>We were also responsible for the data synchronization between Nextel
data center and the cloud. Securely moving tons of GB of users' data to
the cloud daily without data loss or duplication.&lt;/p>
&lt;h1 id="solution">Solution&lt;/h1>
&lt;h2 id="technical-implementation">Technical implementation&lt;/h2>
&lt;p>We choose GlusterFS to ensure consistency, installed between Nextel Data
Center and AWS. Users' data (e.g. data plan consumption, minutes of
call) synchronization went through GlusterFS to AWS.
Nextel IT operations inserted data into GlusterFS directly from cell
phone towers in near-real-time.&lt;/p>
&lt;p>Once the data is available at AWS mounted volumes, the Celery
implementation comes into play. At the core of the architecture, Celery
(implemented in Python 3) using Redis as the message broker, running
asynchronous jobs inspects events on the GlusterFS. When Celery detects
a new file it parses the content and starts the multi-part upload to AWS
S3, then compares the checksums to ensure consistency (and retries in
case it&amp;rsquo;s inconsistent).&lt;/p>
&lt;p>After reaching AWS S3 the object event triggers a AWS Lambda function to
parse the content and index it on Elasticsearch, whose are later served
to clients through an REST API.&lt;/p>
&lt;p>The entire infrastructure setup was immutable, to facilitate the
evolution and reliability, using Ansible as a Configuration Manager and
AWS CloudFormation as the Cloud Provisioner. In just a couple minutes it
is possible to recreate everything with minimum effort.&lt;/p>
&lt;h2 id="impact-and-results">Impact and results&lt;/h2>
&lt;p>The entire process of making data from a cell phone tower available to
end users time went down from 1 day to 5 minutes. This reduced
in ~56% the contact rate on Nextel call centers, due to a self-service
alternative provided in the mobile app.&lt;/p>
&lt;p>In addition, users can manage their call history and
plan consumption directly on the mobile phone, with updates in
near-real-time. Providing consistent and interactive feedback.&lt;/p></description></item></channel></rss>