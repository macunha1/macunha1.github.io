[{"authors":["admin"],"categories":null,"content":"Hello!\nIâ€™m Matheus Cunha (or just Macunha), a Systems Engineer focused on DevOps and Data Engineering.\nExperienced architecting decentralized and highly available systems, launched projects in all major clouds: AWS, Azure, and GCP. Including multi-cloud and hybrid-cloud approaches with on-premises.\nIâ€™ve worked as a consultant for various businesses, helping them to be more future-proof and competitive in a highly changing market. Always looking for ways to improve, learn more, and move further. Achieving a higher potential for the businesses and projects I got involved in.\nRegarding the techy nerd side, Iâ€™m an open-source technology lover high-skilled in Linux, that writes code in Java, Python, Go, JavaScript (Node.js), and Lua.\nMostly, using the following databases:\n PostgreSQL and MySQL (for relational data); InfluxDB (including TICK stack) for timeseries; Elasticsearch (including ELK and EBK) for logging and searching; Hadoop (HDFS, S3 or Google Cloud Storage) for data warehousing; Kafka as a high-throughput message broker; and Redis for caching and as a medium-throughput message broker.  With Prometheus for infrastructure monitoring being backed-up by InfluxData TICK stack. Ansible to have configuration management as code followed by Terraform (preferably implementing immutable infrastructure), and containers running in production orchestrated by Kubernetes.\nðŸ˜„\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1590897201,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://macunha.me/en/author/matheus-cunha/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/matheus-cunha/","section":"authors","summary":"Hello!\nIâ€™m Matheus Cunha (or just Macunha), a Systems Engineer focused on DevOps and Data Engineering.\nExperienced architecting decentralized and highly available systems, launched projects in all major clouds: AWS, Azure, and GCP.","tags":null,"title":"Matheus Cunha","type":"authors"},{"authors":null,"categories":null,"content":"Introduction The Apache Spark Operator for Kubernetes Since its launch in 2014 by Google, Kubernetes has gained a lot of popularity along with Docker itself and since 2016 has become the de facto Container Orchestrator, established as a market standard. Having cloud-managed versions available in all the major Clouds. [1] [2] [3] (including Digital Ocean and Alibaba).\nWith this popularity came various implementations and use-cases of the orchestrator, among them the execution of Stateful applications including databases using containers.\nWhat would be the motivation to host an orchestrated database? That\u0026rsquo;s a great question. But let\u0026rsquo;s focus on the Spark Operator running workloads on Kubernetes.\nA native Spark Operator idea came out in 2016, before that you couldn\u0026rsquo;t run Spark jobs natively except some hacky alternatives, like running Apache Zeppelin inside Kubernetes or creating your Apache Spark cluster inside Kubernetes (from the official Kubernetes organization on GitHub) referencing the Spark workers in Stand-alone mode.\nHowever, the native execution would be far more interesting for taking advantage of Kubernetes Scheduler responsible for taking action of allocating resources, giving elasticity and an simpler interface to manage Apache Spark workloads.\nConsidering that, Apache Spark Operator development got attention, merged and released into Spark version 2.3.0 launched in February, 2018.\nIf you\u0026rsquo;re eager for reading more regarding the Apache Spark proposal, you can head to the design document published in Google Docs.\nWhy Kubernetes? As companies are currently seeking to reinvent themselves through the widely spoken digital transformation in order for them to be competitive and, above all, to survive in an increasingly dynamic market, it is common to see approaches that include Big Data, Artificial Intelligence and Cloud Computing [1] [2] [3].\nAmong the benefits of using Cloud instead of On-premises we can list:\nAn interesting comparison can be read at Databricks blog which is the company founded by the creators of Apache Spark.\nAs we see a widespread adoption of Cloud Computing (even by companies that would be able to afford the hardware and run on-premises), we notice that most of these Cloud implementations don\u0026rsquo;t have an Apache Hadoop since the Data Teams (BI/Data Science/Analytics) increasingly choose to use tools like Google BigQuery or AWS Redshift, it doesn\u0026rsquo;t make sense to climb a Hadoop only to use YARN to manage the resources.\nAn alternative is the use of Hadoop cluster providers such as Google DataProc or AWS EMR for the creation of ephemeral clusters. Just to name a few options.\nTo better understand the design of Spark Operator, the doc from GCP on GitHub as a no-brainer.\nLet\u0026rsquo;s get our hands-on! Warming up the engine Now that the word has been spread, let\u0026rsquo;s get our hands on it to show the engine running. For that, let\u0026rsquo;s use:\n  Docker as the container engine for Kubernetes (installation guide); Minikube (installation guide) to facilitate the provisioning of the Kubernetes (yes, it will be a local execution); For interaction with the Kubernetes API it is necessary to have kubectl installed, if you don\u0026rsquo;t have it, follow instructions here. a compiled version of Apache Spark larger than 2.3.0.  you can either compile source code, which will took some hours to finish, or download a compiled version here (recommended).    Once the necessary tools are installed, it\u0026rsquo;s necessary to include Apache Spark path in PATH environment variable, to ease the invokation of Apache Spark executables. Simply run:\nexport PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin  Creating the Minikube \u0026ldquo;cluster\u0026rdquo; At last, to have a Kubernetes \u0026ldquo;cluster\u0026rdquo; we will start a minikube with the intention of running a example from Spark repository called SparkPi just as a demonstration.\nminikube start --cpus=2 \\ --memory=4g  Building the Docker image Let\u0026rsquo;s use the Minikube Docker daemon to not depend on an external registry (and only generate Docker image layers on the VM, facilitating a garbage disposal later). Minikube has a wrapper that makes our life easier:\neval $(minikube docker-env)  After having the daemon environment variables configured, we need a Docker image to run the jobs. There is a shell script in the Spark repository to help with this. Considering that our PATH was properly configured, just run:\ndocker-image-tool.sh -m -t latest build  FYI: The -m parameter here indicates a minikube build.\nLet\u0026rsquo;s take the highway to execute SparkPi, using the same command that would be used for a Hadoop Spark cluster spark-submit.\nHowever, Spark Operator supports defining jobs in the \u0026ldquo;Kubernetes language\u0026rdquo; using CRD, here are some examples - for later.\nFire in the hole! Mid the gap between the Scala version and .jar when you\u0026rsquo;re parameterizing with your Apache Spark version:\nspark-submit --master k8s://https://$(minikube ip):8443 \\ --deploy-mode cluster \\ --name spark-pi \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.executor.instances=2 \\ --executor-memory 1024m \\ --conf spark.kubernetes.container.image=spark:latest \\ local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # here  What\u0026rsquo;s new is:\n --master: Accepts a prefix k8s:// in the URL, for the Kubernetes master API endpoint, exposed by the command https://$(minikube ip):8443. BTW, in case you want to know, it\u0026rsquo;s a shell command substitution; --conf spark.kubernetes.container.image=: Configures the Docker image to run in Kubernetes.  Sample output:\n... 19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed, new state: pod name: spark-pi-1566485909677-driver namespace: default labels: spark-app-selector -\u0026gt; spark-20477e803e7648a59e9bcd37394f7f60, spark-role -\u0026gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d creation time: 2019-08-22T14:58:30Z service account name: default volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn node name: minikube start time: 2019-08-22T14:58:30Z container images: spark:docker phase: Succeeded status: [ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d 6406258ef247648a5902bf6ac09801cc, image=spark:docker, imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3 a87de32c21787ff82f, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties={}), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0, finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed, signal=null, startedAt=2019-08-22T14:58:32Z, additionalProperties={}), waiting=null, additionalProperties={}), additionalProperties={})] 19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final statuses: Container name: spark-kubernetes-driver Container image: spark:docker Container state: Terminated Exit code: 0  To see the job result (and the whole execution) we can run a kubectl logs passing the name of the driver pod as a parameter:\nkubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')  Which brings the output (omitted some entries), similar to:\n... 19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2) 19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool19/08/22 14:59:08 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 0.957 s 19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473 19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at http://spark-pi-1566485909677-driver-svc.default.svc:4040 19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting down all executors 19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down 19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.) 19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped! 19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared 19/08/22 14:59:08 INFO BlockManager: BlockManager stopped 19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped 19/08/22 14:59:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped! 19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext 19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called 19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2 19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory /var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90 d0-4a0d-9cab-f36a7bb18910 ...  The result appears in:\n19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473  Finally, let\u0026rsquo;s delete the VM that Minikube generates, to clean up the environment (unless you want to keep playing with it):\nminikube delete  Last words I hope your curiosity got sparked and some ideas for further development have raised for your Big Data workloads. If you have any doubt or suggestion, don\u0026rsquo;t hesitate to share on the comment section.\n","date":1590094857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590140840,"objectID":"359ac010e6e64574925f2121741a0cf4","permalink":"https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/","publishdate":"2020-05-21T23:00:57+02:00","relpermalink":"/en/post/2020/05/quickstart-apache-spark-on-kubernetes/","section":"post","summary":"Introduction The Apache Spark Operator for Kubernetes Since its launch in 2014 by Google, Kubernetes has gained a lot of popularity along with Docker itself and since 2016 has become the de facto Container Orchestrator, established as a market standard.","tags":["data-eng","kubernetes"],"title":"Quickstart: Apache Spark on Kubernetes","type":"post"},{"authors":null,"categories":["DevOps","Agile Manifesto"],"content":"Introduction Main benefits that a company generally expects and finds in the adoption of culture:\nFaster and Cheaper Releases Since releases will be continuous and frequent, deliverables will turn into small changes with the benefit of increasing speed in the development cycle (delivering always).\nImproved Operational support with quick fixed If there is a failure during delivery, the impact is minimal because the amount of modifications is small, just as the rollback is faster. Having a simple inspection and debugging.\nBetter Time-to-market (TTM) The software will be delivered much earlier when it\u0026rsquo;s still an MVP. Customers will be integrated as part of the development process, bringing insights and feedback to the development team. Thus allowing for a higher launch speed in the market.\nSuperior quality products As has been said before, early failures prevent defects from being delivered to production, because:\n Reduces the volume of defects in the product as a whole; Increases frequency of new features and releases; Appropriate development processes in teams, including automation.  Now we understood WHY, let\u0026rsquo;s talk about HOW Continuous releases (integration, delivery, deployment) Usually follows a code versioning approach (through Git) using specific branches for each environment (e.g.: feature branches with git flow).\nContinuous integration Automatic execution of unit tests, integration tests and code quality analysis against a git branch, to ensure that there was no disruption of the modified piece of code.\nContinuous delivery Packaging the software that is tested and approved, to deliver it somewhere that it is possible to use in a deploy later. Examples are libs delivered in repositories to be integrated into the code during the next update and code deploy.\nContinuous deployment Once you have completed all of the above steps, you can do automated deployments right in the environments, when the team is more confident about the tools they are testing, as well as the risk they\u0026rsquo;re taking and also understanding that there is a possibility of failure in a tests environment without worrying that it\u0026rsquo;s going to be divergent from production.\nConfiguration (and/or Infrastructure) as code To be able to test software with assertiveness, and to understand that it will transit between environments without changing behavior, it is essential that the configurations are also expressed in code. This allows the settings to be also versioned, following the code. Also guaranteeing a uniformity among the environments, which enables:\n Reduction in maintenance costs, having a single point to look at and understand the operation of the system; Easy to recreate the infrastructure, if it is necessary to move everything to another place, this can happen with a few manual interactions; Allows for a code review of infrastructure and configurations, which consequently brings a culture of collaboration in the development, sharing of knowledge and increases the democratization of the infra; Documentation as code, helping new team members get a faster warm up.  These points were well-stressed by the Heroku team and gave rise to the famous paper: [The Twelve-Factor App] (https://12factor.net/). It\u0026rsquo;s an excellent reading for the explanation of the benefits of configuration management.\nObservability, Monitoring, and self-healing At the end of the delivery process, the software must be monitored. Avoiding to wait for an external report of failures, ensuring that the actions are proactive rather than reactive.\nWith mature monitoring, it\u0026rsquo;s possible to create trigger against alerts, creating a self-healing system in which actions (scripts) are performed to fix known failures in the infrastructure so that everyone can sleep peacefully at night, without having to worry about the on-call schedule that makes you read some documentation at dawn. (If you have had experience with this, you know for sure how bad it is).\nScaling up only those cases that are extreme exceptions (mistakes not known/expected) in the process for the employee to act, ensuring higher health in operation.\nProcesses automation All processes that cause Muda should be addressed with automation, allowing people to work more quickly. Good examples of processes that are usually automated are:\n Deployment; Self-healing (system resilience in response to anomalies); Renewal of Certificates; Execution of tests (unitary, integration, functional, etc.); Monitoring (with auto-discovery); User Governance;  DevOps toolchain A combination of tools to facilitate the maintenance and operation of the system, with the flow:\n Note: Any similarity to the PDCA is pure certainty.\n  Plan: Project planning phase, in which feedbacks are collected for requirements survey, and backlog creation; Create: Creation of a deliverable (to validate a hypothesis), such as an MVP; Verify: Pass the deliverable to the test phase; Package: Package the build to be able to put it in some testing environment; Release: Deploy packaged deliverable; Configure: Perform the configuration of the deliverable in the testing environment, trying to get as close as possible to the twelve-factor app. Monitor: After deploying to the environment, track business metrics and infrastructure to ensure everything is working as expected.  Conclusion During the implementation of these techniques it is possible to observe improvements in the development process, the most notable gains are:\n Increase in team engagement; Knowledge sharing; Reduction of bottlenecks; More free time to do work that really matters (adds value to the user experience or generates impact); Greater confidence in delivering software.  ","date":1547244000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589823897,"objectID":"234c851ba757bb2bbbda260c3e11abe3","permalink":"https://macunha.me/en/post/2019/01/devops-benefits/","publishdate":"2019-01-11T22:00:00Z","relpermalink":"/en/post/2019/01/devops-benefits/","section":"post","summary":"Introduction Main benefits that a company generally expects and finds in the adoption of culture:\nFaster and Cheaper Releases Since releases will be continuous and frequent, deliverables will turn into small changes with the benefit of increasing speed in the development cycle (delivering always).","tags":["devops","culture","agile","lean"],"title":"DevOps: Benefits","type":"post"},{"authors":null,"categories":["DevOps","Agile Manifesto"],"content":"Introduction First of all, it\u0026rsquo;s all about agile.\nThe DevOps methodology was created on top of agile methods, to deliver a higher value inside software releases, automating feature release through pipelines, that can test hypothesis faster allowing higher adaptability using \u0026ldquo;fail-fast\u0026rdquo; approaches. Those changes are more cultural than technical, so it\u0026rsquo;s normal to see DevOps being called culture.\nThe implementation of DevOps happens through processes automation, having a strong sense of processes re-engineering inside the company. Comparing to the cultural change, the technical is easy to implement. Therefore the role that a \u0026ldquo;DevOps Engineer/Analyst\u0026rdquo; performs is very confusing, which enables many SysAdmins and Infra Analysts assuming the role of \u0026ldquo;DevOps.\u0026rdquo;\nLean is the basis of Agile Reality is not as happy as it sounds. After World War II, Japan was destroyed and under-resourced after losing the war. With a limited amount of resources, the country needed to reinvent itself and survive after a time of severe depression. During that time two guys gained attention inside a company that later gave its name after the methodology.\nThose guys were Eiji Toyoda and Taiichi Ohno, inside Toyota Motor Corporation. They\u0026rsquo;re the founders of the \u0026ldquo;Toyota production model\u0026rdquo; also known as Toyotism.\nToyota gave birth to Lean Lean teaches how to optimize the end-to-end process, focusing on processes that create value for customers. Bottlenecks in the process must be removed, and wasteful activities need to be identified and avoided. Both explained and defined by LEAN 3M: Muda, Mura, and Muri.\nAlso teaches to improve yourself day after day and always focus on quality through Kaizen (continuous improvement).\nJapanese culture truly believes that quality is the main objective to deliver value to customers since quality is what brings your clients back.\nKaizen A mindset that helps to look at each part of the process exclusively and think about the improvements. Involving the people who are part of the process, encourage the inclusion of these people in the decisions of change, since:\n It is much easier to accept a change when it is not imposed (top-down); There is a greater absorption of change by people when they\u0026rsquo;re included in the planning; The people who are involved in the process bring their concerns and suggestions, which contribute positively to the evolution of the change, making the idea more robust.  The process of defining improvements through Kaizen happens (usually) in the following order:\n Define data-driven objectives; Review the current state and develop an improvement plan; Implement improvement; Review the implementation and improve what does not work; Report the results and determine the items to be monitored.  This process is also called PDCA: Plain-Do-Control-Act, which is summarized in:\n Plan (develop the hypothesis); Do (experiment); Check (validate results); Act (refine the experiment and start over).  3M: Muda, Mura, Muri Muda (waste) Any activity that consumes time without adding value to the final consumer. e.g.:\n over-production; idle time in the process; products with a defect.  It\u0026rsquo;s important to remember that there are different levels of Muda that can be removed quickly or not, and the classification depends on the time for removal.\nAn example of a more time-consuming Muda is the discontinuation of legacy software that ends up with longer release cycles, causing teams to be idle, followed by an often long or manual test routine.\nMura (unevenness) Unevenness in operation, caused by activities that are very changeable and unpredictable, generating different results in all executions. e.g., the execution of tasks that were not well planned and ended up arriving with strict deadlines. The team runs in the rush, generating exhaustion, despair, and moreover, when finished leaves the people who have performed these tasks waiting (for feedback, or confirmation that it is completed).\nMuri (overload) Overburdening equipment or operators by requiring them to run at a higher or harder pace beyond the limit, to achieve some goal or expectation, causing fatigue and consequently failures during the process. These failures are usually human errors caused by fatigue during overwork.\nBack to Agile In 2000 a group of 17 people met at a resort in Oregon to talk about ideas that could improve the flow of software development. After a year of mature ideas, these people met again and published the ideas, which we now know as Agile Manifesto.\nMain points are:\nIndividuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan\nI will restrict the explanation of these points with the DevOps point of view, keeping on track (now).\nIndividuals and interactions over processes and tools\nFirst comes the individuals, they should receive the necessary tooling to work with, and then be empowered to do their jobs. Interactions between people are greatly encouraged, for sharing knowledge and also for facilitating creative flow within development teams.\nAn excellent example of interaction encouraged through DevOps is the code review habit. Considering that small parts of the software will be iterated and approved in the pipeline passing through different environments, automatically, the best way to prevent defects is through code review.\nThis habit brings benefits such as:\n Knowledge sharing; Observation of the problem from a different point of view; Team engagement; Lesser bugs.  Working software over comprehensive documentation\nHere\u0026rsquo;s a trick in \u0026ldquo;working software,\u0026rdquo; software that works is not code that compiles. The software that works is what meets the requirements of the user; i.e., the software that solves the problem and the pains of the user.\nAs the market is very dynamic, and evolves with high speed, often during the software development project the requirements change due to external factors. Therefore, knowing that it is not possible to predict all the elements, many \u0026ldquo;workarounds\u0026rdquo; are made during development and documented. Passing the responsibility to the user to handle the faults, and perform the workarounds, expending more effort than would be required to perform the tasks using the software.\n Deliver a working software frequently, ranging from a few weeks to a few months, considering shorter time-scale. - Agile Manifesto\n Encouraging as many deployments as possible, so that failures happen as early as possible, thus allowing their impact to be much less.\nFail-fast! Failures are understood and encouraged because it\u0026rsquo;s part of the mindset. Because:\n Only those who do make mistakes; Shit happens.  Therefore, it\u0026rsquo;s best for failures to occur early, while the cost of correction is still low. Failing a controlled testing environment allows the fix to be much faster (and cheaper) than it would if the fix were already in production.\nFor this approach to succeed, there is a premise that environments are production copies, or at least as close as possible. Otherwise, there will be behavioral changes in the software between the environments, making the test environment unfeasible.\nIf the environments are divergent, the promotion of bugs for production will be very frequent, causing late failures, which are expensive failures.\nCustomer collaboration over contract negotiation\nKnow your client! Including it in the process is the best approach to have working software. After iterating over deliverables, it\u0026rsquo;s essential to create a positive feedback loop with your client, bringing it as close as possible to the development of the tools that he/she is going to use.\nWe can describe this situation with:\n From point A it is possible to see only point B; From point B it is possible to see point C;  Therefore there is a great incentive for the software to be delivered in parts, continuously. Thus gathering user feedback on the next steps, following the concepts of evolutionary prototyping, which were widely publicized through The Lean Startup.\nThis point contrasts sharply with the previous one about continuous release, so that it is possible to present the prototype and evolve it throughout the project.\nLearn who your customer/consumer/user is, and whom you are making the software for, as this is the only way you can deliver value to that customer. An essential part of the software development process is to be empathic with user problems, and to truly understand what the problem is to be solved, and the result of the impact on software development (value creation for the user).\nResponding to change over following a plan\nRedesigning the requirements overtime is part of the job, and a necessary step to success. If you want to build something useful that is going to grow and have absorption, it\u0026rsquo;s a key feature to include your client in the implementation process.\nIt will be the only way to bring all the problems of the user to the table and create the best solution for all these problems because the user is the only person that knows the real challenges he faces in their routine dealing with software.\nWith continuous delivery of software along with monitoring results, the process of collecting feedback is much simpler and faster.\nDevOps, DevOps, DevOps With the popularization of DevOps, a lot of disagreement came out there followed by a significant confusion about the subject. It is very common to come across different interpretations of what is DevOps. There is a lot of euphemism in the area, and gourmetization on LinkedIn, with many SysAdmins calling themselves DevOps since they learned to code shell script inside Python.\nDo you want to keep reading? Here are the benefits of adopting DevOps techniques.\n","date":1547240400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589734215,"objectID":"1ea695d486767aac5f192dfd0ddf47d4","permalink":"https://macunha.me/en/post/2019/01/devops-genesis/","publishdate":"2019-01-11T21:00:00Z","relpermalink":"/en/post/2019/01/devops-genesis/","section":"post","summary":"Introduction First of all, it\u0026rsquo;s all about agile.\nThe DevOps methodology was created on top of agile methods, to deliver a higher value inside software releases, automating feature release through pipelines, that can test hypothesis faster allowing higher adaptability using \u0026ldquo;fail-fast\u0026rdquo; approaches.","tags":["devops","culture","agile","lean"],"title":"DevOps: The Genesis","type":"post"},{"authors":null,"categories":null,"content":"Introduction Summary In order to be able to innovate and maintain itself in a highly changing and evolving market, Dotz went through a process of digital transformation and had the help of some consultants along the way.\nAmong the steps to get closer to a digital model, the implementation of a Data Lake emerged, with the requirements of being serverless and cloud-native to support the decision-making process and shorten time-to-market during the launch of products.\nProblem Dotz is one of the largest companies in the field of loyalty program in Brazil and they\u0026rsquo;d face a high number of issues with data disconnection making it difficult to analyze their users' behaviors. Since they received data from numerous supermarkets and stores, it\u0026rsquo;s difficult to clusterizate products, since the name is different depending on the source. To help with this analysis, they decided to build a Data Lake.\nSolution Technical Implementation We built and deployed a Big Data's managed architecture using Google's Cloud Platform (GCP) to support this strategy and allow a 360-degree view of customers (users with points a.k.a. Dotz) and partners (the supermarkets offering the loyalty program).\nThe design was focused on cloud-managed services and serverless computing offered by Google, serving the core competencies of a Data Lake such as scalable storage using Google Cloud Storage, and Google BigQuery. With part of the process running inside Kubernetes, responsible for data cleansing ETL flow management.\nWe streamed data with Apache Beam running under Google DataFlow, parallel mass processing with Apache Spark jobs running on Google DataProc, exploratory analysis with Google DataLab, Machine Learning Analysis with Google ML and Data visualization in Google Data Studio.\nData is transported using an event-driven model, where every data is collected using a streaming model, even the ETL (which runs on a micro-batch, to enable near real-time exploration). These data goes through the data pipeline using Google Pub/Sub message-oriented middleware, and every message is serialized using Avro format, which reduces the payload and allows transportation to be cost-effective, fast and reliable.\nImpact and results It all allowed Dotz to have a better structure on their analytical platform, previously managed inside a large MS SQL Server instance, which moved to a Data Lake with layers allowing data categorization, governance, quality and security.\nSupporting analytical processes of users data, faster exploration and monetization of their knowledge on customers' behavior.\n","date":1518825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589871935,"objectID":"425ab80046da88c0b83001b57e4e7a84","permalink":"https://macunha.me/en/project/2017/dotz-data-labs/","publishdate":"2018-02-17T00:00:00Z","relpermalink":"/en/project/2017/dotz-data-labs/","section":"project","summary":"Serverless and cloud-managed Big Data architecture using Google's Cloud Platform (GCP) to support a 360-degree view of customers and partners of Dotz, one of the largest companies in the field of loyalty program in Brazil","tags":["data-eng"],"title":"Dotz Data Labs","type":"project"},{"authors":null,"categories":null,"content":"Introduction Summary Within Easynvest\u0026rsquo;s annual planning, an investment in the expansion of data \u0026amp; analytics team aimed at shortening decision making process and in delivering a higher quality to customers through a low cost operational process.\nAmong the main objectives of this project, we had the automation of credit analysis (executed during the approval of customer registration, using Machine Learning), a process that until then was long and manual, being handled by the back office.\nFollowed by a better offer of products to the client, carrying out the categorization according to the profile of each customer, allowing suggestions of more attractive products, in line with personal preferences, as well as according to the profile of each investor (conservative, moderate or aggressive).\nLast but not least, the intelligent detection of money laundry and reporting to the responsible authorities.\nProblem However, there were limitations in the data tools, mainly due to the fact that they were proprietary software (with limited licenses) and designed for usage inside data centers. In addition, the analytical database was modeled for traditional Business Intelligence models (OLAP, etc), making the decision making process heavy, due to the demanding amount of interactions during ETL.\nPreviously for a client to be approved, the process took 10 to 15 days. Gathering all necessary information, providing a complete perspective of the profile, including credit analysis. After collecting the information, the back office generated an internal credit analysis score.\nIn most cases, the client was not notified of updates regarding the process and did not receive feedback at the end (if refused) unless explicitly requested (by contacting support via chat or email, for example) which made the process time consuming and costly. Not to mention the countless amounts of customers lost to the competition during this long wait.\nSolution Technical Implementation To make it possible, we built an hybrid-cloud implementation using AWS cloud-based components (mainly AWS S3, EMR and ECS), to extend the data centers' capability, implementing a cloud-first Hadoop ecosystem (replacing the proprietary software components with open-source equivalents). Giving Easynvest the possibility to grow the Data Lake exponentially.\nThe Data Lake design was robust, aimed to handle the execution of heavy analytical processes through Machine Learning models, with support for data quality, metadata governance, information security and data serving (data owners could share data with consumers from other areas within the company, allowing to self-service their analytical data).\nA Chatbot was also used to reduce the operational load in the environment, this bot is responsible for maintaining and updating infrastructure components. From triggering deployments to generating encryption keys on-demand for data security and governance. Implemented with the Errbot framework for Python interacting with Slack.\nGoing further, we implemented the best practices in DevOps, using Jenkins as a tool for CI/CD of the developed components alongside with Ansible for Configuration Management.\nImpact and results Thanks to the utilization of layers in the Data Lake and the implementation of data pipelines, we were able to reduce the data ingestion time by 78% and to include metadata and data catalog, in addition to automating much of the work that used to be done manually.\nThus bringing positive results, especially reducing the registration approval time for consumers from roughly 10 days to 1 day. It also made the data platform more democratic, providing relevant information that facilitates the analysis of areas such as risk (credit analysis) and support, without having to give up security.\n","date":1499040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589871935,"objectID":"85d7e13b7fc188beab732ff2f4c83ca7","permalink":"https://macunha.me/en/project/2017/easynvest-data-platform/","publishdate":"2017-07-03T00:00:00Z","relpermalink":"/en/project/2017/easynvest-data-platform/","section":"project","summary":"Hybrid-cloud Data Lake with most of its capabilities running in AWS. Among the main objectives we had the automation of credit analysis, targeted campaigns to investors according to profile and intelligent detection of money laundry","tags":["data-eng"],"title":"Easynvest Data Platform","type":"project"},{"authors":null,"categories":null,"content":"Introduction Summary JSL Holdings Ltd, holder of Julio SimÃµes Logistica (biggest logistics players in LATAM) bought Movida Rent a Car in 2013 to expand the portfolio and open new market opportunities on car rental and selling markets.\nJSL invested around R$1.8 billion in Movida, and multiplied its annual revenues by 21 times from BRL 58m to BRL 1.2b, in 3 years. Based on these successful results, JSL Holdings Ltd planned an IPO for Movida.\nProblem In order to be public traded, Movida had to pass through an audition. However, the software solution did not comply with some security standards.\nThe project started on December 2016, planning to implement an automated software release process adopting DevOps on their data center. With the goals:\n security; no person would need to access the Linux servers. and productivity; releasing features faster to shorten their time-to-market.  Solution Technical Implementation Our first goal was to implement the CI/CD pipeline using Jenkins, responsible to pack new features, create a release, and deploy it on their data center. Apart from the production deployment, the pipeline also supported the creation of ephemeral on-demand environments for feature homologation and feedback retrieval from users.\nTo have a faster and more controlled release cycle, we migrated the Git server from a cloud-hosted to the data center. Through this action we reduced in 5 minutes the deployment overall time and increased the control over accesses in their repositories.\nThe CI/CD implementation used Jenkins to control the CI/CD flow, GitLab with LDAP authentication, and Ansible as a Configuration Manager. A complete deployment took around 2 minutes from the git push to having code running on production.\nApart from the CI/CD deployment process, we also had to work in a self-service strategy for running jobs without directly SSH access to servers. Rundeck came into place, with RBAC configurations and visibility over the history of executed jobs.\nImpact and results Movida went through audition early on January 2017, by the end of January 2017 they received the approval.\nTwo weeks later, in February 2017 Movida launched their IPO, marked as the first Brazilian IPO of 2017. Movida went public on the 8th of February, 2017, raising BRL 645m.\n","date":1486425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589871935,"objectID":"bb268eef80befd26cecdab4785cba57a","permalink":"https://macunha.me/en/project/2016/movida-rent-a-devops/","publishdate":"2017-02-07T00:00:00Z","relpermalink":"/en/project/2016/movida-rent-a-devops/","section":"project","summary":"Movida DevOps initial project, responsible for implementing the base for Continuous Deployment, configuration management and improve servers' security.","tags":["devops"],"title":"Movida Rent A DevOps","type":"project"},{"authors":null,"categories":null,"content":"Introduction Summary Nextel planed to develop a mobile application to reduce their contact rate and operational costs with call centers. \u0026ldquo;Nextel Digital\u0026rdquo; was the name given to the project responsible for releasing this application.\n\u0026ldquo;Nextel Digital\u0026rdquo; absorbed more goals like improving the User experience, and turned into a new product called \u0026ldquo;Happy\u0026rdquo;, a digital cell phone operator. Nextel Happy allows users to manage their plans and data entirely from the mobile app, from activating your SIM to managing your family plan.\nThis project helped Nextel to increase their customers base, improved the users\u0026rsquo; experience, and decrease operational costs (in 16%) all at once.\nProblem Nextel Brazil executive team decided to work with outsourcing on the development of this product to absorb the knowledge from digital companies and to complement their internal capabilities. Also to bring different perspectives into play, improving the creative process.\nOur team assumed the responsibility to architect and to implement the Cloud infrastructure ensuring high-availability, resilience and consistency of the software.\nWe were also responsible for the data synchronization between Nextel data center and the cloud. Securely moving tons of GB of users\u0026rsquo; data to the cloud daily without data loss or duplication.\nSolution Technical implementation We choose GlusterFS to ensure consistency, installed between Nextel Data Center and AWS. Users\u0026rsquo; data (e.g. data plan consumption, minutes of call) synchronization went through GlusterFS to AWS. Nextel IT operations inserted data into GlusterFS directly from cell phone towers in near-real-time.\nOnce the data is available at AWS mounted volumes, the Celery implementation comes into play. At the core of the architecture, Celery (implemented in Python 3) using Redis as the message broker, running asynchronous jobs inspects events on the GlusterFS. When Celery detects a new file it parses the content and starts the multi-part upload to AWS S3, then compares the checksums to ensure consistency (and retries in case it\u0026rsquo;s inconsistent).\nAfter reaching AWS S3 the object event triggers a AWS Lambda function to parse the content and index it on Elasticsearch, whose are later served to clients through an REST API.\nThe entire infrastructure setup was immutable, to facilitate the evolution and reliability, using Ansible as a Configuration Manager and AWS CloudFormation as the Cloud Provisioner. In just a couple minutes it is possible to recreate everything with minimum effort.\nImpact and results The entire process of making data from a cell phone tower available to end users time went down from 1 day to 5 minutes. This reduced in ~56% the contact rate on Nextel call centers, due to a self-service alternative provided in the mobile app.\nIn addition, users can manage their call history and plan consumption directly on the mobile phone, with updates in near-real-time. Providing consistent and interactive feedback.\n","date":1480464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589871935,"objectID":"833e846000cba1cddc227ac12811391c","permalink":"https://macunha.me/en/project/2016/nextel-digital-release/","publishdate":"2016-11-30T00:00:00Z","relpermalink":"/en/project/2016/nextel-digital-release/","section":"project","summary":"Digital transformation project at Nextel Brazil, which evolved into a product called \"Happy\", a digital telephony operator. Improving the user experience and reducing operating costs.","tags":["devops"],"title":"Nextel Digital Release","type":"project"}]