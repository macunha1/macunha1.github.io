<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>It's me, Macunha!</title><link>https://macunha.me/pt/</link><atom:link href="https://macunha.me/pt/index.xml" rel="self" type="application/rss+xml"/><description>It's me, Macunha!</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>pt-br</language><lastBuildDate>Thu, 21 May 2020 23:00:57 +0200</lastBuildDate><image><url>https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_512x512_fill_lanczos_center_2.png</url><title>It's me, Macunha!</title><link>https://macunha.me/pt/</link></image><item><title>Quickstart: Apache Spark no Kubernetes</title><link>https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/</link><pubDate>Thu, 21 May 2020 23:00:57 +0200</pubDate><guid>https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="apache-spark-operator-para-kubernetes">Apache Spark Operator para Kubernetes&lt;/h2>
&lt;p>Desde o seu lançamento em 2014 pela Google, o Kubernetes tem ganhado
muita popularidade junto com o próprio Docker e desde 2016 passou a
ser o &lt;em>de facto Container orchestrator&lt;/em>, estabelecido como um padrão
de mercado. Possuindo versões gerenciadas em &lt;strong>todas&lt;/strong> as &lt;em>major
Clouds&lt;/em>
&lt;a href="https://cloud.google.com/kubernetes-engine/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://docs.microsoft.com/en-us/azure/aks/" target="_blank" rel="noopener">[3]&lt;/a> (inclusive
&lt;a href="https://www.digitalocean.com/products/kubernetes/" target="_blank" rel="noopener">Digital Ocean&lt;/a> e
&lt;a href="https://www.alibabacloud.com/product/kubernetes" target="_blank" rel="noopener">Alibaba&lt;/a>).&lt;/p>
&lt;p>Toda essa popularidade tem atraído novas implementações e &lt;em>use-cases&lt;/em>
para o orquestrador, dentre eles a execução de
&lt;a href="https://kubernetes.io/docs/tutorials/stateful-application/" target="_blank" rel="noopener">Stateful
applications&lt;/a>
incluindo
&lt;a href="https://vitess.io/zh/docs/get-started/kubernetes/" target="_blank" rel="noopener">bancos de dados em containers&lt;/a>.&lt;/p>
&lt;p>Qual seria a necessidade de ter um banco de dados orquestrado? Ótima
pergunta. Por hoje, vamos focar na utilização do
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md" target="_blank" rel="noopener">Spark Operator&lt;/a>
para executar &lt;strong>Spark jobs&lt;/strong> em Kubernetes.&lt;/p>
&lt;p>A idéia do Spark Operator
&lt;a href="https://github.com/kubernetes/kubernetes/issues/34377" target="_blank" rel="noopener">surgiu&lt;/a>
em 2016, antes disso haviam apenas alguns &lt;em>jeitinhos&lt;/em>, por exemplo:
&lt;a href="https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/" target="_blank" rel="noopener">com Apache Zeppelin&lt;/a>
dentro do Kubernetes, ou então, mais refinado ainda
&lt;a href="https://github.com/kubernetes/examples/tree/master/staging/spark" target="_blank" rel="noopener">criando o seu
próprio Apache Spark cluster dentro do Kubernetes (exemplo do
repositório oficial do Kubernetes)&lt;/a>
que usaria o
&lt;a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Spark Standalone mode&lt;/a>.&lt;/p>
&lt;p>Porém, executar nativamente seria muito mais interessante pois poderia
aproveitar o
&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank" rel="noopener">Kubernetes Scheduler&lt;/a>
para ações relacionadas à alocação dos recursos no cluster, dando mais
elasticidade e uma interface mais simples para gerenciar os workloads
no Apache Spark.&lt;/p>
&lt;p>Considernando esses pontos o
&lt;a href="https://issues.apache.org/jira/browse/SPARK-18278" target="_blank" rel="noopener">desenvolvimento do Apache Spark Operator
ganhou atenção&lt;/a>,
foi &lt;em>mergeado&lt;/em> e publicado na
&lt;a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">versão do Apache Spark 2.3.0&lt;/a>
em
&lt;a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">Fevereiro de 2018&lt;/a>.&lt;/p>
&lt;p>Se você estiver interessado em ler mais sobre a proposta do Apache
Spark Operator, existe um
&lt;a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target="_blank" rel="noopener">design document publicado no Google Docs.&lt;/a>&lt;/p>
&lt;h2 id="por-que-kubernetes">Por que Kubernetes?&lt;/h2>
&lt;p>Como atualmente as empresas estão buscando se
&lt;a href="https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html" target="_blank" rel="noopener">reinventar por meio da
tão falada transformação
digital&lt;/a>
para que possam ter competitividade e, principalmente, sobreviver
diante de um mercado cada vez mais dinâmico, é comum ver abordagens
que incluam Big Data, Inteligência Artificial e Cloud Computing
&lt;a href="https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/" target="_blank" rel="noopener">[1]&lt;/a>
&lt;a href="https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/" target="_blank" rel="noopener">[2]&lt;/a>
&lt;a href="https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/" target="_blank" rel="noopener">[3]&lt;/a>.&lt;/p>
&lt;p>Para compreender os benefícios de utilizar Cloud ao invés de On-premises no
contexto de Big Data vale a pena ler o artigo
&lt;a href="https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html" target="_blank" rel="noopener">da Databricks&lt;/a>,
que é a empresa
&lt;a href="https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/" target="_blank" rel="noopener">fundada pelos criadores do Apache
Spark&lt;/a>.&lt;/p>
&lt;p>Como nós vemos uma adoção de Cloud Computing generalizada (até por
empresas que teriam condições de bancar o próprio hardware), também
podemos notar que na maiorira dessas implementações de Cloud não
existem clusters de
&lt;a href="https://hadoop.apache.org/" target="_blank" rel="noopener">Apache Hadoop&lt;/a>
já que os times de Dados (BI/Data Science/Analytics) optam cada vez
mais por utilizar ferramentas como
&lt;a href="https://cloud.google.com/bigquery/" target="_blank" rel="noopener">Google BigQuery&lt;/a>
ou
&lt;a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">AWS Redshift&lt;/a>. Portanto, não faz sentido
subir um Hadoop apenas para utilizar o
&lt;a href="https://hortonworks.com/apache/yarn/" target="_blank" rel="noopener">YARN&lt;/a>
como gerenciador os recursos.&lt;/p>
&lt;p>Uma alternativa é a utilização de provisionadores de clusters Hadoop
como o
&lt;a href="https://cloud.google.com/dataproc" target="_blank" rel="noopener">Google DataProc&lt;/a> ou o
&lt;a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener">AWS
EMR&lt;/a> para a criação de clusters efêmeros.
Apenas para nomear algumas opções.&lt;/p>
&lt;p>Para entender melhor o design do Spark Operator, recomendo a leitura
da documentação gerada pela equipe da
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#the-crd-controller" target="_blank" rel="noopener">GCP no
GitHub&lt;/a>.&lt;/p>
&lt;h1 id="hora-de-meter-a-mão-na-massa">Hora de meter a mão na massa!&lt;/h1>
&lt;h2 id="aquecendo-o-motor">Aquecendo o motor&lt;/h2>
&lt;p>Agora que toda a palavra já foi passada, vamos ao hands-on para
mostrar a coisa acontecendo. Para isso, vamos usar:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://www.docker.com/" target="_blank" rel="noopener">Docker&lt;/a> como motor de container no
Kubernetes, e construção da imagem
&lt;a href="https://docs.docker.com/install/" target="_blank" rel="noopener">(link para
instalação)&lt;/a>;&lt;/li>
&lt;li>Minikube
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">(link para instalação)&lt;/a>
para facilitar o provisionamento do Kubernetes (sim, será uma
execução local);&lt;/li>
&lt;li>Para interagir com a API do Kubernetes é preciso ter o &lt;code>kubectl&lt;/code>
instalado.
&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">Se você não tiver, siga as instruções aqui&lt;/a>.&lt;/li>
&lt;li>uma versão compilada do Apache Spark que seja maior do que a
2.3.0.
&lt;ol>
&lt;li>você pode tanto compilar
&lt;a href="https://github.com/apache/spark" target="_blank" rel="noopener">do código fonte&lt;/a>,
que vai tomar &lt;em>algumas horas&lt;/em> até terminar, quanto&lt;/li>
&lt;li>fazer o download de uma versão compilada
&lt;a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">aqui&lt;/a>
(recomendado).&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>Assim que o Apache estiver descompactado, vamos adicionar o
caminho no PATH para facilitar a execução:&lt;/p>
&lt;pre>&lt;code class="language-bash">export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin
&lt;/code>&lt;/pre>
&lt;h2 id="criando-o-cluster-com-minikube">Criando o &amp;ldquo;cluster&amp;rdquo; com Minikube&lt;/h2>
&lt;p>Agora, para ter um Kubernetes vamos iniciar um &lt;code>minikube&lt;/code> com o
propósito de rodar um dos exemplos disponíveis no
&lt;a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala" target="_blank" rel="noopener">repositório do Spark&lt;/a>
chamado &lt;code>SparkPi&lt;/code> apenas para demonstração.&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube start --cpus=2 \
--memory=4g
&lt;/code>&lt;/pre>
&lt;h2 id="buildando-a-imagem-docker">Buildando a imagem Docker&lt;/h2>
&lt;p>Vamos utilizar o Docker daemon do Minikube para não depender de um
registry externo (e só gerar lixo na VM, facilitando a limpeza
depois). Para isso, o minikube tem um wrapper que facilita a nossa
vida:&lt;/p>
&lt;pre>&lt;code class="language-bash">eval $(minikube docker-env)
&lt;/code>&lt;/pre>
&lt;p>Após ter configurado as variáveis de ambiente para o Docker daemon,
vamos precisar de uma imagem Docker para executar os jobs.
Existe um
&lt;a href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh" target="_blank" rel="noopener">shell script no repositório do Spark&lt;/a>
para ajudar com isso. Considerando que o &lt;code>PATH&lt;/code> está configurado
corretamente, basta executar:&lt;/p>
&lt;pre>&lt;code class="language-bash">docker-image-tool.sh -m -t latest build
&lt;/code>&lt;/pre>
&lt;p>&lt;em>Obs.:&lt;/em> O parâmetro &lt;code>-m&lt;/code> aqui indica que é um build para o minikube.&lt;/p>
&lt;p>Vamos pegar a via expressa para executar o SparkPi, usando o mesmo
comando que seria utilizado para um cluster Spark
&lt;a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">spark-submit&lt;/a>.&lt;/p>
&lt;p>Porém, o Spark Operator dá suporte a definição de jobs no &amp;ldquo;dialeto do
Kubernetes&amp;rdquo; usando
&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">CRD&lt;/a>,
&lt;a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples" target="_blank" rel="noopener">aqui tem alguns exemplos&lt;/a> - para depois.&lt;/p>
&lt;h2 id="fire-in-the-hole">Fire in the hole!&lt;/h2>
&lt;p>Cuidado com o vão entre a versão do Scala e a plataforma quando
estiver parametrizando o job:&lt;/p>
&lt;pre>&lt;code class="language-bash">spark-submit --master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=2 \
--executor-memory 1024m \
--conf spark.kubernetes.container.image=spark:latest \
local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # aqui
&lt;/code>&lt;/pre>
&lt;p>O que temos de novidade é:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--master&lt;/code>: Aceita o prefixo &lt;code>k8s://&lt;/code> na URL, para o endpoint da
API do Kubernetes, exposta pelo commando &lt;code>https://$(minikube ip):8443&lt;/code>.
Aliás, se estiver interessado, isso é um
&lt;a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html" target="_blank" rel="noopener">command substitution no
shell&lt;/a>;&lt;/li>
&lt;li>&lt;code>--conf spark.kubernetes.container.image=&lt;/code>: Configuração para a
imagem Docker que será executada no Kubernetes.&lt;/li>
&lt;/ul>
&lt;p>Com o output:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&amp;gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&amp;gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]
19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
&lt;/code>&lt;/pre>
&lt;p>Para ver o resultado do job (e toda a execução), podemos mandar um
&lt;code>kubectl logs&lt;/code> passando o nome do pod do driver como parâmetro:&lt;/p>
&lt;pre>&lt;code class="language-bash">kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')
&lt;/code>&lt;/pre>
&lt;p>Que traz o output (algumas entradas foram omitidas), parecido com:&lt;/p>
&lt;pre>&lt;code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
&lt;/code>&lt;/pre>
&lt;p>O resultado aparece no stdout:&lt;/p>
&lt;pre>&lt;code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
&lt;/code>&lt;/pre>
&lt;p>Para finalizar, vamos deletar a VM que o Minikube gera, para limpar o
ambiente (a menos que você queira continuar brincando com ele):&lt;/p>
&lt;pre>&lt;code class="language-bash">minikube delete
&lt;/code>&lt;/pre>
&lt;h2 id="últimas-palavras">Últimas palavras&lt;/h2>
&lt;p>Espero ter dispertado bastante curiosidade e algumas ideias para
ir além no desenvolvimento dos seus workloads de Big Data.
Se tiver alguma dúvida ou sugestão, não deixe de postar na seção de
comentários.&lt;/p></description></item><item><title>ReclameAQUI Data Lake</title><link>https://macunha.me/pt/project/2019/reclameaqui-data-lake/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://macunha.me/pt/project/2019/reclameaqui-data-lake/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;h2 id="resumo">Resumo&lt;/h2>
&lt;p>ReclameAQUI é um negócio interessante e único. É um agregador de conteúdo para
troca de experiências dos clientes (especialmente más experiências) sobre
compras (online e offline). No entanto, vai além de um simples &amp;ldquo;site de
reclamações&amp;rdquo;, oferecendo uma interface para as empresas responderem às
reclamações, ajudando os clientes com seus problemas.&lt;/p>
&lt;p>O serviço é simplesmente o maior neste sentido (mundial) recebendo diariamente
600K visitantes únicos, buscando a reputação de uma empresa antes de fechar um
negócio/compra.&lt;/p>
&lt;h2 id="problema">Problema&lt;/h2>
&lt;p>Apesar de já estarem avançados na abordagem de business digital, tendo a
maioria dos serviços hospedados em Cloud computing e cultura analítica, seu
Data Lake precisava de alguns upgrades. O maior motivador deste projeto foram as
contas altíssimas da GCP, especialmente relacionadas ao consumo de dados no
BigQuery.&lt;/p>
&lt;p>Além das tarefas de redução de custos e otimização do processo de ingestão de
dados, aproveitamos a oportunidade para implementar criptografia de dados
at-rest, governança e obfuscação durante as queries contra o data lake. Tornando
os dados acessíveis por todos na empresa, controlando o acesso e gerenciamento
de identidade através do LDAP (auditando cada acesso, para estar em total
conformidade com a GDPR), pudemos oferecer um Data Lake self-service para que
diferentes atores empresariais pudessem satisfazer suas necessidades &amp;ldquo;bebendo&amp;rdquo;
do lake.&lt;/p>
&lt;h1 id="solução">Solução&lt;/h1>
&lt;h2 id="implementação-técnica">Implementação técnica&lt;/h2>
&lt;p>Os principais objetivos foram otimização de custos do Data Lake existente,
melhoria (e extensão) dos pipelines de ingestão de dados existentes, e
aperfeiçoamentos na segurança.&lt;/p>
&lt;p>Partindo pela otimização de custos do Data Lake, reimplantamos os pipelines de
ingestão de dados, utilizando uma área de &amp;ldquo;landing&amp;rdquo; para dados brutos e
posteriormente transformações eram realizadas para adequar os modelos de dados
desejados. Salvando os resultados em outras camadas do Data Lake, para atingir
maior performance nas queries.&lt;/p>
&lt;p>Removemos as Streaming inserts no BigQuery adicionando um step para fazer load
dos dados ao fim da ingestão. O Apache NiFi foi o principal software responsável
pela orquestração e execução da ingestão de dados, abrangendo também as
melhorias da ingestão de dados através da reengenharia dos processos.&lt;/p>
&lt;p>A auditoria no Data Lake foi gerenciada através do Apache Ranger. Para ter
suporte total, implementamos um driver JDBC usando um componente do Apache
Calcite chamado Avatica. A autenticação para o Apache Ranger passou por um
plugin personalizado (também desenvolvido durante o projeto) para LDAP
consumindo informações de usuários do Google Cloud Identity, refletindo a
organização de grupos e usuários existente do Google Suite.&lt;/p>
&lt;p>Para tornar o jogo mais interessante, containerizamos o workflow e utilizamos
bastante Kubernetes (GKE) para gerenciar estes componentes. A maioria dos
componentes Apache não tinha Helm Charts na época e nós os desenvolvemos e
tornamos alguns open-source.&lt;/p>
&lt;h2 id="impacto-e-resultados">Impacto e resultados&lt;/h2>
&lt;p>Durante o tempo do projeto pudemos medir uma estimativa de aproximadamente 56%
de redução de custos do Data Lake através da otimização de recursos e
reengenharia de processos, especialmente a remoção de Streaming inserts no
BigQuery.&lt;/p>
&lt;p>Obtivemos progresso relevante em termos de segurança e governança durante o
projeto, com a introdução do Apache Ranger e auditoria de acesso e uso do Data
Lake, fornecendo capacidades avançadas de segurança à ReclameAQUI, que se
antecipou frente à LGPD e as preocupações em questão da privacidade dos dados.&lt;/p></description></item><item><title>DevOps: Os benefícios da implementação</title><link>https://macunha.me/pt/post/2019/01/devops-benefits/</link><pubDate>Fri, 11 Jan 2019 23:00:00 +0000</pubDate><guid>https://macunha.me/pt/post/2019/01/devops-benefits/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;p>Principais benefícios que uma companhia geralmente espera e encontra na adoção da cultura:&lt;/p>
&lt;h2 id="releases-mais-rápidos-e-baratos">Releases mais rápidos e baratos&lt;/h2>
&lt;p>Como os releases serão frequentes, as entregas acabam sendo pequenas. Entregas pequenas trazem o benefício de aumentar a velocidade no ciclo de desenvolvimento (entregando sempre)&lt;/p>
&lt;h2 id="suporte-operacional-melhorado-com-ajustes-rápidos">Suporte operacional melhorado com ajustes rápidos&lt;/h2>
&lt;p>Caso haja alguma falha durante a entrega, o impacto é menor pois a quantidade de modificações é pequena, assim como o rollback é rápido.&lt;/p>
&lt;h2 id="melhor-time-to-market-ttm">Melhor time to market (TTM)&lt;/h2>
&lt;p>O software será entregue muito mais cedo, quando ainda for um MVP. Os clientes serão integrados como parte do processo de desenvolvimento, trazendo insights e feedbacks para o time. Permitindo assim que haja uma velocidade maior de lançamento no mercado.&lt;/p>
&lt;h2 id="produtos-de-qualidade-superior">Produtos de qualidade superior&lt;/h2>
&lt;p>Como foi falado antes, falhar cedo impede que defeitos sejam entregues em produção&lt;/p>
&lt;p>Assim como:&lt;/p>
&lt;ul>
&lt;li>Menor volume de defeitos no produto como um todo;&lt;/li>
&lt;li>Aumento na frequência de novas features e releases;&lt;/li>
&lt;li>Processos de desenvolvimento apropriados nos times, incluindo automação.&lt;/li>
&lt;/ul>
&lt;h1 id="agora-que-entendemos-o-por-que-vamos-ao-como">Agora que entendemos o POR QUE, vamos ao COMO&lt;/h1>
&lt;h2 id="continuous-releases-integration-delivery-deployment">Continuous releases (integration, delivery, deployment)&lt;/h2>
&lt;p>Geralmente segue uma abordagem de versionamento de código (por meio do Git) utilizando branches específicas para cada ambiente.&lt;/p>
&lt;h2 id="continuous-integration">Continuous integration&lt;/h2>
&lt;p>Execução automática dos testes unitários, integrados e também de qualidade de código na branch, para garantir que não houve quebra de funcionamento do pedaço modificado do código.&lt;/p>
&lt;h2 id="continuous-delivery">Continuous delivery&lt;/h2>
&lt;p>Empacotamento do software que está testado e aprovado, para deixar ele em algum lugar que seja possível fazer um deploy posteriormente. Exemplos são libs entregues em repositórios para ser integradas no código durante o próximo update e deploy de código&lt;/p>
&lt;h2 id="continuous-deployment">Continuous deployment&lt;/h2>
&lt;p>Após conseguir completar todos os passos anteriores, é possível fazer deploy automatizado direto nos ambientes, quando o time estiver com mais confiança em relação às ferramentas que testam, assim como com a questão de assumir riscos e entender que existe a possibilidade de falhar em ambientes de teste, sem preocupações.&lt;/p>
&lt;h2 id="configuration-eou-infrastructure-as-code">Configuration (e/ou Infrastructure) as code&lt;/h2>
&lt;p>Para que seja possível testar o software com assertividade, e entender que ele irá transitar entre os ambientes sem mudar de comportamento, é de extrema importância que as configurações sejam também código. Isso permite que as configurações sejam também versionadas, acompanhando o código. Garantindo também uma uniformidade entre os ambientes, que possibilita:&lt;/p>
&lt;ul>
&lt;li>Redução nos custos de manutenção, tendo um ponto único para olhar e entender o funcionamento do sistema;&lt;/li>
&lt;li>Facilidade para recriar a infraestrutura, caso seja necessário mover tudo para outro lugar, isso pode acontecer com poucas interações manuais;&lt;/li>
&lt;li>Permite que haja code review da infraestrutura e das configurações, que por consequência traz uma cultura de colaboração no desenvolvimento, compartilhamento do conhecimento e aumenta a democratização da infra;&lt;/li>
&lt;li>Documentação como código, ajudando novos membros do time a terem um warm up mais rápido.&lt;/li>
&lt;/ul>
&lt;p>Esses pontos foram bem estressados pelo time da Heroku, e deram origem ao famoso paper:
&lt;a href="https://12factor.net/" target="_blank" rel="noopener">The Twelve-Factor App&lt;/a>. Uma leitura muito boa para a explicação dos benefícios sobre gerência de configuração.&lt;/p>
&lt;h2 id="monitoramento-e-self-healing">Monitoramento e self-healing&lt;/h2>
&lt;p>Ao fim de todo o processo de entrega, o software deverá estar sendo monitorado, para que não seja necessário esperar um report externo de falhas, garantindo que as ações sejam pró-ativas e não reativas.&lt;/p>
&lt;p>Garantir que o monitoramento esteja maduro, também nos permite automatizar a parte de reação aos alertas, criando um sistema de self-healing em que ações (scripts) são executados para corrigir possíveis falhas conhecidas na infraestrutura. Permitindo assim que todos possam dormir tranquilamente de noite, sem ter que ficar preocupado com o plantão tocar e ter que ler documentação de madrugada. (Se você já teve experiência com isso, sabe com certeza o quanto é ruim).&lt;/p>
&lt;p>Escalando assim apenas os casos que forem extremas exceções (erros não conhecidos/esperados) no processo para o plantonista atuar, garantindo uma maior saúde na operação.&lt;/p>
&lt;h2 id="automação-de-processos">Automação de processos&lt;/h2>
&lt;p>Todo o processo que estiver causando Muda é alvo de automação, para que as pessoas possam trabalhar com mais agilidade. Bons exemplos de processos que costumam ser automatizados são:&lt;/p>
&lt;ul>
&lt;li>Deployment;&lt;/li>
&lt;li>Self-healing (resiliência do sistema em resposta às anomalias);&lt;/li>
&lt;li>Renovação de Certificados;&lt;/li>
&lt;li>Execução de testes (unitários, de integração, funcionais, etc);&lt;/li>
&lt;li>Monitoramento (com auto-discovery);&lt;/li>
&lt;li>Governança de usuários;&lt;/li>
&lt;/ul>
&lt;h1 id="devops-toolchainhttpsenwikipediaorgwikidevops_toolchain">
&lt;a href="https://en.wikipedia.org/wiki/DevOps_toolchain" target="_blank" rel="noopener">DevOps toolchain&lt;/a>&lt;/h1>
&lt;p>Uma combinação de ferramentas para facilitar a manutenção e operação do sistema, seguindo o seguinte fluxo:&lt;/p>
&lt;p>&lt;img src="https://macunha.me/img/content/devops-lifecycle.png" alt="Ciclo de desenvolvimento utilizando DevOps">&lt;/p>
&lt;blockquote>
&lt;p>Obs.: Qualquer semelhança com o PDCA é mera certeza.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Plan&lt;/strong>: Fase de planejamento do projeto, em que são coletados os feedbacks para levantamento de requisitos, e criação do backlog;&lt;/li>
&lt;li>&lt;strong>Create&lt;/strong>: Criação de um entregável (para validar uma hipótese), como um MVP;&lt;/li>
&lt;li>&lt;strong>Verify&lt;/strong>: Passar o entregável para a fase de testes;&lt;/li>
&lt;li>&lt;strong>Package:&lt;/strong> Empacotar o entregável (build) para conseguir colocar ele em algum ambiente de testes;&lt;/li>
&lt;li>&lt;strong>Release&lt;/strong>: Fazer o deployment do entregável empacotado;&lt;/li>
&lt;li>&lt;strong>Configure&lt;/strong>: Realizar a configuração do entregável no ambiente de testes, tentando chegar o mais próximo possível do twelve-factor app.&lt;/li>
&lt;li>&lt;strong>Monitor&lt;/strong>: Após fazer o deployment no ambiente, acompanhar as métricas de negócio e infraestrutura, para garantir que está tudo funcionando conforme o esperado.&lt;/li>
&lt;/ul>
&lt;h1 id="conclusão">Conclusão&lt;/h1>
&lt;p>Durante a implementação dessas técnicas é possível observar melhoras no processo de desenvolvimento, os ganhos mais notáveis são:&lt;/p>
&lt;ul>
&lt;li>Melhoria no engajamento do time;&lt;/li>
&lt;li>Compartilhamento de conhecimento;&lt;/li>
&lt;li>Redução de bottlenecks;&lt;/li>
&lt;li>Mais tempo livre para realizar trabalho que realmente importa (agrega valor para a experiência do usuário ou gera impacto);&lt;/li>
&lt;li>Maior confiança ao entregar software.&lt;/li>
&lt;/ul></description></item><item><title>DevOps: De onde vieram e para onde vão</title><link>https://macunha.me/pt/post/2019/01/devops-genesis/</link><pubDate>Fri, 11 Jan 2019 22:00:00 +0000</pubDate><guid>https://macunha.me/pt/post/2019/01/devops-genesis/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;p>&lt;em>First of all, it’s all about Agile.&lt;/em>&lt;/p>
&lt;p>A metodologia DevOps foi criada utilizando os conceitos dos métodos Ágeis, para entregar um grande valor durante o processo de desenvolvimento de software, automatizando o release de features com pipelines, para conseguir provar hipóteses mais rápido e assim se adaptar mais rápido às mudanças por meio de uma abordagem “fail-fast”. Essas mudanças são mais culturais do que técnicas, portanto é comum dizer que DevOps é cultura.&lt;/p>
&lt;p>A implementação de DevOps acontece por meio da automação de processos, tendo muito forte dentro dessa implementação a reengenharia de processos da empresa. A implementação da parte técnica é simples e fácil se comparada com a mudança de comportamento das pessoas. Portanto, é bastante confuso o papel que um “Engenheiro/Analista DevOps” desempenharia, no meio dessa confusão, existem muitos SysAdmins e Analistas de Infra assumindo o papel de “DevOps”.&lt;/p>
&lt;h2 id="first-things-first-lean-é-a-base-do-agile">First things first, Lean é a base do Agile&lt;/h2>
&lt;p>A realidade não é tão feliz quanto parece. Depois da segunda guerra mundial, o Japão estava destruído e com poucos recursos, após ter perdido a guerra. Com uma quantia limitada de recursos, o país precisou se reinventar e sobreviver após uma época de forte depressão, durante essa época dois caras ficaram famosos pelo trabalho realizado dentro de uma empresa, que mais tarde teve o nome dedicado ao modelo que foi criado.&lt;/p>
&lt;p>Esses caras eram Eiji Toyoda e Taiichi Ohno, que trabalhavam para a Toyota Motor Corporation. Foram os fundadores do modelo de produção Toyota, também conhecido como Toyotismo.&lt;/p>
&lt;h2 id="toyota-deu-origem-ao-lean">Toyota deu origem ao Lean&lt;/h2>
&lt;p>Lean é uma metodologia que ensina a otimizar os processos da empresa; end-to-end, para dar mais atenção às tarefas que &lt;strong>entregam valor&lt;/strong> ao consumidor final, incentivando ao máximo a remoção de bottlenecks no processo, assim como a análise de tarefas que são desperdício (definidas pelos 3M) para que sejam identificadas e removidas.&lt;/p>
&lt;p>Essas tarefas consideradas desperdício são classificadas como 3M dentro do Lean que representam: &lt;em>Muda, Mura, e Muri&lt;/em>. Outro ponto importante a destacar no processo é a utilização do método nomeado Kaizen (continuous improvement), com foco em melhorar continuamente buscando atingir um nível de excelência em qualidade.&lt;/p>
&lt;p>A qualidade faz parte da cultura japonesa, pois existe a crença de que um produto de qualidade traz o cliente de volta, mesmo que os produtos demorem mais para estragar, os clientes serão fiéis a eles, pois terão uma boa experiência. Antes mesmo de falarmos sobre user experience, eles já estavam pensando nisso.&lt;/p>
&lt;h2 id="kaizen">Kaizen&lt;/h2>
&lt;p>Um mindset que ajuda a olhar para cada parte do processo exclusivamente e pensar nas melhorias, envolvendo as pessoas que fazem parte do processo, assim incentiva que haja a inclusão dessas pessoas nas decisões de mudança, já que:&lt;/p>
&lt;ul>
&lt;li>Fica muito mais fácil de aceitar uma mudança quando ela não é imposta (top-down);&lt;/li>
&lt;li>Há uma absorção maior das mudanças pelas pessoas, quando elas são incluídas no processo;&lt;/li>
&lt;li>As pessoas que são incluídas no processo trazem as suas preocupações e sugestões, que contribuem positivamente com a evolução da mudança, tornando a ideia mais robusta.&lt;/li>
&lt;/ul>
&lt;p>O processo de definição das melhorias por meio de Kaizen acontece (normalmente) na seguinte ordem:&lt;/p>
&lt;p>&lt;img src="https://cdn-images-1.medium.com/max/600/1*N8KelcHbixESci5Y0JQoYA.png" alt="círculo PDCA">&lt;/p>
&lt;ol>
&lt;li>Definir objetivos com base em dados;&lt;/li>
&lt;li>Revisar o estado atual e desenvolver um plano de melhoria;&lt;/li>
&lt;li>Implementar a melhoria;&lt;/li>
&lt;li>Revisar a implementação e corrigir o que não funciona;&lt;/li>
&lt;li>Reportar os resultados e determinar os itens a serem monitorados.&lt;/li>
&lt;/ol>
&lt;p>Esse processo é bastante chamado de &lt;strong>PDCA: Plain-Do-Control-Act&lt;/strong>, que se resume em:&lt;/p>
&lt;ul>
&lt;li>Plan (desenvolver a hipótese);&lt;/li>
&lt;li>Do (executar um experimento);&lt;/li>
&lt;li>Check (validar os resultados);&lt;/li>
&lt;li>Act (refinar o experimento e recomeçar).&lt;/li>
&lt;/ul>
&lt;h1 id="3m-muda-mura-muri">3M: Muda, Mura, Muri&lt;/h1>
&lt;h2 id="muda-desperdício">Muda (desperdício)&lt;/h2>
&lt;p>Qualquer atividade que consuma tempo sem agregar valor ao consumidor final. Como por exemplo:&lt;/p>
&lt;ul>
&lt;li>produção em excesso;&lt;/li>
&lt;li>tempo parado (ocioso) no processo;&lt;/li>
&lt;li>defeito nos produtos.&lt;/li>
&lt;/ul>
&lt;p>É importante lembrar que existem níveis diferentes de Muda que podem ser ou não removidos com rapidez, e a classificação depende do tempo para remoção.&lt;/p>
&lt;p>Um exemplo de Muda mais demorado é a descontinuação de um software legado que acaba tendo ciclos mais longos de release, causando tempo ocioso nos times, muitas vezes uma rotina de testes longa e manual.&lt;/p>
&lt;h2 id="mura-desigualdade">Mura (desigualdade)&lt;/h2>
&lt;p>Atividades que são muito variáveis e imprevisíveis, gerando resultados diferentes em todas as execuções. Por exemplo: a execução de tarefas que não foram bem planejadas e acabam chegando com prazos rígidos. São executadas na correria, gerando um desgaste, desespero, e ainda por cima, ao terminar deixam as pessoas que executaram essas tarefas esperando (seja um feedback, ou então a confirmação de que está finalizado).&lt;/p>
&lt;h2 id="muri-sobrecarga">Muri (sobrecarga)&lt;/h2>
&lt;p>Exigir que as pessoas (ou os equipamentos) trabalhem além do limite, para atingir algum tipo de meta ou expectativa, gerando cansaço e consequentemente falhas durante o processo. Essas falhas são geralmente erros humanos causados pelo cansaço durante o trabalho excessivo.&lt;/p>
&lt;h1 id="voltando-ao-agile">Voltando ao Agile…&lt;/h1>
&lt;p>No ano 2000 um grupo de 17 pessoas se encontrou em um resort em Oregon para conversar sobre ideias que poderiam melhorar o fluxo de desenvolvimento de software. Depois de um ano de amadurecimento das ideias, essas pessoas se encontraram de novo e publicaram as ideias, que hoje conhecemos como: Agile Manifesto.&lt;/p>
&lt;p>Os principais pontos são:&lt;/p>
&lt;ul>
&lt;li>Individuals and interactions over processes and tools&lt;/li>
&lt;li>Working software over comprehensive documentation&lt;/li>
&lt;li>Customer collaboration over contract negotiation&lt;/li>
&lt;li>Responding to change over following a plan&lt;/li>
&lt;/ul>
&lt;p>Vou me restringuir a explicação desses pontos com o ponto de vista DevOps, para não fugir (mais) do tema.&lt;/p>
&lt;h2 id="individuals-and-interactions">Individuals and interactions&lt;/h2>
&lt;p>&lt;em>over processes and tools&lt;/em>&lt;/p>
&lt;p>As pessoas devem receber o kit de ferramentas (tooling) apropriado para trabalhar e então serem empoderados para realizar seu trabalho. As interações entre pessoas é extremamente incentivada, para o compartilhamento de conhecimento e também para facilitar o fluxo criativo dentro dos times de desenvolvimento.&lt;/p>
&lt;p>Um excelente exemplo de interação incentivada por meio de DevOps é o hábito de code review. Considerando que pequenas partes do software serão iteradas e aprovadas no pipeline passando por diferentes ambientes, de maneira automática, o melhor jeito de prevenir defeitos é por meio de code review.&lt;/p>
&lt;p>Esse hábito traz benefício como:&lt;/p>
&lt;ul>
&lt;li>Compartilhamento de conhecimento no time;&lt;/li>
&lt;li>Observação do problema em diferentes pontos de vista;&lt;/li>
&lt;li>Engajamento no time;&lt;/li>
&lt;li>Redução no número de bugs.&lt;/li>
&lt;/ul>
&lt;h2 id="working-software">Working software&lt;/h2>
&lt;p>&lt;em>over comprehensive documentation&lt;/em>&lt;/p>
&lt;p>Aqui existe um trick na questão do working software, um software que funciona não é “compilou, tá funcionando”. O software que funciona é o que atende aos requisitos do usuário; i.e. o software que resolve o problema e as dores do usuário.&lt;/p>
&lt;p>Como o mercado é muito dinâmico, e evolui com grande velocidade, muitas vezes durante o projeto de desenvolvimento de software os requisitos mudam devido a fatores externos. Portanto, sabendo que não é possível prever todos os fatores, muitas gambiarras são feitas no meio do caminho e documentadas, para que o usuário final aprenda a lidar com as falhas, e executar os workarounds, gastando mais esforço do que seria necessário para realizar as tarefas por meio do software.&lt;/p>
&lt;blockquote>
&lt;p>Entregar frequentemente software funcionando, de poucas semanas a poucos meses, com preferência à menor escala de tempo&lt;/p>
&lt;/blockquote>
&lt;p>Incentivando assim que hajam deployments tanto quanto o possível, para que as falhas aconteçam o mais cedo possível, permitindo assim que o impacto delas seja bem menor.&lt;/p>
&lt;h1 id="fail-fast">Fail-fast!&lt;/h1>
&lt;p>As falhas são compreendidas e incentivadas, pois faz parte do mindset entender que:&lt;/p>
&lt;ul>
&lt;li>Só erra quem faz;&lt;/li>
&lt;li>Falhas acontecem.&lt;/li>
&lt;/ul>
&lt;p>Portanto é melhor que as falhas ocorram cedo, enquanto o custo de correção ainda é baixo. Falhar em um ambiente controlado de testes, permite que a correção seja muito mais rápida (e barata) do que seria caso a correção acontecesse já em produção.&lt;/p>
&lt;p>Para que essa abordagem tenha sucesso, existe a premissa de que os ambientes sejam cópias de produção, ou pelo menos que seja o mais próximo possível. Do contrário, haverão mudanças de comportamento no software entre os ambientes, inviabilizando o ambiente de testes.&lt;/p>
&lt;p>Caso os ambientes sejam divergentes, a promoção de bugs para produção será muito frequente, causando falhas tarde, que são falhas caras.&lt;/p>
&lt;h2 id="customer-collaboration">Customer collaboration&lt;/h2>
&lt;p>&lt;em>over contract negotiation&lt;/em>&lt;/p>
&lt;p>Também é possível que haja um má levantamento de requisitos durante o início do projeto, pois muitas vezes o próprio cliente/usuário não conseguiu prever todas as funcionalidades necessárias.&lt;/p>
&lt;p>Podemos descrever essa situação como:&lt;/p>
&lt;ul>
&lt;li>Do ponto A é possível ver apenas o ponto B;&lt;/li>
&lt;li>Do ponto B é possível ver o ponto C;&lt;/li>
&lt;/ul>
&lt;p>Portanto há um grande incentivo para que o software seja entregue em partes, continuamente. Colhendo assim os feedbacks do usuário sobre os próximos passos, seguindo os conceitos de prototipação evolutiva, que foram muito divulgados por meio do livro &lt;em>The Lean Startup&lt;/em>.&lt;/p>
&lt;p>Esse ponto faz muito contraste com o anterior, em relação à entrega contínua (continuous release), para que seja possível apresentar o protótipo e evoluir ele ao longo do projeto.&lt;/p>
&lt;p>&lt;em>Saiba quem é o seu cliente/consumidor/usuário&lt;/em>, e para quem você está fazendo o software, pois esse é o único jeito de conseguir entregar valor para esse cliente. Parte importante do processo de desenvolvimento de software é ser empático com os problemas do usuário, e entender de verdade qual é o problema a ser resolvido, e o resultado causado pelo impacto no desenvolvimento do software (geração do valor para o usuário).&lt;/p>
&lt;h2 id="responding-to-change">Responding to change&lt;/h2>
&lt;p>&lt;em>over following a plan&lt;/em>&lt;/p>
&lt;p>Fazer um redesign dos requisitos durante o projeto é parte crucial para o sucesso do projeto. Será o único jeito de conseguir trazer para a mesa todos os problemas do usuário, e criar a melhor solução para todos esses problemas, pois só o usuário sabe dos reais problemas que ele enfrenta no dia-a-dia lidando com o software.&lt;/p>
&lt;p>Com a entrega contínua de software junto com o monitoramento dos resultados, o processo de coleta dos feedbacks fica muito mais simples e rápido.&lt;/p>
&lt;h1 id="devops-devops-devops">DevOps, DevOps, DevOps&lt;/h1>
&lt;p>Com a divulgação da palavra DevOps, existe muita gente falando sobre DevOps por aí. Existindo muita divergência no que é falado, e criando uma confusão grande sobre o tema. Acaba sendo muito comum se deparar com diferentes interpretações sobre &lt;strong>o que é DevOps&lt;/strong>. Existe muito eufemismo na área, e gourmetização em cima do LinkedIn, com muitos SysAdmins por aí se dizendo DevOps, pois aprenderam a codar shell script dentro do Python.&lt;/p>
&lt;p>Tá afim de continuar lendo?
&lt;a href="https://macunha.me/pt/post/2019/01/devops-benefits/">Dá uma olhada nos benefícios de implementar DevOps.&lt;/a>&lt;/p></description></item><item><title>Clipping Service News OCR</title><link>https://macunha.me/pt/project/2018/clipping-service-news-ocr/</link><pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/pt/project/2018/clipping-service-news-ocr/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;h2 id="sumário">Sumário&lt;/h2>
&lt;p>Como proprietário do maior conjunto de dados de mídia do Brasil, o líder em
monitoramento de mídia Clipping Service estava com problemas de escalabilidade,
aproximando-se da capacidade máxima de seu data center e de seus &amp;ldquo;leitores&amp;rdquo;.&lt;/p>
&lt;p>Clipping Service opera em grande escala, recebendo cerca de ~4,5K páginas de
mídia por dia de cerca de 300 jornais, tanto na versão digital como na impressa.
Anteriormente os funcionários chamados de &amp;ldquo;leitores&amp;rdquo; eram responsáveis pela
leitura e clipping (adicionando destaque ao conteúdo alvo) para depois serem
repassados para a equipe de &amp;ldquo;revisores&amp;rdquo;.&lt;/p>
&lt;p>Como se o fardo de ler inúmeras páginas por dia não fosse suficiente, a operação
dos leitores começa por volta das 4:30 da manhã quando a &amp;ldquo;primeira leitura&amp;rdquo;
inicia (ou seja, a entrega dos impressos matinais).&lt;/p>
&lt;h2 id="problema">Problema&lt;/h2>
&lt;p>Por mais de 20 anos este conteúdo foi ingerido pelos chamados &amp;ldquo;leitores&amp;rdquo;. Mas,
devido ao advento da internet e do boom da imprensa digital a partir do final
dos anos 90, e atualmente de mídias social as empresas estão transferindo seus
investimentos de clipping para o monitoramento de outras áreas. Exigindo
portanto uma ação do Clipping Service para se manter competitiva no mercado.&lt;/p>
&lt;p>Através da automação da leitura de notícias usando OCR, PLN e inteligência
artificial para categorizar as mídias, o plano era alcançar um maior rendimento
durante a ingestão, dando aos leitores mais tempo livre para revisar o conteúdo.
Consequentemente, alcançar uma maior qualidade no conteúdo, já que nós como
humanos não somos bons em fazer tarefas repetitivas, especialmente quando se
trata de ler páginas intermináveis em busca de nomes e palavras.&lt;/p>
&lt;h1 id="solução">Solução&lt;/h1>
&lt;h2 id="implementação-técnica">Implementação técnica&lt;/h2>
&lt;p>Após algum tempo de pesquisa e benchmarking das alternativas, decidimos usar
Python como linguagem de implementação para lidar com textos, OCR e PLN (usando
NLTK). Dada sua API estendida e bibliotecas para PLN e processamento de imagens.&lt;/p>
&lt;p>Como fornecedor de nuvem escolhemos AWS devido a sua estabilidade e consistência
em relação a outros fornecedores, a conclusão na época foi: AWS possui uma
estimativa de preço 14,67% maior do que a GCP. Entretanto, a popularidade da AWS
é maior do que a GCP, assim como é uma nuvem comprovada em termos de
estabilidade, suporte e integridade. Fazendo uma escolha mais segura por um
preço ligeiramente superior.&lt;/p>
&lt;p>A stack foi: Python 3 usando Dramatiq como biblioteca de processamento de
tarefas, executando OCR por meio do Tesseract, processando texto com NLTK e
imagens com Pillow (wrapper do ImageMagick). Redis foi o mensage broker do
Dramatiq, um banco relacional simples estava no Postgres armazenando métricas
referentes à execução e tínhamos também um Elasticsearch armazenando o conteúdo
processado.&lt;/p>
&lt;p>As solicitações vindas do data center chegaram a um Gateway API, responsável por
executar uma função Lambda, e entregar o resultado do conteúdo.&lt;/p>
&lt;p>A melhor parte do design? Armazenávamos e servimos o conteúdo por meio do AWS
S3. Cada peça foi desenhada com tolerância a falhas, e nós simplesmente
desligávamos toda a infra-estrutura em nuvem depois da operação, para ligar
apenas no dia seguinte.&lt;/p>
&lt;p>Operando apenas das 4h às 14h, um projeto &amp;ldquo;serverless&amp;rdquo; e efêmero com beneficiado
por uma redução de custos em núvem agressiva.&lt;/p>
&lt;h2 id="impacto-e-resultados">Impacto e resultados&lt;/h2>
&lt;p>A Clipping Service reduziu em ~78% o quadro de funcionários da equipe de
leitura, oferecendo contratação interna para outras áreas da empresa e um plano
de demissão voluntário com benefícios, tornando o processo o mais humano
possível para os funcionários que preferissem sair.&lt;/p>
&lt;p>Utilizando automação para tarefas de leitura, a Clipping Service pôde alcançar
melhorias consideráveis durante o processo de ingestão de mídia (cerca de 20
vezes mais rápido), oferecendo maior qualidade no serviço de clipping para seus
clientes e viu a oportunidade de criar posteriormente um serviço de auto-serviço
de recortes de imprensa, uma vez que o custo operacional diminuiu
significativamente.&lt;/p></description></item><item><title>Dotz Data Labs</title><link>https://macunha.me/pt/project/2017/dotz-data-labs/</link><pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate><guid>https://macunha.me/pt/project/2017/dotz-data-labs/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;h2 id="sumário">Sumário&lt;/h2>
&lt;p>Para poder inovar e se manter em um mercado em constante mudança e
evolução, Dotz passou por um processo de transformação digital e teve
a ajuda de alguns consultores neste percurso.&lt;/p>
&lt;p>Entre as iniciativas para se aproximar de um modelo digital, surgiu a
implementação de um Data Lake, com o requisito de ser
&lt;a href="https://en.wikipedia.org/wiki/Serverless_computing" target="_blank" rel="noopener">serverless&lt;/a> e
cloud-native, auxiliando no processo de tomada de decisão e encurtando
o time-to-market durante o lançamento de novos produtos.&lt;/p>
&lt;h2 id="problemática">Problemática&lt;/h2>
&lt;p>A Dotz é uma das maiores empresas no campo de programas de fidelidade no
Brasil, e enfrentaria um grande número de problemas com desconexão de
dados dificultando a análise do comportamento de seus usuários. Como
eles receberam dados de inúmeros supermercados e lojas, é difícil
agrupar os produtos, já que o nome é diferente dependendo da fonte. Para
ajudar nesta análise, eles decidiram construir um Data Lake.&lt;/p>
&lt;h1 id="solução">Solução&lt;/h1>
&lt;h2 id="implementação-técnica">Implementação Técnica&lt;/h2>
&lt;p>Construímos e implantamos uma arquitetura gerenciada da Big Data usando
a Plataforma Cloud do Google (GCP) para suportar esta estratégia e
permitir uma visão de 360 graus dos clientes (usuários com pontos a.k.a.
Dotz) e parceiros que oferecem o programa de fidelidade.&lt;/p>
&lt;p>O design foi focado em serviços gerênciados pela nuvem e serverless
oferecida pelo Google, servindo as principais competências de um Data
Lake como o armazenamento escalável usando o Google Cloud Storage, e o
Google BigQuery. Com parte do processo rodando containerizado em
Kubernetes, responsável pela limpeza de dados e gerenciar o ETL.&lt;/p>
&lt;p>Transmitimos dados com o Apache Beam rodando sob o Google DataFlow,
processamento em massa paralelo com Apache Spark jobs executados no
Google DataProc, análise exploratória com o Google DataLab, Machine
Learning Analysis com o Google ML e visualização de dados no Google Data
Studio.&lt;/p>
&lt;p>Os dados são transportados por meio de um modelo data-driven, onde os
dados foram planejados para streaming, including o ETL (que funciona em
um micro-batch, para permitir a exploração em near-real-time). Estes dados
passam pelo pipeline de dados utilizando o serviço de mensageria do Google
Pub/Sub, em que cada mensagem é serializada utilizando o formato Avro,
reduzindo a carga e permitindo que o transporte seja econômico, rápido e
confiável.&lt;/p>
&lt;h2 id="impacto-e-resultados">Impacto e resultados&lt;/h2>
&lt;p>Tudo isso permitiu à Dotz ter uma melhor estrutura em sua plataforma
analítica, previamente gerenciada em uma grande instância do MS SQL
Server, sendo deslocada para um Data Lake com camadas que permitem a
categorização, governança, qualidade e segurança dos dados.&lt;/p>
&lt;p>Suporte a processos analíticos de dados dos usuários, exploração ágil
e monetização de seus conhecimentos sobre o comportamento dos clientes.&lt;/p></description></item><item><title>Easynvest Data Platform</title><link>https://macunha.me/pt/project/2017/easynvest-data-platform/</link><pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/pt/project/2017/easynvest-data-platform/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;h2 id="sumário">Sumário&lt;/h2>
&lt;p>Dentro do planejamento anual da Easynvest, um investimento na expansão da
equipe de data &amp;amp; analytics teve como objetivo encurtar a tomada de decisão
e entregar maior qualidade aos clientes por meio de um processo operacional
com baixo custo.&lt;/p>
&lt;p>Dentre os principais objetivos deste projeto, tivemos a automatização de
análise de crédito na aprovação do cadastro de clientes utilizando Machine
Learning, um processo que até então era longo e manual, sendo realizado pelo
back office.&lt;/p>
&lt;p>Seguido de uma melhor oferta de produtos ao cliente, realizando a
categorização de acordo com o perfil de cada cliente, permitindo sugestões
mais atraentes de produtos, estando alinhadas com as preferências pessoais,
assim como de acordo com o perfil de cada investidor (conservador, moderado
ou agressivo).&lt;/p>
&lt;p>Por último, mas não menos importante, a detecção inteligente de lavagem de
dinheiro gerando relatórios para as autoridades responsáveis.&lt;/p>
&lt;h2 id="problemática">Problemática&lt;/h2>
&lt;p>Entretanto, houve limitações nas ferramentas de dados, principalmente devido ao
fato de seram softwares proprietários (com licenças limitadas) e projetados para
uso em data centers. Além disso, o banco de dados analítico foi modelado
para modelos tradicionais de Business Intelligence (OLAP, etc), tornando o
processo de tomada de decisão pesado, devido à quantidade exigente de interações
durante o ETL.&lt;/p>
&lt;p>Anteriormente para um cliente ser aprovado, o processo levava de 10 a 15 dias.
Para que todas as informações necessárias fossem coletadas, tendo uma perspectiva
completa do perfil, includindo análises de crédito. Após coletar as informações o
back office gerava uma pontuação na análise interna de crédito.&lt;/p>
&lt;p>Sendo que, na maioria dos casos, o cliente não era notificado sobre atualizações
referentes ao processo e não recebia feedback ao fim (caso recusado), a menos que
fosse explicitamente solicitado (contatando o suporte via chat ou e-mail, por
exemplo) o que tornava o processo demorado e custoso. Sem contar as inúmeras
quantidades de clientes perdidos para a concorrência durante esta longa espera.&lt;/p>
&lt;h1 id="solução">Solução&lt;/h1>
&lt;h2 id="implementação-técnica">Implementação Técnica&lt;/h2>
&lt;p>Para tornar isso possível, construímos uma implementação híbrida em nuvem usando AWS
componentes baseados em nuvem (principalmente AWS S3, EMR e ECS), para extender a
capacidade do data center, implementando um ecossistema Hadoop cloud-first
(substituindo os componentes de software proprietário por open-source equivalentes).
Dando à Easynvest a possibilidade de crescer seu Data Lake exponencialmente.&lt;/p>
&lt;p>O design do Data Lake foi robusto, visando lidar com a execução pesada de processos
analíticos através de modelos de Machine Learning, com suporte para data quality,
governança de metadados, segurança da informação e self-service de dados (os
proprietários dos dados poderiam compartilhar seus dados com consumidores de outras
áreas da empresa, permitindo o auto-serviço de seus dados analíticos).&lt;/p>
&lt;p>Um Chatbot também foi utilizado para reduzir a carga operacional no ambiente, sendo
responsável pela manutenção e atualização dos componentes de infra-estrutura.
Desde o acionamento de deployments até a geração de chaves de criptografia on-demand
para segurança e governança da plataforma de dados. Implementado com Errbot em Python
interagindo com Slack.&lt;/p>
&lt;p>Indo além, implementamos as melhores práticas de DevOps, tendo Jenkins como
ferramenta para CI/CD dos componentes desenvolvidos junto com Ansible para
Gerenciamento da Configuração.&lt;/p>
&lt;h2 id="impacto-e-resultados">Impacto e resultados&lt;/h2>
&lt;p>Graças à utilização de camadas no Data Lake e a implementação de pipelines de dados,
conseguimos reduzir o tempo de ingestão dos dados em 78%, incluindo metadados e
catálogo de dados, além de automatizar grande parte do trabalho que antes era feito
manualmente durante a ingestão.&lt;/p>
&lt;p>Assim, trazendo resultados positivos, principalmente durante a aprovação de novos
registros de usuários, de ~10 dias para aproximadamente 1 dia. Tornando também
a plataforma de dados mais democrática, fornecendo informações relevantes que
facilita a análise de outras áreas da companhia como risco (análise de crédito) e
suporte (atendimento), sem ter que abrir mão da segurança.&lt;/p></description></item><item><title>Movida Rent A DevOps</title><link>https://macunha.me/pt/project/2016/movida-rent-a-devops/</link><pubDate>Tue, 07 Feb 2017 00:00:00 +0000</pubDate><guid>https://macunha.me/pt/project/2016/movida-rent-a-devops/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;h2 id="sumário">Sumário&lt;/h2>
&lt;p>JSL Holdings Ltda, detentora da Julio Simões Logistica (maior player LATAM
de logística) comprou a Movida Rent a Car em 2013 para expandir o portfólio
e abrir novas oportunidades no mercado de locação e venda de veículos mercados.&lt;/p>
&lt;p>A JSL investiu em torno de R$ 1,8 bilhão na Movida, e multiplicou a receita
em 21 vezes de R$ 58m para R$ 1,2b, em 3 anos. Com base nestes resultados
de sucesso, a JSL Holdings Ltd planejou um IPO para a Movida.&lt;/p>
&lt;h2 id="problemática">Problemática&lt;/h2>
&lt;p>Para ser negociada em bolsa, a Movida teria que passar por uma auditoria.
Porém a solução de software não estava de acordo com algumas normas de
segurança.&lt;/p>
&lt;p>O projeto começou em Dezembro de 2016, planejando a implementação de um
processo de automátizado de publicação de software adotando DevOps em seu
data center. Com os seguintes objetivos:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>segurança&lt;/strong>; nenhuma pessoa deveria acessar os servidores Linux. e&lt;/li>
&lt;li>&lt;strong>produtividade&lt;/strong>; liberando recursos mais rapidamente para encurtar o
time-to-market de novas features.&lt;/li>
&lt;/ol>
&lt;h1 id="solução">Solução&lt;/h1>
&lt;h2 id="implementação-técnica">Implementação Técnica&lt;/h2>
&lt;p>Nosso primeiro objetivo era implementar o pipeline suportando CI/CD utilizando
Jenkins, responsável por empacotar novas funcionalidades, criar um deployment
e implementá-lo no data center. Além do deployment em produção, o
pipeline também apoiou a criação de ambientes &lt;em>on-demand&lt;/em> para homologação de
recursos e coleta de feedback dos usuários.&lt;/p>
&lt;p>Para ter um ciclo de aprovação mais rápido e controlado, migramos o servidor
Git para dentro do data center. Através desta ação nós reduziu em 5 minutos
(62%) do tempo total de deployment e aumentamos o controle sobre os acessos
em seus repositórios.&lt;/p>
&lt;p>A implementação de CI/CD utilizou Jenkins Pipeline para CI/CD, GitLab com
autenticação LDAP, e Ansible como Configuration Manager. Um deployment
completo leva cerca de 2 minutos desde o &lt;code>git push&lt;/code> até o código estar em
produção.&lt;/p>
&lt;p>Além da implantação de um pipeline para CI/CD, também trabalhamos em uma
estratégia de self-service para a execução de jobs sem acesso direto por SSH
aos servidores. Rundeck entrou em cena, com configurações de RBAC e
visibilidade sobre o histórico de jobs executados.&lt;/p>
&lt;h2 id="impacto-e-resultados">Impacto e resultados&lt;/h2>
&lt;p>Movida foi auditada em Janeiro 2017, no final de Janeiro de 2017 eles
receberam a aprovação.&lt;/p>
&lt;p>Duas semanas depois, em fevereiro de 2017, a Movida lançou seu IPO,
marcado como o primeiro IPO brasileiro de 2017. Abrindo seu capital
no dia 8 de Fevereiro de 2017, levantando cerca de R$ 645M.&lt;/p></description></item><item><title>Nextel Digital Release</title><link>https://macunha.me/pt/project/2016/nextel-digital-release/</link><pubDate>Wed, 30 Nov 2016 00:00:00 +0000</pubDate><guid>https://macunha.me/pt/project/2016/nextel-digital-release/</guid><description>&lt;h1 id="introdução">Introdução&lt;/h1>
&lt;h2 id="sumário">Sumário&lt;/h2>
&lt;p>A Nextel planejou desenvolver uma aplicação móvel para reduzir seus
custos operacionais com call centers e reduzir o &lt;em>contact rate&lt;/em>.
&amp;ldquo;Nextel Digital&amp;rdquo; foi o nome dado ao projeto responsável por lançar
esta aplicação.&lt;/p>
&lt;p>Com o tempo, o projeto &amp;ldquo;Nextel Digital&amp;rdquo; absorveu mais objetivos, como
melhorar a user experience, e se tornou um novo produto chamado &amp;ldquo;Happy&amp;rdquo;,
uma operadora de telefonia digital. Nextel Happy permite que os usuários
gerenciem seus planos e dados completamente pelo aplicativo, desde a
ativação do seu SIM até a gestão de plano familiar.&lt;/p>
&lt;p>Este projeto ajudou a Nextel a aumentar sua base de clientes, melhorando
a experiência dos usuários, e diminuir os custos operacionais (em 16%).&lt;/p>
&lt;h2 id="problemática">Problemática&lt;/h2>
&lt;p>A equipe executiva da Nextel Brasil decidiu trabalhar com terceirização
no desenvolvimento deste produto para absorver conhecimento das empresas
e para complementar suas capacidades internas. Assim como trazer
diferentes perspectivas em jogo, melhorando o processo criativo.&lt;/p>
&lt;p>Nossa equipe assumiu a responsabilidade de arquitetar e implementar a
infraestrutura de núvem garantindo alta disponibilidade, resiliência e
consistência do software.&lt;/p>
&lt;p>Assim como assumimos a responsabilidade de sincronizar os dados entre o
data center da Nextel e a nuvem. Movendo com segurança uma quantia grande
em GB de dados relacionados à consumo para a núvem diariamente, sem perda
ou duplicação dos dados.&lt;/p>
&lt;h1 id="solução">Solução&lt;/h1>
&lt;h2 id="implementação-técnica">Implementação técnica&lt;/h2>
&lt;p>k
Escolhemos o GlusterFS para garantir a consistência, instalado entre o
data center da Nextel e a AWS, sincronizando os dados de usuários
(ex.: consumo do plano de dados, minutos de chamada).
A equipe de operações da Nextel IT alimentava o GlusterFS diretamente
com os dados de torres telefônicas, permitindo processamento em
near-real-time.&lt;/p>
&lt;p>Assim que os dados estavam disponíveis no volume montado em instâncias
na AWS, a implementação em Celery entra em jogo. No centro da arquitetura,
o Celery (implementado em Python 3) usando Redis como message broker,
executa jobs assíncronos para inspecionar eventos no GlusterFS.&lt;/p>
&lt;p>Uma vez que o Celery detecta um novo arquivo disponível ele analisa o
conteúdo e inicia um multipart upload para o AWS S3, em seguida compara
os checksums para garantir a consistência (e retenta em caso de
inconsistência).&lt;/p>
&lt;p>Após chegar ao AWS S3, o evento do objeto aciona uma função AWS Lambda
para analisar o conteúdo e indexá-lo no Elasticsearch, sendo
posteriormente servido aos clientes através de uma API REST.&lt;/p>
&lt;p>Toda o design da infrastrutura foi planejada para
&lt;a href="https://www.oreilly.com/radar/an-introduction-to-immutable-infrastructure/" target="_blank" rel="noopener">ser imutável&lt;/a>,
facilitando a evolução e confiabilidade, tendo Ansible como Configuration
Manager e AWS CloudFormation como provisionador na nuvem. Em apenas
alguns minutos é possível recriar todo o ecossistema, com esforço mínimo.&lt;/p>
&lt;h2 id="impacto-e-resultados">Impacto e resultados&lt;/h2>
&lt;p>Todo o processo de disponibilização dos dados de uma torre celular que
tomava em torno de 1 dia foi reduzido para 5 minutos.
Como consequência, o tempo de duração das chamadas nos call center caiu
~56%, devido à alternativa de self-service fornecida no aplicativo.&lt;/p>
&lt;p>Além disso, os usuários podem gerenciar seu histórico de chamadas e
planejar o consumo diretamente pelo celular, com atualizações em
near-real-time. Proporcionano um feedback consistente e interativo.&lt;/p></description></item></channel></rss>