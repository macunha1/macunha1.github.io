<!doctype html><html lang=pt-br><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Matheus Cunha"><meta name=description content="Utilizando Apache Spark Operator no Kubernetes para agilizar seus workflows de Big Data com abordagem cloud-native sem depender de um Hadoop cluster."><link rel=alternate hreflang=en href=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/><link rel=alternate hreflang=pt-br href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-dark disabled><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=https://macunha.me/css/academic.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-143794949-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-143794949-1',{'anonymize_ip':true});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=https://macunha.me/pt/index.webmanifest><link rel=icon type=image/png href=https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@_macunha1"><meta property="twitter:creator" content="@_macunha1"><meta property="og:site_name" content="It's me, Macunha!"><meta property="og:url" content="https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/"><meta property="og:title" content="Quickstart: Apache Spark no Kubernetes | It's me, Macunha!"><meta property="og:description" content="Utilizando Apache Spark Operator no Kubernetes para agilizar seus workflows de Big Data com abordagem cloud-native sem depender de um Hadoop cluster."><meta property="og:image" content="https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured.png"><meta property="twitter:image" content="https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured.png"><meta property="og:locale" content="pt-br"><meta property="article:published_time" content="2020-05-21T23:00:57+02:00"><meta property="article:modified_time" content="2020-06-18T07:19:15+02:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/"},"headline":"Quickstart: Apache Spark no Kubernetes","image":["https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured.png"],"datePublished":"2020-05-21T23:00:57+02:00","dateModified":"2020-06-18T07:19:15+02:00","author":{"@type":"Person","name":"Matheus Cunha"},"publisher":{"@type":"Organization","name":"It's me, Macunha!","logo":{"@type":"ImageObject","url":"https://macunha.me/images/icon_hu176de0364afaeda8922c372b574c3cbf_6946_192x192_fill_lanczos_center_2.png"}},"description":"Utilizando Apache Spark Operator no Kubernetes para agilizar seus workflows de Big Data com abordagem cloud-native sem depender de um Hadoop cluster."}</script><script src=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({"palette":{"popup":{"background":"#2962ff","text":"#fff"},"button":{"background":"#fff","text":"#2962ff"}},"theme":"classic","content":{"message":"Este site contém cookies para garantir que você tenha a melhor experência.","dismiss":"Entendi!","link":"Saiba mais","href":"https://www.cookiesandyou.com"}})});</script><title>Quickstart: Apache Spark no Kubernetes | It's me, Macunha!</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Pesquisar</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Pesquisar... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=https://macunha.me/pt/>It's me, Macunha!</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Alterar navegação">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=https://macunha.me/pt/>It's me, Macunha!</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=https://macunha.me/pt/project/><span>Projetos</span></a></li><li class=nav-item><a class="nav-link active" href=https://macunha.me/pt/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=https://macunha.me/pt/author/matheus-cunha/><span>Sobre</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Pesquisar><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">Português</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>Português</span></div><a class=dropdown-item href=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/><span>English</span></a></div></li></ul></div></nav><article class=article><div class="article-container pt-3"><h1>Quickstart: Apache Spark no Kubernetes</h1><p class=page-subtitle>Utilizando Apache Spark Operator no Kubernetes</p><div class=article-metadata><span class=article-date>Última atualização em
18/06/2020</span>
<span class=middot-divider></span><span class=article-reading-time>6 minutos de leitura</span>
<span class=middot-divider></span><a href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/#disqus_thread></a><span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=https://macunha.me/pt/category/tutorials/>Tutorials</a></span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:480px><div style=position:relative><img src=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured_hu41e40b398e8bb52ece3d3b259cd36487_2617045_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><div class=article-style><h1 id=introduction>Introduction</h1><h2 id=apache-spark-operator-para-kubernetes>Apache Spark Operator para Kubernetes</h2><p>Desde o seu lançamento em 2014 pela Google, o Kubernetes tem ganhado
muita popularidade junto com o próprio Docker e desde 2016 passou a
ser o <em>de facto Container orchestrator</em>, estabelecido como um padrão
de mercado. Possuindo versões gerenciadas em <strong>todas</strong> as <em>major
Clouds</em>
<a href=https://cloud.google.com/kubernetes-engine/ target=_blank rel=noopener>[1]</a>
<a href=https://aws.amazon.com/eks/ target=_blank rel=noopener>[2]</a>
<a href=https://docs.microsoft.com/en-us/azure/aks/ target=_blank rel=noopener>[3]</a> (inclusive
<a href=https://www.digitalocean.com/products/kubernetes/ target=_blank rel=noopener>Digital Ocean</a> e
<a href=https://www.alibabacloud.com/product/kubernetes target=_blank rel=noopener>Alibaba</a>).</p><p>Toda essa popularidade tem atraído novas implementações e <em>use-cases</em>
para o orquestrador, dentre eles a execução de
<a href=https://kubernetes.io/docs/tutorials/stateful-application/ target=_blank rel=noopener>Stateful
applications</a>
incluindo
<a href=https://vitess.io/zh/docs/get-started/kubernetes/ target=_blank rel=noopener>bancos de dados em containers</a>.</p><p>Qual seria a necessidade de ter um banco de dados orquestrado? Ótima
pergunta. Por hoje, vamos focar na utilização do
<a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md target=_blank rel=noopener>Spark Operator</a>
para executar <strong>Spark jobs</strong> em Kubernetes.</p><p>A idéia do Spark Operator
<a href=https://github.com/kubernetes/kubernetes/issues/34377 target=_blank rel=noopener>surgiu</a>
em 2016, antes disso haviam apenas alguns <em>jeitinhos</em>, por exemplo:
<a href=https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/ target=_blank rel=noopener>com Apache Zeppelin</a>
dentro do Kubernetes, ou então, mais refinado ainda
<a href=https://github.com/kubernetes/examples/tree/master/staging/spark target=_blank rel=noopener>criando o seu
próprio Apache Spark cluster dentro do Kubernetes (exemplo do
repositório oficial do Kubernetes)</a>
que usaria o
<a href=http://spark.apache.org/docs/latest/spark-standalone.html target=_blank rel=noopener>Spark Standalone mode</a>.</p><p>Porém, executar nativamente seria muito mais interessante pois poderia
aproveitar o
<a href=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ target=_blank rel=noopener>Kubernetes Scheduler</a>
para ações relacionadas à alocação dos recursos no cluster, dando mais
elasticidade e uma interface mais simples para gerenciar os workloads
no Apache Spark.</p><p>Considernando esses pontos o
<a href=https://issues.apache.org/jira/browse/SPARK-18278 target=_blank rel=noopener>desenvolvimento do Apache Spark Operator
ganhou atenção</a>,
foi <em>mergeado</em> e publicado na
<a href=https://spark.apache.org/releases/spark-release-2-3-0.html target=_blank rel=noopener>versão do Apache Spark 2.3.0</a>
em
<a href=https://spark.apache.org/news/index.html target=_blank rel=noopener>Fevereiro de 2018</a>.</p><p>Se você estiver interessado em ler mais sobre a proposta do Apache
Spark Operator, existe um
<a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target=_blank rel=noopener>design document publicado no Google Docs.</a></p><h2 id=por-que-kubernetes>Por que Kubernetes?</h2><p>Como atualmente as empresas estão buscando se
<a href=https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html target=_blank rel=noopener>reinventar por meio da
tão falada transformação
digital</a>
para que possam ter competitividade e, principalmente, sobreviver
diante de um mercado cada vez mais dinâmico, é comum ver abordagens
que incluam Big Data, Inteligência Artificial e Cloud Computing
<a href=https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/ target=_blank rel=noopener>[1]</a>
<a href=https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/ target=_blank rel=noopener>[2]</a>
<a href=https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/ target=_blank rel=noopener>[3]</a>.</p><p>Para compreender os benefícios de utilizar Cloud ao invés de On-premises no
contexto de Big Data vale a pena ler o artigo
<a href=https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html target=_blank rel=noopener>da Databricks</a>,
que é a empresa
<a href=https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/ target=_blank rel=noopener>fundada pelos criadores do Apache
Spark</a>.</p><p>Como nós vemos uma adoção de Cloud Computing generalizada (até por
empresas que teriam condições de bancar o próprio hardware), também
podemos notar que na maiorira dessas implementações de Cloud não
existem clusters de
<a href=https://hadoop.apache.org/ target=_blank rel=noopener>Apache Hadoop</a>
já que os times de Dados (BI/Data Science/Analytics) optam cada vez
mais por utilizar ferramentas como
<a href=https://cloud.google.com/bigquery/ target=_blank rel=noopener>Google BigQuery</a>
ou
<a href=https://aws.amazon.com/redshift/ target=_blank rel=noopener>AWS Redshift</a>. Portanto, não faz sentido
subir um Hadoop apenas para utilizar o
<a href=https://hortonworks.com/apache/yarn/ target=_blank rel=noopener>YARN</a>
como gerenciador os recursos.</p><p>Uma alternativa é a utilização de provisionadores de clusters Hadoop
como o
<a href=https://cloud.google.com/dataproc target=_blank rel=noopener>Google DataProc</a> ou o
<a href=https://aws.amazon.com/emr/ target=_blank rel=noopener>AWS
EMR</a> para a criação de clusters efêmeros.
Apenas para nomear algumas opções.</p><p>Para entender melhor o design do Spark Operator, recomendo a leitura
da documentação gerada pela equipe da
<a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#the-crd-controller target=_blank rel=noopener>GCP no
GitHub</a>.</p><h1 id=hora-de-meter-a-mão-na-massa>Hora de meter a mão na massa!</h1><h2 id=aquecendo-o-motor>Aquecendo o motor</h2><p>Agora que toda a palavra já foi passada, vamos ao hands-on para
mostrar a coisa acontecendo. Para isso, vamos usar:</p><ul><li><a href=https://www.docker.com/ target=_blank rel=noopener>Docker</a> como motor de container no
Kubernetes, e construção da imagem
<a href=https://docs.docker.com/install/ target=_blank rel=noopener>(link para
instalação)</a>;</li><li>Minikube
<a href=https://kubernetes.io/docs/tasks/tools/install-minikube/ target=_blank rel=noopener>(link para instalação)</a>
para facilitar o provisionamento do Kubernetes (sim, será uma
execução local);</li><li>Para interagir com a API do Kubernetes é preciso ter o <code>kubectl</code>
instalado.
<a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/ target=_blank rel=noopener>Se você não tiver, siga as instruções aqui</a>.</li><li>uma versão compilada do Apache Spark que seja maior do que a
2.3.0.<ol><li>você pode tanto compilar
<a href=https://github.com/apache/spark target=_blank rel=noopener>do código fonte</a>,
que vai tomar <em>algumas horas</em> até terminar, quanto</li><li>fazer o download de uma versão compilada
<a href=https://spark.apache.org/downloads.html target=_blank rel=noopener>aqui</a>
(recomendado).</li></ol></li></ul><p>Assim que o Apache estiver descompactado, vamos adicionar o
caminho no PATH para facilitar a execução:</p><pre><code class=language-bash>export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin
</code></pre><h2 id=criando-o-cluster-com-minikube>Criando o &ldquo;cluster&rdquo; com Minikube</h2><p>Agora, para ter um Kubernetes vamos iniciar um <code>minikube</code> com o
propósito de rodar um dos exemplos disponíveis no
<a href=https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala target=_blank rel=noopener>repositório do Spark</a>
chamado <code>SparkPi</code> apenas para demonstração.</p><pre><code class=language-bash>minikube start --cpus=2 \
    --memory=4g
</code></pre><h2 id=buildando-a-imagem-docker>Buildando a imagem Docker</h2><p>Vamos utilizar o Docker daemon do Minikube para não depender de um
registry externo (e só gerar lixo na VM, facilitando a limpeza
depois). Para isso, o minikube tem um wrapper que facilita a nossa
vida:</p><pre><code class=language-bash>eval $(minikube docker-env)
</code></pre><p>Após ter configurado as variáveis de ambiente para o Docker daemon,
vamos precisar de uma imagem Docker para executar os jobs.
Existe um
<a href=https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh target=_blank rel=noopener>shell script no repositório do Spark</a>
para ajudar com isso. Considerando que o <code>PATH</code> está configurado
corretamente, basta executar:</p><pre><code class=language-bash>docker-image-tool.sh -m -t latest build
</code></pre><p><em>Obs.:</em> O parâmetro <code>-m</code> aqui indica que é um build para o minikube.</p><p>Vamos pegar a via expressa para executar o SparkPi, usando o mesmo
comando que seria utilizado para um cluster Spark
<a href=https://spark.apache.org/docs/latest/submitting-applications.html target=_blank rel=noopener>spark-submit</a>.</p><p>Porém, o Spark Operator dá suporte a definição de jobs no &ldquo;dialeto do
Kubernetes&rdquo; usando
<a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ target=_blank rel=noopener>CRD</a>,
<a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples target=_blank rel=noopener>aqui tem alguns exemplos</a> - para depois.</p><h2 id=fire-in-the-hole>Fire in the hole!</h2><p>Cuidado com o vão entre a versão do Scala e a plataforma quando
estiver parametrizando o job:</p><pre><code class=language-bash>spark-submit --master k8s://https://$(minikube ip):8443 \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=2 \
    --executor-memory 1024m \
    --conf spark.kubernetes.container.image=spark:latest \
    local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # aqui
</code></pre><p>O que temos de novidade é:</p><ul><li><code>--master</code>: Aceita o prefixo <code>k8s://</code> na URL, para o endpoint da
API do Kubernetes, exposta pelo commando <code>https://$(minikube ip):8443</code>.
Aliás, se estiver interessado, isso é um
<a href=https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html target=_blank rel=noopener>command substitution no
shell</a>;</li><li><code>--conf spark.kubernetes.container.image=</code>: Configuração para a
imagem Docker que será executada no Kubernetes.</li></ul><p>Com o output:</p><pre><code>...

19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]

19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
</code></pre><p>Para ver o resultado do job (e toda a execução), podemos mandar um
<code>kubectl logs</code> passando o nome do pod do driver como parâmetro:</p><pre><code class=language-bash>kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')
</code></pre><p>Que traz o output (algumas entradas foram omitidas), parecido com:</p><pre><code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
</code></pre><p>O resultado aparece no stdout:</p><pre><code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
</code></pre><p>Para finalizar, vamos deletar a VM que o Minikube gera, para limpar o
ambiente (a menos que você queira continuar brincando com ele):</p><pre><code class=language-bash>minikube delete
</code></pre><h2 id=últimas-palavras>Últimas palavras</h2><p>Espero ter dispertado bastante curiosidade e algumas ideias para
ir além no desenvolvimento dos seus workloads de Big Data.
Se tiver alguma dúvida ou sugestão, não deixe de postar na seção de
comentários.</p></div><div class=article-tags><a class="badge badge-light" href=https://macunha.me/pt/tag/data-engineering/>data-engineering</a>
<a class="badge badge-light" href=https://macunha.me/pt/tag/kubernetes/>kubernetes</a></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=https://macunha.me/pt/author/matheus-cunha/avatar_hu538abb559021e5e818cc3fcb6b905317_34815_270x270_fill_q90_lanczos_center.jpg alt="Matheus Cunha"><div class=media-body><h5 class=card-title><a href=https://macunha.me>Matheus Cunha</a></h5><h6 class=card-subtitle>Engenheiro de Sistemas e Mágico</h6><p class=card-text>Apenas um amante de tecnologia empoderando empresas com computação &ldquo;high-tech&rdquo; para ajudar na inovação (:</p><ul class=network-icon aria-hidden=true><li><a href=https://github.com/macunha1/ target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://gitlab.com/macunha/ target=_blank rel=noopener><i class="fab fa-gitlab"></i></a></li><li><a href=https://twitter.com/_macunha1 target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.linkedin.com/in/macunha/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://medium.com/@macunha1 target=_blank rel=noopener><i class="fab fa-medium"></i></a></li></ul></div></div><section id=comments><div id=disqus_thread></div><script>let disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='https://'+"macunha1-me"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/pt/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"Sem resultados","placeholder":"Pesquisar...","results":"Resultados encontrados"};const content_type={'post':"Posts",'project':"Projetos",'publication':"Publicações",'talk':"Palestras",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script id=dsq-count-scr src=https://macunha1-me.disqus.com/count.js async></script><script src=https://macunha.me/js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-sa/4.0 rel="noopener noreferrer" target=_blank><img src=https://search.creativecommons.org/static/img/cc_icon.svg alt="CC icon">
<img src=https://search.creativecommons.org/static/img/cc-by_icon.svg alt="CC by icon">
<img src=https://search.creativecommons.org/static/img/cc-nc_icon.svg alt="CC NC icon">
<img src=https://search.creativecommons.org/static/img/cc-nd_icon.svg alt="CC ND icon">
<img src=https://search.creativecommons.org/static/img/cc-sa_icon.svg alt="CC SA icon"></a></p><p class=powered-by>Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Citação</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copiar</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>