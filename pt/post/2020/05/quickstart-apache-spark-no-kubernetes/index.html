<!doctype html><html lang=pt-br><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=author content="Matheus Cunha"><meta name=description content="Utilizando Apache Spark Operator no Kubernetes para agilizar seus workflows de Big Data com abordagem cloud-native sem depender de um Hadoop cluster."><link rel=alternate hreflang=en href=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/><link rel=alternate hreflang=pt-br href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://macunha.me/css/vendor-bundle.min.1e75974c6945afa9e52fa8182894548d.css media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><link rel=stylesheet href=https://macunha.me/css/wowchemy.26d8385638dee6b08cdbedb72412d0f6.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-143794949-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(a,b){gtag('event','click',{event_category:'outbound',event_label:a,transport_type:'beacon',event_callback:function(){b!=='_blank'&&(document.location=a)}}),console.debug("Outbound link clicked: "+a)}function onClickCallback(a){if(a.target.tagName!=='A'||a.target.host===window.location.host)return;trackOutboundLink(a.target,a.target.getAttribute('target'))}gtag('js',new Date),gtag('config','UA-143794949-1',{anonymize_ip:!0}),gtag('set',{cookie_flags:'SameSite=None;Secure'}),document.addEventListener('click',onClickCallback,!1)</script><link rel=manifest href=https://macunha.me/pt/index.webmanifest><link rel=icon type=image/png href=https://macunha.me/media/icon_hu176de0364afaeda8922c372b574c3cbf_6946_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=https://macunha.me/media/icon_hu176de0364afaeda8922c372b574c3cbf_6946_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="It's me, Macunha!"><meta property="og:url" content="https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/"><meta property="og:title" content="Quickstart: Apache Spark no Kubernetes | It's me, Macunha!"><meta property="og:description" content="Utilizando Apache Spark Operator no Kubernetes para agilizar seus workflows de Big Data com abordagem cloud-native sem depender de um Hadoop cluster."><meta property="og:image" content="https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured.png"><meta property="twitter:image" content="https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured.png"><meta property="og:locale" content="pt-br"><meta property="article:published_time" content="2020-05-21T23:00:57+02:00"><meta property="article:modified_time" content="2020-06-18T07:19:15+02:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/"},"headline":"Quickstart: Apache Spark no Kubernetes","image":["https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured.png"],"datePublished":"2020-05-21T23:00:57+02:00","dateModified":"2020-06-18T07:19:15+02:00","author":{"@type":"Person","name":"Matheus Cunha"},"publisher":{"@type":"Organization","name":"It's me, Macunha!","logo":{"@type":"ImageObject","url":"https://macunha.me/media/icon_hu176de0364afaeda8922c372b574c3cbf_6946_192x192_fill_lanczos_center_2.png"}},"description":"Utilizando Apache Spark Operator no Kubernetes para agilizar seus workflows de Big Data com abordagem cloud-native sem depender de um Hadoop cluster."}</script><script src=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({palette:{popup:{background:"#1565c0",text:"rgb(255, 255, 255)"},button:{background:"rgb(255, 255, 255)",text:"#1565c0"}},theme:"classic",content:{message:"Este site contém cookies para garantir que você tenha a melhor experência.",dismiss:"Entendi!",link:"Saiba mais",href:"https://www.cookiesandyou.com"}})})</script><title>Quickstart: Apache Spark no Kubernetes | It's me, Macunha!</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=359ac010e6e64574925f2121741a0cf4><script src=https://macunha.me/js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Pesquisar</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Pesquisar... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Pesquisar...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=https://macunha.me/pt/>It's me, Macunha!</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Alterar navegação">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=https://macunha.me/pt/>It's me, Macunha!</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=https://macunha.me/pt/project/><span>Projetos</span></a></li><li class=nav-item><a class="nav-link active" href=https://macunha.me/pt/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=https://macunha.me/pt/author/matheus-cunha/><span>Sobre</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Pesquisar><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-label=Languages><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">Português</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>Português</span></div><a class=dropdown-item href=https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/><span>English</span></a></div></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Quickstart: Apache Spark no Kubernetes</h1><p class=page-subtitle>Utilizando Apache Spark Operator no Kubernetes</p><div class=article-metadata><span class=article-date>Última atualização em
18/06/2020</span>
<span class=middot-divider></span><span class=article-reading-time>6 minutos de leitura</span>
<span class=middot-divider></span><a href=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/#disqus_thread></a><span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=https://macunha.me/pt/category/tutorials/>Tutorials</a></span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:480px><div style=position:relative><img src=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/featured_hu41e40b398e8bb52ece3d3b259cd36487_2617045_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><div class=article-style><h1 id=introduction>Introduction</h1><h2 id=apache-spark-operator-para-kubernetes>Apache Spark Operator para Kubernetes</h2><p>Desde o seu lançamento em 2014 pela Google, o Kubernetes tem ganhado
muita popularidade junto com o próprio Docker e desde 2016 passou a
ser o <em>de facto Container orchestrator</em>, estabelecido como um padrão
de mercado. Possuindo versões gerenciadas em <strong>todas</strong> as <em>major
Clouds</em><a href=https://cloud.google.com/kubernetes-engine/ target=_blank rel=noopener>[1]</a>
<a href=https://aws.amazon.com/eks/ target=_blank rel=noopener>[2]</a>
<a href=https://docs.microsoft.com/en-us/azure/aks/ target=_blank rel=noopener>[3]</a> (inclusive
<a href=https://www.digitalocean.com/products/kubernetes/ target=_blank rel=noopener>Digital Ocean</a> e
<a href=https://www.alibabacloud.com/product/kubernetes target=_blank rel=noopener>Alibaba</a>).</p><p>Toda essa popularidade tem atraído novas implementações e <em>use-cases</em>
para o orquestrador, dentre eles a execução de <a href=https://kubernetes.io/docs/tutorials/stateful-application/ target=_blank rel=noopener>Stateful
applications</a>
incluindo <a href=https://vitess.io/zh/docs/get-started/kubernetes/ target=_blank rel=noopener>bancos de dados em containers</a>.</p><p>Qual seria a necessidade de ter um banco de dados orquestrado? Ótima
pergunta. Por hoje, vamos focar na utilização do <a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md target=_blank rel=noopener>Spark Operator</a>
para executar <strong>Spark jobs</strong> em Kubernetes.</p><p>A idéia do Spark Operator <a href=https://github.com/kubernetes/kubernetes/issues/34377 target=_blank rel=noopener>surgiu</a>
em 2016, antes disso haviam apenas alguns <em>jeitinhos</em>, por exemplo:
<a href=https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/ target=_blank rel=noopener>com Apache Zeppelin</a>
dentro do Kubernetes, ou então, mais refinado ainda <a href=https://github.com/kubernetes/examples/tree/master/staging/spark target=_blank rel=noopener>criando o seu
próprio Apache Spark cluster dentro do Kubernetes (exemplo do
repositório oficial do Kubernetes)</a>
que usaria o <a href=http://spark.apache.org/docs/latest/spark-standalone.html target=_blank rel=noopener>Spark Standalone mode</a>.</p><p>Porém, executar nativamente seria muito mais interessante pois poderia
aproveitar o <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ target=_blank rel=noopener>Kubernetes Scheduler</a>
para ações relacionadas à alocação dos recursos no cluster, dando mais
elasticidade e uma interface mais simples para gerenciar os workloads
no Apache Spark.</p><p>Considernando esses pontos o <a href=https://issues.apache.org/jira/browse/SPARK-18278 target=_blank rel=noopener>desenvolvimento do Apache Spark Operator
ganhou atenção</a>,
foi <em>mergeado</em> e publicado na <a href=https://spark.apache.org/releases/spark-release-2-3-0.html target=_blank rel=noopener>versão do Apache Spark 2.3.0</a>
em <a href=https://spark.apache.org/news/index.html target=_blank rel=noopener>Fevereiro de 2018</a>.</p><p>Se você estiver interessado em ler mais sobre a proposta do Apache
Spark Operator, existe um <a href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" target=_blank rel=noopener>design document publicado no Google Docs.</a></p><h2 id=por-que-kubernetes>Por que Kubernetes?</h2><p>Como atualmente as empresas estão buscando se <a href=https://www.cio.com/article/3211428/what-is-digital-transformation-a-necessary-disruption.html target=_blank rel=noopener>reinventar por meio da
tão falada transformação
digital</a>
para que possam ter competitividade e, principalmente, sobreviver
diante de um mercado cada vez mais dinâmico, é comum ver abordagens
que incluam Big Data, Inteligência Artificial e Cloud Computing
<a href=https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/ target=_blank rel=noopener>[1]</a>
<a href=https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/ target=_blank rel=noopener>[2]</a>
<a href=https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/ target=_blank rel=noopener>[3]</a>.</p><p>Para compreender os benefícios de utilizar Cloud ao invés de On-premises no
contexto de Big Data vale a pena ler o artigo <a href=https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html target=_blank rel=noopener>da Databricks</a>,
que é a empresa <a href=https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/ target=_blank rel=noopener>fundada pelos criadores do Apache
Spark</a>.</p><p>Como nós vemos uma adoção de Cloud Computing generalizada (até por
empresas que teriam condições de bancar o próprio hardware), também
podemos notar que na maiorira dessas implementações de Cloud não
existem clusters de <a href=https://hadoop.apache.org/ target=_blank rel=noopener>Apache Hadoop</a>
já que os times de Dados (BI/Data Science/Analytics) optam cada vez
mais por utilizar ferramentas como <a href=https://cloud.google.com/bigquery/ target=_blank rel=noopener>Google BigQuery</a>
ou <a href=https://aws.amazon.com/redshift/ target=_blank rel=noopener>AWS Redshift</a>. Portanto, não faz sentido
subir um Hadoop apenas para utilizar o <a href=https://hortonworks.com/apache/yarn/ target=_blank rel=noopener>YARN</a>
como gerenciador os recursos.</p><p>Uma alternativa é a utilização de provisionadores de clusters Hadoop
como o <a href=https://cloud.google.com/dataproc target=_blank rel=noopener>Google DataProc</a> ou o <a href=https://aws.amazon.com/emr/ target=_blank rel=noopener>AWS
EMR</a> para a criação de clusters efêmeros.
Apenas para nomear algumas opções.</p><p>Para entender melhor o design do Spark Operator, recomendo a leitura
da documentação gerada pela equipe da <a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#the-crd-controller target=_blank rel=noopener>GCP no
GitHub</a>.</p><h1 id=hora-de-meter-a-mão-na-massa>Hora de meter a mão na massa!</h1><h2 id=aquecendo-o-motor>Aquecendo o motor</h2><p>Agora que toda a palavra já foi passada, vamos ao hands-on para
mostrar a coisa acontecendo. Para isso, vamos usar:</p><ul><li><a href=https://www.docker.com/ target=_blank rel=noopener>Docker</a> como motor de container no
Kubernetes, e construção da imagem <a href=https://docs.docker.com/install/ target=_blank rel=noopener>(link para
instalação)</a>;</li><li>Minikube <a href=https://kubernetes.io/docs/tasks/tools/install-minikube/ target=_blank rel=noopener>(link para instalação)</a>
para facilitar o provisionamento do Kubernetes (sim, será uma
execução local);</li><li>Para interagir com a API do Kubernetes é preciso ter o <code>kubectl</code>
instalado. <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/ target=_blank rel=noopener>Se você não tiver, siga as instruções aqui</a>.</li><li>uma versão compilada do Apache Spark que seja maior do que a
2.3.0.<ol><li>você pode tanto compilar <a href=https://github.com/apache/spark target=_blank rel=noopener>do código fonte</a>,
que vai tomar <em>algumas horas</em> até terminar, quanto</li><li>fazer o download de uma versão compilada <a href=https://spark.apache.org/downloads.html target=_blank rel=noopener>aqui</a>
(recomendado).</li></ol></li></ul><p>Assim que o Apache estiver descompactado, vamos adicionar o
caminho no PATH para facilitar a execução:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>export PATH<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>PATH<span style=color:#e6db74>}</span>:/path/to/apache-spark-X.Y.Z/bin
</code></pre></div><h2 id=criando-o-cluster-com-minikube>Criando o &ldquo;cluster&rdquo; com Minikube</h2><p>Agora, para ter um Kubernetes vamos iniciar um <code>minikube</code> com o
propósito de rodar um dos exemplos disponíveis no <a href=https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala target=_blank rel=noopener>repositório do Spark</a>
chamado <code>SparkPi</code> apenas para demonstração.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>minikube start --cpus<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --memory<span style=color:#f92672>=</span>4g
</code></pre></div><h2 id=buildando-a-imagem-docker>Buildando a imagem Docker</h2><p>Vamos utilizar o Docker daemon do Minikube para não depender de um
registry externo (e só gerar lixo na VM, facilitando a limpeza
depois). Para isso, o minikube tem um wrapper que facilita a nossa
vida:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>eval <span style=color:#66d9ef>$(</span>minikube docker-env<span style=color:#66d9ef>)</span>
</code></pre></div><p>Após ter configurado as variáveis de ambiente para o Docker daemon,
vamos precisar de uma imagem Docker para executar os jobs.
Existe um <a href=https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh target=_blank rel=noopener>shell script no repositório do Spark</a>
para ajudar com isso. Considerando que o <code>PATH</code> está configurado
corretamente, basta executar:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>docker-image-tool.sh -m -t latest build
</code></pre></div><p><em>Obs.:</em> O parâmetro <code>-m</code> aqui indica que é um build para o minikube.</p><p>Vamos pegar a via expressa para executar o SparkPi, usando o mesmo
comando que seria utilizado para um cluster Spark <a href=https://spark.apache.org/docs/latest/submitting-applications.html target=_blank rel=noopener>spark-submit</a>.</p><p>Porém, o Spark Operator dá suporte a definição de jobs no &ldquo;dialeto do
Kubernetes&rdquo; usando <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ target=_blank rel=noopener>CRD</a>,
<a href=https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples target=_blank rel=noopener>aqui tem alguns exemplos</a> - para depois.</p><h2 id=fire-in-the-hole>Fire in the hole!</h2><p>Cuidado com o vão entre a versão do Scala e a plataforma quando
estiver parametrizando o job:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>spark-submit --master k8s://https://<span style=color:#66d9ef>$(</span>minikube ip<span style=color:#66d9ef>)</span>:8443 <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --deploy-mode cluster <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --name spark-pi <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --class org.apache.spark.examples.SparkPi <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --conf spark.executor.instances<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --executor-memory 1024m <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --conf spark.kubernetes.container.image<span style=color:#f92672>=</span>spark:latest <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar <span style=color:#75715e># aqui</span>
</code></pre></div><p>O que temos de novidade é:</p><ul><li><code>--master</code>: Aceita o prefixo <code>k8s://</code> na URL, para o endpoint da
API do Kubernetes, exposta pelo commando <code>https://$(minikube ip):8443</code>.
Aliás, se estiver interessado, isso é um <a href=https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html target=_blank rel=noopener>command substitution no
shell</a>;</li><li><code>--conf spark.kubernetes.container.image=</code>: Configuração para a
imagem Docker que será executada no Kubernetes.</li></ul><p>Com o output:</p><pre><code>...

19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,
new state: pod name: spark-pi-1566485909677-driver namespace: default
labels: spark-app-selector -&gt; spark-20477e803e7648a59e9bcd37394f7f60,
spark-role -&gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d
creation time: 2019-08-22T14:58:30Z service account name: default
volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn
node name: minikube start time: 2019-08-22T14:58:30Z container
images: spark:docker phase: Succeeded status:
[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d
6406258ef247648a5902bf6ac09801cc, image=spark:docker,
imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3
a87de32c21787ff82f, lastState=ContainerState(running=null,
terminated=null, waiting=null, additionalProperties={}),
name=spark-kubernetes-driver, ready=false, restartCount=0,
state=ContainerState(running=null,
terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe
e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,
finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,
signal=null, startedAt=2019-08-22T14:58:32Z,
additionalProperties={}), waiting=null, additionalProperties={}),
additionalProperties={})]

19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final
statuses: Container name: spark-kubernetes-driver Container image:
spark:docker Container state: Terminated Exit code: 0
</code></pre><p>Para ver o resultado do job (e toda a execução), podemos mandar um
<code>kubectl logs</code> passando o nome do pod do driver como parâmetro:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl logs <span style=color:#66d9ef>$(</span>kubectl get pods | grep <span style=color:#e6db74>&#39;spark-pi.*-driver&#39;</span><span style=color:#66d9ef>)</span>
</code></pre></div><p>Que traz o output (algumas entradas foram omitidas), parecido com:</p><pre><code>...
19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0
(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)
19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose
tasks have all completed, from pool19/08/22 14:59:08 INFO
DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in
0.957 s
19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at
http://spark-pi-1566485909677-driver-svc.default.svc:4040
19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting
down all executors
19/08/22 14:59:08 INFO
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking
each executor to shut down
19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes
client has been closed (this is expected if the application is
shutting down.)
19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:
MapOutputTrackerMasterEndpoint stopped!
19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared
19/08/22 14:59:08 INFO BlockManager: BlockManager stopped
19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/22 14:59:08 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext
19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2
19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory
/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90
d0-4a0d-9cab-f36a7bb18910
</code></pre><p>O resultado aparece no stdout:</p><pre><code>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at
SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473
</code></pre><p>Para finalizar, vamos deletar a VM que o Minikube gera, para limpar o
ambiente (a menos que você queira continuar brincando com ele):</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>minikube delete
</code></pre></div><h2 id=últimas-palavras>Últimas palavras</h2><p>Espero ter dispertado bastante curiosidade e algumas ideias para
ir além no desenvolvimento dos seus workloads de Big Data.
Se tiver alguma dúvida ou sugestão, não deixe de postar na seção de
comentários.</p></div><div class=article-tags><a class="badge badge-light" href=https://macunha.me/pt/tag/data-engineering/>data-engineering</a>
<a class="badge badge-light" href=https://macunha.me/pt/tag/kubernetes/>kubernetes</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/&text=Quickstart:%20Apache%20Spark%20no%20Kubernetes" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/&t=Quickstart:%20Apache%20Spark%20no%20Kubernetes" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Quickstart:%20Apache%20Spark%20no%20Kubernetes&body=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/&title=Quickstart:%20Apache%20Spark%20no%20Kubernetes" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Quickstart:%20Apache%20Spark%20no%20Kubernetes%20https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://macunha.me/pt/post/2020/05/quickstart-apache-spark-no-kubernetes/&title=Quickstart:%20Apache%20Spark%20no%20Kubernetes" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://macunha.me><img class="avatar mr-3 avatar-circle" src=https://macunha.me/pt/author/matheus-cunha/avatar_hu538abb559021e5e818cc3fcb6b905317_34815_270x270_fill_q90_lanczos_center.jpg alt="Matheus Cunha"></a><div class=media-body><h5 class=card-title><a href=https://macunha.me>Matheus Cunha</a></h5><h6 class=card-subtitle>Engenheiro de Sistemas e Mágico</h6><p class=card-text>Apenas um amante de tecnologia empoderando empresas com computação &ldquo;high-tech&rdquo; para ajudar na inovação (:</p><ul class=network-icon aria-hidden=true><li><a href=https://github.com/macunha1/ target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://gitlab.com/macunha/ target=_blank rel=noopener><i class="fab fa-gitlab"></i></a></li><li><a href=https://www.linkedin.com/in/macunha/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://medium.com/@macunha1 target=_blank rel=noopener><i class="fab fa-medium"></i></a></li></ul></div></div><section id=comments><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='https://macunha1-me.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section><div class="article-widget content-widget-hr"><h3>Relacionados</h3><ul><li><a href=https://macunha.me/pt/project/reclameaqui-data-lake/>ReclameAQUI Data Lake</a></li><li><a href=https://macunha.me/pt/project/dotz-data-labs/>Dotz Data Labs</a></li><li><a href=https://macunha.me/pt/project/easynvest-data-platform/>Easynvest Data Platform</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© 2021 Matheus Cunha</p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with Wowchemy — the free, open source website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Citação</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copiar</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://macunha.me/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script id=dsq-count-scr src=https://macunha1-me.disqus.com/count.js async></script><script src=https://macunha.me/pt/js/wowchemy.min.866966750963019650a87719f3d8159b.js></script><script async defer src=https://buttons.github.io/buttons.js></script></body></html>